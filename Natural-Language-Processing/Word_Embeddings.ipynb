{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a88b62a",
   "metadata": {},
   "source": [
    "# üß© Word Embeddings ‚Äî From Words to Vectors\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ What & Why ‚Äî High-level Intuition\n",
    "\n",
    "**Goal:** Represent each word $ w $ in a vocabulary $ V $ as a vector so that **semantic** and **syntactic** relationships between words are captured geometrically (e.g., similar words are close in space).\n",
    "\n",
    "Two broad families of feature extraction techniques:\n",
    "\n",
    "1. **Count / Frequency-based Representations**\n",
    "   - **Bag of Words (BoW):** Counts of token occurrences.\n",
    "   - **One-Hot Encoding:** A unique binary vector per token.\n",
    "   - **TF‚ÄìIDF:** Weighted frequency emphasizing informative terms.\n",
    "\n",
    "2. **Neural / Deep Learning-based Representations**\n",
    "   - **Word2Vec:** Learns dense word vectors by predicting context (CBOW) or target (Skip-Gram).\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Conceptual Hierarchy of Word Embeddings\n",
    "\n",
    "A clear hierarchical view showing the evolution of word embedding techniques ‚Äî from frequency-based to deep learning-based models.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\boxed{\\Large \\textbf{Word Embedding}} \\\\[10pt]\n",
    "\n",
    "% Main outer arrows to both grouped boxes\n",
    "\\begin{array}{cc}\n",
    "\\swarrow & \\searrow\n",
    "\\end{array}\n",
    "\\\\[8pt]\n",
    "\n",
    "% Two main grouped boxes side-by-side\n",
    "\\begin{array}{cc}\n",
    "\n",
    "% ---------- LEFT GROUP ----------\n",
    "\\color{#2E8B57}{\n",
    "\\boxed{\n",
    "\\begin{array}{c}\n",
    "\\textbf{Count / Frequency-based Models} \\\\[8pt]\n",
    "\\Downarrow \\\\[6pt]\n",
    "\\boxed{\\text{One-Hot}}\n",
    "\\;\\longrightarrow\\;\n",
    "\\boxed{\\text{BoW}}\n",
    "\\;\\longrightarrow\\;\n",
    "\\boxed{\\text{TF‚ÄìIDF}}\n",
    "\\end{array}\n",
    "}}\n",
    "\n",
    "&\n",
    "% ---------- RIGHT GROUP ----------\n",
    "\\color{#4682B4}{\n",
    "\\boxed{\n",
    "\\begin{array}{c}\n",
    "\\textbf{Deep Learning-based Models} \\\\[8pt]\n",
    "\\Downarrow \\\\[6pt]\n",
    "\\boxed{\\text{Word2Vec}} \\\\[6pt]\n",
    "\\Downarrow \\\\[4pt]\n",
    "\\boxed{\\text{CBOW (Continuous BoW)}} \n",
    "\\;\\longleftrightarrow\\;\n",
    "\\boxed{\\text{Skip-Gram}}\n",
    "\\end{array}\n",
    "}}\n",
    "\\end{array}\n",
    "\\\\[12pt]\n",
    "\n",
    "\n",
    "% connecting label\n",
    "\\color{gray}{\n",
    "\\text{(Traditional statistical approaches} \\;\\;\\longrightarrow\\;\\; \\text{Learned neural representations)}\n",
    "}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Explanation\n",
    "\n",
    "#### üåø **Count / Frequency-based Models**\n",
    "| Model | Description |\n",
    "|--------|--------------|\n",
    "| **One-Hot** | Each word represented by a unique binary vector (no relation captured). |\n",
    "| **BoW** | Counts word occurrences per document (order ignored). |\n",
    "| **TF‚ÄìIDF** | Weights terms by importance ‚Äî downweights frequent but less informative words. |\n",
    "\n",
    "‚û°Ô∏è These are **sparse**, **non-contextual**, and depend purely on frequency statistics.\n",
    "\n",
    "---\n",
    "\n",
    "#### üí† **Deep Learning-based Models**\n",
    "| Model | Description |\n",
    "|--------|--------------|\n",
    "| **Word2Vec** | Learns word embeddings using a shallow neural network. |\n",
    "| **CBOW (Continuous BoW)** | Predicts a word from its context words. |\n",
    "| **Skip-Gram** | Predicts context words from the center word. |\n",
    "\n",
    "‚û°Ô∏è These are **dense**, **semantic**, and **context-aware** ‚Äî capturing relationships like:  \n",
    "$$\n",
    "\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Summary of the Evolution\n",
    "1. **Start:** Discrete vectors ‚Üí *(One-Hot)*  \n",
    "2. **Add frequency context:** *(BoW)*  \n",
    "3. **Add weighting for importance:** *(TF‚ÄìIDF)*  \n",
    "4. **Add learning and semantics:** *(Word2Vec)*  \n",
    "5. **Add directionality/context modeling:** *(CBOW / Skip-Gram)*  \n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Formal Definitions\n",
    "\n",
    "### üîπ One-Hot Encoding\n",
    "For vocabulary size $ |V| $, the one-hot vector for word $ w $ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_w \\in \\{0,1\\}^{|V|}, \\quad\n",
    "(\\mathbf{x}_w)_i =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } i = \\operatorname{index}(w) \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Pros:** Simple, unambiguous.\n",
    "- **Cons:** Extremely sparse; no similarity between words.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Bag-of-Words (BoW)\n",
    "A document $ d $ is represented as a count vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{c}_d \\in \\mathbb{R}^{|V|}, \\quad\n",
    "(\\mathbf{c}_d)_i = \\text{count of token } v_i \\text{ in } d.\n",
    "$$\n",
    "\n",
    "- **Pros:** Straightforward and effective for linear models.\n",
    "- **Cons:** Ignores word order and context.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ TF‚ÄìIDF (Term Frequency ‚Äì Inverse Document Frequency)\n",
    "Weights terms based on their importance across documents:\n",
    "\n",
    "$$\n",
    "\\operatorname{tfidf}(t, d) = \\operatorname{tf}(t, d) \\cdot \\operatorname{idf}(t)\n",
    "$$\n",
    "$$\n",
    "\\operatorname{idf}(t) = \\log\\!\\left( \\frac{N}{1 + \\operatorname{df}(t)} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ N $: total number of documents  \n",
    "- $ \\operatorname{df}(t) $: number of documents containing term $ t $\n",
    "\n",
    "**Common similarity metric:**\n",
    "\n",
    "$$\n",
    "\\cos(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a}^\\top \\mathbf{b}}{\\lVert \\mathbf{a}\\rVert_2 \\lVert \\mathbf{b}\\rVert_2}\n",
    "$$\n",
    "\n",
    "- **Pros:** Reduces weight of common words.\n",
    "- **Cons:** Sparse, still ignores semantic meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Word2Vec (Learned Dense Embeddings)\n",
    "Word2Vec learns dense vector representations through a neural network.\n",
    "\n",
    "Each word $ w $ has:\n",
    "- **Input vector:** $ \\mathbf{v}_w $\n",
    "- **Output vector:** $ \\mathbf{u}_w $\n",
    "\n",
    "---\n",
    "\n",
    "#### üü¢ Skip-Gram Model\n",
    "Predicts context words $ w_c $ from a center word $ w_t $:\n",
    "\n",
    "$$\n",
    "\\max_\\Theta \\sum_{t} \\sum_{w_c \\in \\mathcal{C}_t} \n",
    "\\log p(w_c | w_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(w_c | w_t) =\n",
    "\\frac{\\exp(\\mathbf{u}_{w_c}^{\\top}\\mathbf{v}_{w_t})}\n",
    "{\\sum_{w' \\in V}\\exp(\\mathbf{u}_{w'}^{\\top}\\mathbf{v}_{w_t})}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### üü£ Continuous Bag-of-Words (CBOW)\n",
    "Predicts the target word from the average of its surrounding context words:\n",
    "\n",
    "$$\n",
    "\\bar{\\mathbf{v}}_{\\mathcal{C}_t} =\n",
    "\\frac{1}{|\\mathcal{C}_t|} \\sum_{w_c \\in \\mathcal{C}_t} \\mathbf{v}_{w_c}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\max_\\Theta \\sum_t \\log p(w_t | \\bar{\\mathbf{v}}_{\\mathcal{C}_t})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è Negative Sampling (Efficient Approximation)\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NS}} =\n",
    "- \\Big[\n",
    "\\log \\sigma(\\mathbf{u}_{w_c}^{\\top}\\mathbf{v}_{w_t})\n",
    "+ \\sum_{i=1}^{k} \\log \\sigma(-\\mathbf{u}_{w_i^{-}}^{\\top}\\mathbf{v}_{w_t})\n",
    "\\Big]\n",
    "$$\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "- **Pros:** Dense, compact, captures analogies and semantics.  \n",
    "- **Cons:** Needs training, one vector per word sense.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Conceptual Comparison (Feature Types)\n",
    "\n",
    "| **Aspect** | **One-Hot** | **BoW** | **TF‚ÄìIDF** | **Word2Vec (CBOW/SG)** |\n",
    "|-------------|--------------|----------|-------------|---------------------------|\n",
    "| **Dimensionality** | $ |V| $ | $ |V| $ | $ |V| $ | $ d \\ll |V| $ |\n",
    "| **Sparsity** | Very high | High | High | **Low (dense)** |\n",
    "| **Context Awareness** | ‚úó | ‚úó | ‚úó | **‚úì** |\n",
    "| **Semantic Similarity** | ‚úó | Limited | Limited | **‚úì‚úì‚úì** |\n",
    "| **Training Needed** | No | No | No | **Yes** |\n",
    "| **Order / Syntax** | ‚úó | ‚úó | ‚úó | **Partial (via context window)** |\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Mathematical Relationships (KaTeX Visuals)\n",
    "\n",
    "**Cosine Similarity:**\n",
    "$$\n",
    "\\text{similarity}(w_i, w_j) = \n",
    "\\cos(\\mathbf{e}_{w_i}, \\mathbf{e}_{w_j}) =\n",
    "\\frac{\\mathbf{e}_{w_i}^{\\top}\\mathbf{e}_{w_j}}\n",
    "{\\lVert \\mathbf{e}_{w_i}\\rVert_2 \\lVert \\mathbf{e}_{w_j}\\rVert_2}\n",
    "$$\n",
    "\n",
    "**Analogy Relationship:**\n",
    "$$\n",
    "\\mathbf{e}_{\\text{king}} - \\mathbf{e}_{\\text{man}} + \\mathbf{e}_{\\text{woman}}\n",
    "\\approx \\mathbf{e}_{\\text{queen}}\n",
    "$$\n",
    "\n",
    "**Context Window ( $ m $) around token $ t $:**\n",
    "$$\n",
    "\\mathcal{C}_t = \\{ w_{t-m}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+m} \\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Practical Notes\n",
    "\n",
    "- Use **TF‚ÄìIDF** for classical ML models (SVM, Logistic Regression) when interpretability matters.  \n",
    "- Use **Word2Vec** for deep learning tasks requiring semantic understanding.  \n",
    "- Hybrid approaches combine both:  \n",
    "  $$\n",
    "  \\mathbf{v}_{\\text{doc}} = [\\text{TF‚ÄìIDF} \\; \\| \\; \\text{Mean(Word2Vec)}]\n",
    "  $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a9aaab",
   "metadata": {},
   "source": [
    "# üß© Word Embeddings ‚Äî From Words to Vectors (with Examples)\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Understanding the Landscape\n",
    "\n",
    "Word embeddings transform **words ‚Üí numeric vectors**, enabling models to understand semantics and similarity.\n",
    "\n",
    "Two major categories:\n",
    "\n",
    "- **Count/Frequency-based Representations**\n",
    "  - Represent text using counts or weighted frequencies.\n",
    "- **Neural/Deep Learning-based Representations**\n",
    "  - Learn continuous dense vectors using training objectives.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Step-by-Step Examples with Substituted Values\n",
    "\n",
    "Let‚Äôs take a **toy corpus** of two simple sentences:\n",
    "\n",
    "> üóÇÔ∏è Corpus:  \n",
    "> D‚ÇÅ = \"I love dogs\"  \n",
    "> D‚ÇÇ = \"I love cats\"\n",
    "\n",
    "Vocabulary $ V = \\{ \\text{I}, \\text{love}, \\text{dogs}, \\text{cats} \\} \\Rightarrow |V| = 4 $\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Example 1 ‚Äî One-Hot Encoding\n",
    "\n",
    "Each word is represented as a binary vector of size $ |V| = 4 $.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{I}      &\\rightarrow [1, 0, 0, 0] \\\\\n",
    "\\text{love}   &\\rightarrow [0, 1, 0, 0] \\\\\n",
    "\\text{dogs}   &\\rightarrow [0, 0, 1, 0] \\\\\n",
    "\\text{cats}   &\\rightarrow [0, 0, 0, 1]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Each position corresponds to a vocabulary index.\n",
    "No relationship between ‚Äúdogs‚Äù and ‚Äúcats‚Äù is captured ‚Äî they‚Äôre orthogonal.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Example 2 ‚Äî Bag-of-Words (BoW)\n",
    "\n",
    "Represent each document as counts over the vocabulary:\n",
    "\n",
    "| Term  | I | love | dogs | cats |\n",
    "|--------|---|------|------|------|\n",
    "| D‚ÇÅ     | 1 | 1 | 1 | 0 |\n",
    "| D‚ÇÇ     | 1 | 1 | 0 | 1 |\n",
    "\n",
    "$$\n",
    "\\mathbf{c}_{D_1} = [1, 1, 1, 0], \\quad\n",
    "\\mathbf{c}_{D_2} = [1, 1, 0, 1]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Example 3 ‚Äî TF‚ÄìIDF Calculation (with substitution)\n",
    "\n",
    "Compute TF‚ÄìIDF weights for each word:\n",
    "\n",
    "$$\n",
    "\\operatorname{tfidf}(t, d) = \\operatorname{tf}(t, d) \\times \\operatorname{idf}(t)\n",
    "$$\n",
    "$$\n",
    "\\operatorname{idf}(t) = \\log\\!\\left( \\frac{N}{1 + \\operatorname{df}(t)} \\right)\n",
    "$$\n",
    "\n",
    "Where $ N = 2 $ (number of documents).\n",
    "\n",
    "| Term | D‚ÇÅ tf | D‚ÇÇ tf | df(t) | idf(t) = log(2 / (1 + df)) |\n",
    "|------|--------|--------|-------|-----------------------------|\n",
    "| I | 1 | 1 | 2 | log(2 / 3) = -0.176 |\n",
    "| love | 1 | 1 | 2 | log(2 / 3) = -0.176 |\n",
    "| dogs | 1 | 0 | 1 | log(2 / 2) = 0 |\n",
    "| cats | 0 | 1 | 1 | log(2 / 2) = 0 |\n",
    "\n",
    "$$\n",
    "\\text{TF‚ÄìIDF}(D_1) = [1 \\times (-0.176), 1 \\times (-0.176), 1 \\times 0, 0 \\times 0]\n",
    "$$\n",
    "$$\n",
    "\\text{TF‚ÄìIDF}(D_2) = [1 \\times (-0.176), 1 \\times (-0.176), 0, 1 \\times 0]\n",
    "$$\n",
    "\n",
    "Result (rounded):\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{D_1} = [-0.18, -0.18, 0, 0], \\quad\n",
    "\\mathbf{v}_{D_2} = [-0.18, -0.18, 0, 0]\n",
    "$$\n",
    "\n",
    "‚ö†Ô∏è Both documents look identical for ‚ÄúI love X‚Äù ‚Üí TF‚ÄìIDF can‚Äôt distinguish meaning (semantic gap).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Example 4 ‚Äî Word2Vec (Conceptual Substitution)\n",
    "\n",
    "**Idea:** Learn embedding vectors such that context predicts target (Skip-Gram) or vice versa (CBOW).\n",
    "\n",
    "Let‚Äôs assume:\n",
    "$$\n",
    "\\text{Sentence: \"I love dogs\"}\n",
    "$$\n",
    "and window size $ m = 1 $.\n",
    "\n",
    "**Context windows:**\n",
    "\n",
    "| Target | Context |\n",
    "|---------|----------|\n",
    "| I | [love] |\n",
    "| love | [I, dogs] |\n",
    "| dogs | [love] |\n",
    "\n",
    "---\n",
    "\n",
    "#### (a) Skip-Gram Objective Example\n",
    "\n",
    "For pair $ (w_t = \\text{love}, w_c = \\text{dogs}) $:\n",
    "\n",
    "$$\n",
    "p(w_c | w_t) = \\frac{\\exp(\\mathbf{u}_{w_c}^{\\top}\\mathbf{v}_{w_t})}\n",
    "{\\sum_{w' \\in V} \\exp(\\mathbf{u}_{w'}^{\\top}\\mathbf{v}_{w_t})}\n",
    "$$\n",
    "\n",
    "Assume 2D embeddings:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{\\text{love}} = [0.2, 0.8], \\quad\n",
    "\\mathbf{u}_{\\text{dogs}} = [0.3, 0.9]\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mathbf{u}_{\\text{dogs}}^{\\top}\\mathbf{v}_{\\text{love}} =\n",
    "(0.3)(0.2) + (0.9)(0.8) = 0.06 + 0.72 = 0.78\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(\\text{dogs}|\\text{love}) \\propto e^{0.78} = 2.18\n",
    "$$\n",
    "\n",
    "After normalization over the vocabulary, the probability of ‚Äúdogs‚Äù appearing after ‚Äúlove‚Äù becomes high ‚Äî exactly what we want semantically.\n",
    "\n",
    "---\n",
    "\n",
    "#### (b) CBOW Objective Example\n",
    "\n",
    "Predict center word $ w_t = \\text{love} $ from context [I, dogs]:\n",
    "\n",
    "$$\n",
    "\\bar{\\mathbf{v}}_{\\mathcal{C}_t} =\n",
    "\\frac{\\mathbf{v}_{\\text{I}} + \\mathbf{v}_{\\text{dogs}}}{2}\n",
    "$$\n",
    "\n",
    "Assume:\n",
    "$$\n",
    "\\mathbf{v}_{\\text{I}} = [0.1, 0.2], \\quad\n",
    "\\mathbf{v}_{\\text{dogs}} = [0.3, 0.9]\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\bar{\\mathbf{v}}_{\\mathcal{C}_t} = \\frac{[0.1 + 0.3, 0.2 + 0.9]}{2} = [0.2, 0.55]\n",
    "$$\n",
    "\n",
    "The model then predicts the word ‚Äúlove‚Äù using this averaged context embedding.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Summary Table ‚Äî Intuition and Representation\n",
    "\n",
    "| **Aspect** | **One-Hot** | **BoW** | **TF‚ÄìIDF** | **Word2Vec** |\n",
    "|-------------|--------------|----------|-------------|---------------|\n",
    "| **Representation** | Binary vector | Count vector | Weighted count | Learned dense vector |\n",
    "| **Captures Context?** | ‚úó | ‚úó | ‚úó | ‚úì |\n",
    "| **Handles Synonyms?** | ‚úó | ‚úó | ‚úó | ‚úì |\n",
    "| **Dimensionality** | $ |V| $ | $ |V| $ | $ |V| $ | $ d \\ll |V| $ |\n",
    "| **Sparsity** | High | High | High | Low |\n",
    "| **Example Dim (|V|=4)** | [1,0,0,0] | [1,1,1,0] | [-0.18,-0.18,0,0] | [0.2,0.8] |\n",
    "| **Semantic Power** | ‚ùå | ‚ö™Ô∏è | ‚ö™Ô∏è | ‚úÖ‚úÖ‚úÖ |\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Takeaway\n",
    "\n",
    "- **Count-based** methods rely on surface frequency ‚Äî simple, interpretable, but lack meaning.  \n",
    "- **Word2Vec** learns meaning from context ‚Äî geometrically encoding relationships:\n",
    "  $$\n",
    "  \\mathbf{e}_{\\text{king}} - \\mathbf{e}_{\\text{man}} + \\mathbf{e}_{\\text{woman}}\n",
    "  \\approx \\mathbf{e}_{\\text{queen}}\n",
    "  $$\n",
    "- This allows downstream models to generalize better and understand analogy.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
