{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b4e753",
   "metadata": {},
   "source": [
    "# 🔧 Text Preprocessing — Stemming & Its Types\n",
    "\n",
    "**Stemming** is a **rule-based text normalization process** that reduces words to their **base stem** by chopping off **suffixes** or **prefixes** using a series of predefined rules.  \n",
    "It is a **fast**, **statistical**, and **dictionary-free** approach that approximates a word’s **root form (lemma)**.\n",
    "\n",
    "---\n",
    "\n",
    "📘 **Important Distinction**\n",
    "\n",
    "> While **lemmatization** uses vocabulary and morphological analysis to find valid words (lemmas),  \n",
    "> **stemming** merely truncates tokens based on heuristic rules — meaning the result *may not* be a valid dictionary word.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Example\n",
    "\n",
    "| Original Word | Stem Output | Valid Word? |\n",
    "|:--------------|:-------------|:-------------|\n",
    "| connections | connect | ✅ |\n",
    "| studies | studi | ❌ |\n",
    "| organizing | organiz | ✅ (approximation) |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Formal Definition\n",
    "\n",
    "Let $w$ be a token (word). The stemming process applies a function:\n",
    "$$\n",
    "\\text{stem}(w) \\rightarrow s\n",
    "$$\n",
    "where $s$ is the derived stem that may or may not correspond to a valid lemma.\n",
    "\n",
    "Given a corpus $C = \\{w_1, w_2, \\dots, w_n\\}$,\n",
    "$$\n",
    "S = \\{\\text{stem}(w_1), \\text{stem}(w_2), \\dots, \\text{stem}(w_n)\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Why We Use Stemming\n",
    "\n",
    "✅ Reduces the **vocabulary size** → smaller, faster models  \n",
    "✅ Groups **morphological variants** → improves recall in text search  \n",
    "⚠️ May **over-stem** (merge unrelated words) or **under-stem** (fail to merge related words)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Stemming vs Lemmatization\n",
    "\n",
    "| Feature | Stemming | Lemmatization |\n",
    "|:--------|:----------|:---------------|\n",
    "| Output | Truncated stem | Valid dictionary word |\n",
    "| Logic | Rule-based suffix chopping | Morphology + POS + Dictionary |\n",
    "| Accuracy | Approximate | Precise |\n",
    "| Speed | ⚡ Fast | 🐢 Slower |\n",
    "| Example | *studies → studi* | *studies → study* |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0509e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Corpus Preview — Tokens for Stemming Exploration\n",
      "Total tokens: 42\n",
      "\n",
      "['connect', 'connected', 'connection', 'connections', 'connecting', 'study', 'studies', 'studying', 'studied', 'happy', 'happiness', 'unhappily', 'organization', 'organize', 'organized', 'organizing', 'generalize', 'generalized', 'generalization', 'go', 'going', 'goes', 'gone', 'better', 'best', 'relational', 'relate', 'related', 'relating', 'cats', 'boxes', 'mice', 'practically', 'practical', 'practicable', 'writing', 'writes', 'programming', 'programs', 'history', 'finally', 'finalized']\n"
     ]
    }
   ],
   "source": [
    "# A diverse test vocabulary containing verbs, nouns, adjectives, adverbs, and edge cases.\n",
    "# Each \"family\" groups related morphological variants to reveal how stemmers conflate them.\n",
    "tokens = [\n",
    "    # ----- connect-family (regular verb/noun derivations)\n",
    "    \"connect\", \"connected\", \"connection\", \"connections\", \"connecting\",\n",
    "    # ----- study-family (y→i + inflections)\n",
    "    \"study\", \"studies\", \"studying\", \"studied\",\n",
    "    # ----- happy-family (adj/adv/negation)\n",
    "    \"happy\", \"happiness\", \"unhappily\",\n",
    "    # ----- organize-family (ize/ization derivations)\n",
    "    \"organization\", \"organize\", \"organized\", \"organizing\",\n",
    "    # ----- generalize-family (ize/ization derivations)\n",
    "    \"generalize\", \"generalized\", \"generalization\",\n",
    "    # ----- go-family (irregular verb forms)\n",
    "    \"go\", \"going\", \"goes\", \"gone\",\n",
    "    # ----- comparatives/superlatives (irregular scale)\n",
    "    \"better\", \"best\",\n",
    "    # ----- relate-family (derivational adjective)\n",
    "    \"relational\", \"relate\", \"related\", \"relating\",\n",
    "    # ----- pluralization and irregular plurals\n",
    "    \"cats\", \"boxes\", \"mice\",\n",
    "    # ----- practical-family (derivational/orthographic similarity)\n",
    "    \"practically\", \"practical\", \"practicable\",\n",
    "    # write-family: test progressive (-ing) and 3rd-person singular (-s)\n",
    "    \"writing\", \"writes\",\n",
    "    # program-family: noun vs. verb derivations (+plural -s)\n",
    "    \"programming\", \"programs\",\n",
    "    # history-family: bare noun (tests that some words have no useful conflation)\n",
    "    \"history\",\n",
    "    # 🏁 final-family: adverb (-ly) vs. past participle (-ed)\n",
    "    \"finally\", \"finalized\",\n",
    "]\n",
    "\n",
    "# Display metadata about our sample\n",
    "print(\"🧾 Corpus Preview — Tokens for Stemming Exploration\")\n",
    "print(f\"Total tokens: {len(tokens)}\\n\")\n",
    "print(tokens)\n",
    "\n",
    "# 💡 Design notes:\n",
    "# - Regular and irregular inflections (connect, study, go)\n",
    "# - Adjective/adverb forms and negation (happy → unhappily)\n",
    "# - Derivational morphology (organization, generalization)\n",
    "# - Comparatives/superlatives (better, best) to see if stemmers over-conflate\n",
    "# - Plural forms (cats, boxes) and irregular plurals (mice)\n",
    "# - Orthographically similar but semantically distinct (practical vs practicable)\n",
    "# - Newly added: write/program/final/history families to probe -ing, -s, -ly, -ed endings\n",
    "#   and noun/verb derivations common in software/text corpora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df60870b",
   "metadata": {},
   "source": [
    "## 🧰 Types of Stemmers in NLP\n",
    "\n",
    "There are several stemmers available in NLTK, each with its own level of aggressiveness and rule set.\n",
    "\n",
    "| Stemmer | Description | Behavior |\n",
    "|:--------|:-------------|:------------|\n",
    "| **PorterStemmer** | 🧩 The most classic rule-based stemmer; moderate and reliable | Conservative |\n",
    "| **SnowballStemmer (Porter2)** | ❄️ Enhanced Porter version; supports multiple languages | Balanced |\n",
    "| **LancasterStemmer** | ⚡ Extremely aggressive; very short stems | Over-stems often |\n",
    "| **RegexpStemmer** | 🔍 Custom regex-based stemmer; great for domain control | Fully customizable |\n",
    "\n",
    "---\n",
    "\n",
    "Each stemmer applies pattern-based rules:\n",
    "$$\n",
    "\\text{stemmer}(w) = w - \\text{(suffixes according to pattern rules)}\n",
    "$$\n",
    "\n",
    "Let's compare how these differ in practice 👇\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b90869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Import stemmers from NLTK and define a sample corpus of tokens\n",
    "# We'll test how different stemmers handle various morphological forms.\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c5b4c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Porter Stemmer Results:\n",
      "\n",
      "        connect  →  connect\n",
      "      connected  →  connect\n",
      "     connection  →  connect\n",
      "    connections  →  connect\n",
      "     connecting  →  connect\n",
      "          study  →  studi\n",
      "        studies  →  studi\n",
      "       studying  →  studi\n",
      "        studied  →  studi\n",
      "          happy  →  happi\n",
      "      happiness  →  happi\n",
      "      unhappily  →  unhappili\n",
      "   organization  →  organ\n",
      "       organize  →  organ\n",
      "      organized  →  organ\n",
      "     organizing  →  organ\n",
      "     generalize  →  gener\n",
      "    generalized  →  gener\n",
      " generalization  →  gener\n",
      "             go  →  go\n",
      "          going  →  go\n",
      "           goes  →  goe\n",
      "           gone  →  gone\n",
      "         better  →  better\n",
      "           best  →  best\n",
      "     relational  →  relat\n",
      "         relate  →  relat\n",
      "        related  →  relat\n",
      "       relating  →  relat\n",
      "           cats  →  cat\n",
      "          boxes  →  box\n",
      "           mice  →  mice\n",
      "    practically  →  practic\n",
      "      practical  →  practic\n",
      "    practicable  →  practic\n",
      "        writing  →  write\n",
      "         writes  →  write\n",
      "    programming  →  program\n",
      "       programs  →  program\n",
      "        history  →  histori\n",
      "        finally  →  final\n",
      "      finalized  →  final\n"
     ]
    }
   ],
   "source": [
    "# ✅ 1. Porter Stemmer — the most commonly used baseline\n",
    "#    Developed by Martin Porter in 1980, uses a fixed set of rules (about 60)\n",
    "#    Typically produces readable stems and avoids excessive truncation.\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "print(\"🔹 Porter Stemmer Results:\\n\")\n",
    "for w in tokens:\n",
    "    stem = porter.stem(w)\n",
    "    print(f\"{w:>15}  →  {stem}\")\n",
    "\n",
    "# 🧠 Notes:\n",
    "# - \"connections\" → \"connect\"\n",
    "# - \"studies\" → \"studi\"\n",
    "# - \"organizing\" → \"organ\"\n",
    "# Porter tries to keep stems interpretable, but not necessarily valid words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc87590a",
   "metadata": {},
   "source": [
    "## 🧩 Why Porter Stemmer Produces “studi” and “organ”\n",
    "\n",
    "The **Porter Stemmer** works through a series of **rule-based substitution phases**, where each step applies a pattern like:\n",
    "\n",
    "$$\n",
    "\\text{(suffix)} \\rightarrow \\text{(replacement)}\n",
    "$$\n",
    "\n",
    "These rules are *ordered and conditional*, meaning:\n",
    "- The stemmer checks for certain suffix patterns in a fixed sequence.\n",
    "- Once a rule fires, it may not revisit the word with later patterns.\n",
    "\n",
    "Let’s analyze the examples 👇\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `\"connections\" → \"connect\"`\n",
    "**Rule triggered:**\n",
    "- Remove plural **-s** or **-es** → “connection”\n",
    "- Remove nominal suffix **-ion** if preceded by “ct” → “connect”\n",
    "\n",
    "✅ This is a *perfect case*: Porter correctly identifies the root form “connect”.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `\"studies\" → \"studi\"`\n",
    "**Rule triggered:**\n",
    "- Step 1: `ies` → `i` (Porter replaces “ies” with “i” to generalize plural forms)\n",
    "  \n",
    "$$\n",
    "\\text{studies} \\rightarrow \\text{studi}\n",
    "$$\n",
    "\n",
    "🔍 Porter assumes that words ending in “ies” are plural or 3rd person forms of verbs ending with “y”.\n",
    "But it does **not** convert “i” back to “y”, since it’s unaware of morphological semantics.\n",
    "\n",
    "⚠️ **Result:** “studi” is **not** a valid word (should have been “study”).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 `\"organizing\" → \"organ\"`\n",
    "**Rule triggered:**\n",
    "- Step 1: Remove **-ing** → “organiz”\n",
    "- Step 2: If the remaining word ends in **-iz**, and a rule matches **-ize → e**, Porter sometimes truncates the suffix inconsistently.\n",
    "- Since the rule chain doesn’t always reconstruct “organize”, it stops at “organ”.\n",
    "\n",
    "⚠️ This happens because Porter doesn’t look at full morphology — it only applies simple text-based rules, not grammar.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Drawbacks of Porter Stemmer\n",
    "\n",
    "| Limitation | Description | Example |\n",
    "|:------------|:-------------|:----------|\n",
    "| **1️⃣ Over-stemming** | Different words reduced to same root (loss of meaning) | “organization” & “organism” → “organ” |\n",
    "| **2️⃣ Under-stemming** | Related words fail to merge | “unhappy” & “happiness” remain separate |\n",
    "| **3️⃣ Non-dictionary roots** | Outputs stems like “studi”, “happi”, “gener” | “studies” → “studi” |\n",
    "| **4️⃣ No POS or context awareness** | Doesn’t know if word is noun/verb/adjective | “better” → “bett” |\n",
    "| **5️⃣ Fixed English-only rules** | Limited cross-linguistic support | Works poorly on non-English corpora |\n",
    "\n",
    "---\n",
    "\n",
    "📌 **In short:**\n",
    "- Porter is **fast**, **deterministic**, and great for **IR tasks** (like search engines).\n",
    "- But it’s **linguistically naive** — it just “chops”, it doesn’t “understand”.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ When Porter Stemmer is Still Useful\n",
    "- For **Information Retrieval (IR)** — when perfect word forms aren’t necessary.  \n",
    "- For **keyword-based search systems** (search “connect” finds “connections”).  \n",
    "- When you need lightweight preprocessing in large text pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "📚 **When NOT to use Porter:**\n",
    "- When your downstream model depends on *precise grammatical or lexical meaning*  \n",
    "  (like text generation, translation, or semantic similarity tasks).  \n",
    "In those cases, prefer **Lemmatization**.\n",
    "\n",
    "---\n",
    "\n",
    "💡 **Summary Insight**\n",
    "\n",
    "$$\n",
    "\\text{PorterStemmer} \\approx \\text{HeuristicSuffixRemover}\n",
    "$$\n",
    "\n",
    "✅ Efficient for search  \n",
    "❌ Not semantically accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84725c",
   "metadata": {},
   "source": [
    "## 🧩 RegexpStemmer — Custom, Rule-Driven Stemming\n",
    "\n",
    "**`RegexpStemmer`** (short for *Regular Expression Stemmer*) is a **lightweight and customizable** stemmer provided by NLTK.  \n",
    "Unlike other stemmers (Porter, Snowball, Lancaster), which use **predefined linguistic rules**,  \n",
    "this stemmer allows you to define **your own pattern-based stripping rules** using **regular expressions (regex)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Working Principle\n",
    "\n",
    "The stemmer applies a **regex substitution** to remove specific suffixes or endings.\n",
    "\n",
    "$$\n",
    "\\text{stem}(w) = w - \\text{(regex\\_matched\\_suffix)}\n",
    "$$\n",
    "\n",
    "In other words:\n",
    "1. The stemmer looks for patterns that match your regex rule.  \n",
    "2. If the pattern appears **at the end** of the word, it is replaced with an empty string (i.e., removed).  \n",
    "3. The operation is purely **text-based** — no morphological knowledge, no POS awareness.\n",
    "\n",
    "---\n",
    "\n",
    "### 📘 Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|:--|:--|:--|\n",
    "| **pattern** | `str` (regex) | A pattern describing suffixes to remove (e.g., `(ing|ly|ed|s)$`) |\n",
    "| **min** | `int` | Minimum length of the remaining word (prevents over-stripping) |\n",
    "| **ignore_case** | `bool` | Whether to match case-insensitive suffixes |\n",
    "| **repl** | `str` | Replacement string (default: empty) |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Example\n",
    "\n",
    "```python\n",
    "from nltk.stem import RegexpStemmer\n",
    "regexp = RegexpStemmer(regexp=r'(ing|ly|ed|ious|ies|ive|es|s|ment)$', min=3)\n",
    "\n",
    "print(regexp.stem(\"studying\"))      # → study\n",
    "print(regexp.stem(\"boxes\"))         # → box\n",
    "print(regexp.stem(\"connections\"))   # → connection\n",
    "print(regexp.stem(\"happily\"))       # → happi\n",
    "```\n",
    "---\n",
    "\n",
    "### 🧩 Explanation\n",
    "- The regex removes common English endings (like -ing, -ed, -s).\n",
    "- `min=3` ensures that very short words (like “is”, “as”) are not truncated.\n",
    "- The `$` ensures suffixes are matched only at the end of the token.\n",
    "---\n",
    "\n",
    "### ✅ Advantages\n",
    "\n",
    "| Benefit             | Description                                                     |\n",
    "| :------------------ | :-------------------------------------------------------------- |\n",
    "| **Customizable**    | You control exactly which suffixes are removed                  |\n",
    "| **Lightweight**     | No heavy rule set or model loading                              |\n",
    "| **Fast**            | Regex-based → extremely quick on large corpora                  |\n",
    "| **Domain-friendly** | Ideal for domain-specific text (medical, legal, software, etc.) |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Limitations\n",
    "\n",
    "| Drawback                  | Description                                           |\n",
    "| :------------------------ | :---------------------------------------------------- |\n",
    "| ❌ No linguistic knowledge | Doesn’t know valid lemmas or word families            |\n",
    "| ❌ Can under- or over-stem | Regex too broad or too narrow causes errors           |\n",
    "| ❌ English-only by design  | You’d need new regex rules per language               |\n",
    "| ❌ Not context aware       | “goes” → “go” works, but “was” or “went” won’t change |\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Best Use Cases\n",
    "- You want full control over stemming rules.\n",
    "- You work in restricted domains (e.g., biomedical, legal, or software corpora).\n",
    "- You need a fast and transparent way to reduce word variants without external dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Mathematical Recap\n",
    "\n",
    "$$\n",
    "\\text{RegexpStemmer}(w) =\n",
    "\\begin{cases}\n",
    "w - \\text{regex\\_suffix}, & \\text{if pattern matches the end of } w \\\\\n",
    "w, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "--- \n",
    "\n",
    "### 💡 Summary\n",
    "\n",
    "| Feature      | Description                                            |\n",
    "| :----------- | :----------------------------------------------------- |\n",
    "| **Approach** | Regex-based suffix removal                             |\n",
    "| **Accuracy** | Depends entirely on your regex quality                 |\n",
    "| **Speed**    | ⚡ Very fast                                            |\n",
    "| **Output**   | Non-linguistic stems                                   |\n",
    "| **Best for** | Controlled pipelines and domain-specific normalization |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68d00966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Regexp Stemmer Results (Custom Suffix Chopping):\n",
      "\n",
      "        connect  →  connect\n",
      "      connected  →  connect\n",
      "     connection  →  connection\n",
      "    connections  →  connection\n",
      "     connecting  →  connect\n",
      "          study  →  study\n",
      "        studies  →  stud\n",
      "       studying  →  study\n",
      "        studied  →  studi\n",
      "          happy  →  happy\n",
      "      happiness  →  happines\n",
      "      unhappily  →  unhappi\n",
      "   organization  →  organization\n",
      "       organize  →  organize\n",
      "      organized  →  organiz\n",
      "     organizing  →  organiz\n",
      "     generalize  →  generalize\n",
      "    generalized  →  generaliz\n",
      " generalization  →  generalization\n",
      "             go  →  go\n",
      "          going  →  go\n",
      "           goes  →  go\n",
      "           gone  →  gone\n",
      "         better  →  better\n",
      "           best  →  best\n",
      "     relational  →  relational\n",
      "         relate  →  relate\n",
      "        related  →  relat\n",
      "       relating  →  relat\n",
      "           cats  →  cat\n",
      "          boxes  →  box\n",
      "           mice  →  mice\n",
      "    practically  →  practical\n",
      "      practical  →  practical\n",
      "    practicable  →  practicable\n",
      "        writing  →  writ\n",
      "         writes  →  writ\n",
      "    programming  →  programm\n",
      "       programs  →  program\n",
      "        history  →  history\n",
      "        finally  →  final\n",
      "      finalized  →  finaliz\n"
     ]
    }
   ],
   "source": [
    "# 🧪 Regexp Stemmer — create domain-specific rules\n",
    "# Here, we remove a few common English suffixes when they appear at the end of a word.\n",
    "\n",
    "# Pattern: remove -ing, -ly, -ed, -ious, -ies, -ive, -es, -s, -ment\n",
    "regexp = RegexpStemmer(\n",
    "    regexp=r'(ing|ly|ed|ious|ies|ive|es|s|ment)$',\n",
    "    min=3\n",
    ")\n",
    "\n",
    "print(\"\\n🔹 Regexp Stemmer Results (Custom Suffix Chopping):\\n\")\n",
    "for w in tokens:\n",
    "    print(f\"{w:>15}  →  {regexp.stem(w)}\")\n",
    "\n",
    "# 📝 Notes:\n",
    "# - `min=3` prevents stripping when the remainder would be < 3 chars\n",
    "# - Tune the regex to your domain (biomedical, legal, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50374d8",
   "metadata": {},
   "source": [
    "## ⚠️ Why Some Outputs Are Incorrect in RegexpStemmer\n",
    "\n",
    "The **RegexpStemmer** is purely **pattern-based**, meaning it does not understand language morphology.  \n",
    "It simply strips the suffixes that match your custom regular expression — even when doing so **breaks the word form**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Example Analysis (Based on Output)\n",
    "\n",
    "| Word | Output | Expected | What Happened |\n",
    "|:--|:--|:--|:--|\n",
    "| `studies` | `stud` | `study` | Regex removed `ies`, leaving `stud`; it doesn’t replace `ies → y` like linguistic stemmers. |\n",
    "| `studied` | `studi` | `study` | Removed `ed` → produced non-word root. |\n",
    "| `happiness` | `happines` | `happy` | Removed only final `s`, not `ness`; pattern didn’t match `ness`. |\n",
    "| `unhappily` | `unhappi` | `unhappy` | Removed `ly`, but didn’t restore `y`. |\n",
    "| `organized` | `organiz` | `organize` | Removed `ed`, unaware that base form has `e`. |\n",
    "| `programming` | `programm` | `program` | Removed `ing`, but double `m` rule (from `mming` → `m`) isn’t handled. |\n",
    "| `writing` | `writ` | `write` | Removed `ing`, unaware that root should regain the `e`. |\n",
    "| `finalized` | `finaliz` | `finalize` | Removed `ed`, unaware of morphological “restore e” rule. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Why This Happens\n",
    "\n",
    "Your regex:\n",
    "\n",
    "```python\n",
    "regexp = RegexpStemmer(\n",
    "    regexp=r'(ing|ly|ed|ious|ies|ive|es|s|ment)$',\n",
    "    min=3\n",
    ")\n",
    "```\n",
    "\n",
    "... blindly removes suffixes, so it:\n",
    "Removes endings like `-ed`, `-s`, `-ing`, etc.\n",
    "Does not know when to add back missing letters (like `y` or `e`).\n",
    "Does not skip partial matches (e.g., removes `s` in `happiness`).\n",
    "This makes it fast ⚡ but linguistically dumb 🤖.\n",
    "\n",
    "### 🧮 Mathematically\n",
    "\n",
    "$$\n",
    "\\text{RegexpStemmer}(w) =\n",
    "\\begin{cases}\n",
    "w - \\text{regex\\_suffix}, & \\text{if the suffix matches the regex pattern at the end of } w \\\\\n",
    "w, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Unlike **Porter** or **Snowball**, it lacks post-processing rules such as:\n",
    "\n",
    "$$\n",
    "\\text{if } w' \\text{ ends with } \"i\" \\Rightarrow \\text{replace with } \"y\"\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb29835f",
   "metadata": {},
   "source": [
    "## 🧊 Snowball Stemmer (aka Porter2)\n",
    "\n",
    "The **Snowball Stemmer** — also known as **Porter2 Stemmer** — is an improved, modernized version of the classic **Porter Stemmer**.  \n",
    "It was introduced by **Martin Porter** himself as part of the **Snowball framework** for multilingual stemming.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Overview\n",
    "\n",
    "Unlike the original Porter Stemmer, which was written in English-only rules,  \n",
    "the Snowball version generalizes the same concept for **multiple languages** using a cleaner, more maintainable rule definition syntax.\n",
    "\n",
    "It uses a **larger rule set**, **stricter conditions**, and **refined suffix handling**, making it:\n",
    "- More **consistent** across morphological cases  \n",
    "- Slightly **less aggressive** than Lancaster, but more **accurate** than Porter  \n",
    "- **Multilingual**, supporting languages such as `english`, `french`, `spanish`, `german`, `italian`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Working Logic\n",
    "\n",
    "1️⃣ **Input**: token $w$  \n",
    "2️⃣ **Identify** suffixes and endings based on the target language  \n",
    "3️⃣ **Apply** language-specific morphological rules  \n",
    "4️⃣ **Return** truncated stem $s$\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "\\text{SnowballStemmer}(w) =\n",
    "\\begin{cases}\n",
    "w - \\text{language\\_specific\\_suffix}, & \\text{if rule applies for language} \\\\\n",
    "w, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Example (English)\n",
    "\n",
    "```python\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "\n",
    "for w in [\"connected\", \"connections\", \"studies\", \"studying\", \"happiness\", \"generalization\"]:\n",
    "    print(f\"{w:>15}  →  {snowball.stem(w)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "### 🧩 Sample Output\n",
    "\n",
    "| Word           | Snowball Stem |\n",
    "| :------------- | :------------ |\n",
    "| connected      | connect       |\n",
    "| connections    | connect       |\n",
    "| studies        | studi         |\n",
    "| studying       | studi         |\n",
    "| happiness      | happi         |\n",
    "| generalization | general       |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Features & Advantages\n",
    "\n",
    "| Feature                          | Description                                     |\n",
    "| :------------------------------- | :---------------------------------------------- |\n",
    "| 🌍 **Multilingual support**      | Works for multiple languages                    |\n",
    "| ⚖️ **Balanced approach**         | Avoids over-stemming seen in Lancaster          |\n",
    "| 📏 **Improved rule definitions** | Simpler and more uniform rule syntax            |\n",
    "| ⚡ **Fast and lightweight**       | Similar performance to Porter                   |\n",
    "| 🧩 **Stable results**            | Produces consistent stems across similar tokens |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Limitations\n",
    "\n",
    "| Limitation                        | Description                                                    |\n",
    "| :-------------------------------- | :------------------------------------------------------------- |\n",
    "| ❌ Still heuristic-based           | No understanding of real word meaning                          |\n",
    "| ❌ Not lemmatization               | “studies” → “studi” (non-word)                                 |\n",
    "| ❌ English bias                    | Best for Indo-European languages                               |\n",
    "| ⚠️ Slight differences from Porter | Can produce smaller or larger stems depending on rule ordering |\n",
    "\n",
    "---\n",
    "\n",
    "### 📘 When to Use Snowball Stemmer\n",
    "- ✅ Choose Snowball Stemmer when:\n",
    "    - You need a fast and accurate rule-based stemmer for English or European languages\n",
    "    - You want consistency and clarity in stemming behavior\n",
    "    - You’re preprocessing for Information Retrieval, Topic Modeling, or Search Indexing\n",
    "- ❌ Avoid when:\n",
    "    - You need linguistically correct root forms (→ use WordNet Lemmatizer)\n",
    "    - You’re working on languages unsupported by Snowball\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Summary\n",
    "\n",
    "| Aspect           | Porter               | Snowball (Porter2)     |\n",
    "| :--------------- | :------------------- | :--------------------- |\n",
    "| Rules            | ~60 hardcoded        | ~85 structured         |\n",
    "| Accuracy         | Moderate             | High                   |\n",
    "| Language support | English only         | Multi-language         |\n",
    "| Aggressiveness   | Moderate             | Controlled             |\n",
    "| Output example   | “studying” → “studi” | “studying” → “studi”   |\n",
    "| Use case         | IR tasks             | IR + NLP preprocessing |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Mathematical Insight\n",
    "\n",
    "$$\n",
    "\\text{SnowballStemmer}(w) =\n",
    "\\text{normalize}\\Bigg(\n",
    "w - \n",
    "\\sum_{i=1}^{n}\n",
    "\\text{suffix}_i \\cdot \n",
    "\\mathbb{1}_{\\text{rule}_i(w)}\n",
    "\\Bigg)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbb{1}_{\\text{rule}_i(w)}$ is an **indicator function**:\n",
    "  $$\n",
    "  \\mathbb{1}_{\\text{rule}_i(w)} =\n",
    "  \\begin{cases}\n",
    "  1, & \\text{if linguistic rule } i \\text{ applies to } w \\\\\n",
    "  0, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- $\\text{suffix}_i$ represents each possible removable suffix.  \n",
    "- $\\text{normalize}(\\cdot)$ denotes **post-processing** (like removing double letters or trailing vowels).  \n",
    "\n",
    "---\n",
    "\n",
    "📘 **Intuition**\n",
    "\n",
    "The Snowball Stemmer applies a series of $n$ language-specific rules.  \n",
    "Each rule checks if a word $w$ matches a condition (via $\\text{rule}_i$).  \n",
    "If true ($\\mathbb{1}=1$), it removes the corresponding $\\text{suffix}_i$,  \n",
    "and then the result is normalized to ensure consistency across derived stems.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b2afeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Snowball Stemmer (Porter2) Results:\n",
      "\n",
      "        connect  →  connect\n",
      "      connected  →  connect\n",
      "     connection  →  connect\n",
      "    connections  →  connect\n",
      "     connecting  →  connect\n",
      "          study  →  studi\n",
      "        studies  →  studi\n",
      "       studying  →  studi\n",
      "        studied  →  studi\n",
      "          happy  →  happi\n",
      "      happiness  →  happi\n",
      "      unhappily  →  unhappili\n",
      "   organization  →  organ\n",
      "       organize  →  organ\n",
      "      organized  →  organ\n",
      "     organizing  →  organ\n",
      "     generalize  →  general\n",
      "    generalized  →  general\n",
      " generalization  →  general\n",
      "             go  →  go\n",
      "          going  →  go\n",
      "           goes  →  goe\n",
      "           gone  →  gone\n",
      "         better  →  better\n",
      "           best  →  best\n",
      "     relational  →  relat\n",
      "         relate  →  relat\n",
      "        related  →  relat\n",
      "       relating  →  relat\n",
      "           cats  →  cat\n",
      "          boxes  →  box\n",
      "           mice  →  mice\n",
      "    practically  →  practic\n",
      "      practical  →  practic\n",
      "    practicable  →  practic\n",
      "        writing  →  write\n",
      "         writes  →  write\n",
      "    programming  →  program\n",
      "       programs  →  program\n",
      "        history  →  histori\n",
      "        finally  →  final\n",
      "      finalized  →  final\n"
     ]
    }
   ],
   "source": [
    "# ✅ 2. Snowball Stemmer — also called Porter2\n",
    "#    Developed as an improved version of Porter with:\n",
    "#    - Better consistency\n",
    "#    - Multi-language support\n",
    "#    - More transparent rules\n",
    "             \n",
    "snowball = SnowballStemmer(language=\"english\")\n",
    "\n",
    "print(\"\\n🔹 Snowball Stemmer (Porter2) Results:\\n\")\n",
    "for w in tokens:\n",
    "    stem = snowball.stem(w)\n",
    "    print(f\"{w:>15}  →  {stem}\")\n",
    "\n",
    "# 🧠 Notes:\n",
    "# - Handles 'happiness' → 'happi' like Porter\n",
    "# - More consistent rule application\n",
    "# - Supports many languages: Arabic, Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Norwegian,\n",
    "#   Portuguese, Romanian, Russian, Spanish and Swedish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391ad78",
   "metadata": {},
   "source": [
    "## ⚠️ Why Some Snowball Stemmer Outputs Look Incorrect\n",
    "\n",
    "Although **Snowball Stemmer (Porter2)** is an improvement over Porter,  \n",
    "it’s still a **rule-based stemmer** — not a lemmatizer.  \n",
    "It applies **heuristic truncation rules**, which means it:\n",
    "- Strips **common suffixes** like *-ed*, *-ing*, *-ly*, *-ation*, *-ize*, etc.\n",
    "- Does **not** restore missing characters (like `y`, `e`)\n",
    "- Treats **word derivation families** as morphologically equivalent\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Example Analysis\n",
    "\n",
    "| Word | Output | Expected Lemma | Explanation |\n",
    "|:--|:--|:--|:--|\n",
    "| `studies` | `studi` | `study` | Replaces *-ies* → *-i*, doesn’t restore *y* |\n",
    "| `studied` | `studi` | `study` | Removes *-ed*, no post-fix rule to add *y* |\n",
    "| `happiness` | `happi` | `happy` | Removes *-ness*, doesn’t change *i* → *y* |\n",
    "| `unhappily` | `unhappili` | `unhappy` | Removes *-ly*, but also drops *y → i* from earlier suffix logic |\n",
    "| `organization` | `organ` | `organize` | Removes derivational suffix *-ization* → *ize*, then normalizes to base “organ” |\n",
    "| `organized` | `organ` | `organize` | Removes *-ed*, then *-ize*, collapsing both |\n",
    "| `generalization` | `general` | `generalize` | Removes *-ization*, truncating the base |\n",
    "| `goes` | `goe` | `go` | Strips *-es* but lacks *restore e → o* fixup |\n",
    "| `history` | `histori` | `history` | Treats *-y* as possible derivational ending and removes *y → i* |\n",
    "| `practically` | `practic` | `practical` | Removes *-ally* (double rule: *al + ly*) |\n",
    "| `writing` | `write` | `write` | ✅ Correct — known morphological case handled well |\n",
    "| `programming` | `program` | `program` | ✅ Correct — doubled consonant handled |\n",
    "| `finalized` | `final` | `finalize` | Removes *-ized*, returns the root *final* |\n",
    "| `finally` | `final` | `final` | ✅ Expected stem; adverb stripped correctly |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Why These Happen — Mechanism of Snowball (Porter2)\n",
    "\n",
    "The **Porter2 algorithm** operates in **five sequential phases**  \n",
    "where each step applies pattern-based transformations.\n",
    "\n",
    "$$\n",
    "\\text{SnowballStemmer}(w) = \n",
    "\\text{normalize}\\Big(\n",
    "w - \\sum_{i=1}^{n} \\text{suffix}_i \\cdot \\mathbb{1}_{\\text{rule}_i(w)}\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "Each rule ($\\text{rule}_i$):\n",
    "- Checks if the word ends with a specific suffix (e.g., `-ed`, `-ing`, `-ation`)\n",
    "- Verifies a *minimum stem length* condition\n",
    "- Applies replacements like:  \n",
    "  - `ies → i`  \n",
    "  - `ization → ize`  \n",
    "  - `ational → ate`  \n",
    "  - `fulness → ful`\n",
    "\n",
    "🧠 However:\n",
    "- There is **no restoration rule** (like *i → y*, *add back e*)  \n",
    "- There is **no dictionary check** to verify if the result is a valid word  \n",
    "- It assumes words sharing the same morphological stem should reduce to the same root\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Design Philosophy (Intentional Behavior)\n",
    "\n",
    "The Snowball stemmer intentionally produces **canonical base forms**,  \n",
    "not **valid English words** — because it’s built for **Information Retrieval (IR)**, not grammar.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Use Case | Goal |\n",
    "|:--|:--|\n",
    "| Search “connections” | Should match documents containing “connect”, “connecting”, “connection” |\n",
    "| Text classification | Token frequency of “connect” should count all related forms |\n",
    "| Linguistic correctness | ❌ Not required |\n",
    "\n",
    "Hence, truncations like “studi”, “happi”, “organ” are **acceptable stems** for IR,  \n",
    "since they merge semantically related words into one root.\n",
    "\n",
    "---\n",
    "\n",
    "### 📘 Summary: Stemming ≠ Lemmatization\n",
    "\n",
    "| Aspect | Stemming (Porter/Snowball) | Lemmatization (WordNet) |\n",
    "|:--|:--|:--|\n",
    "| Logic | Heuristic rules | Morphological + dictionary |\n",
    "| Output | Non-word stems | Valid dictionary words |\n",
    "| Context awareness | ❌ None | ✅ Uses POS tags |\n",
    "| Example | “studies” → “studi” | “studies” → “study” |\n",
    "| Use case | IR / Search / Topic Modeling | Linguistic / Semantic tasks |\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Key Takeaway\n",
    "\n",
    "- ❗ **Incorrect-looking stems ≠ wrong** — they’re **intentional** truncations.  \n",
    "- ❗ The **Snowball Stemmer** doesn’t “understand” language — it only applies suffix heuristics.  \n",
    "- ✅ For **real-word roots**, move to **WordNet Lemmatizer** (which we’ll cover next).\n",
    "\n",
    "---\n",
    "\n",
    "📚 **In one line:**\n",
    "\n",
    "> *Snowball stemmer trims words for equality, not for readability.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0e974",
   "metadata": {},
   "source": [
    "## ⚖️ Porter vs Snowball Stemmer — Behavior on `fairly` and `sportingly`\n",
    "\n",
    "We’ll compare how the **Porter Stemmer** and **Snowball (Porter2) Stemmer** process  \n",
    "two adverbial words: `fairly` and `sportingly`.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Code Used\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "\n",
    "print(\"Porter:\", porter.stem(\"fairly\"), \",\", porter.stem(\"sportingly\"))\n",
    "print(\"Snowball:\", snowball.stem(\"fairly\"), \",\", snowball.stem(\"sportingly\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3bffd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter: fairli , sportingli\n",
      "Snowball: fair , sport\n"
     ]
    }
   ],
   "source": [
    "print(\"Porter:\", porter.stem(\"fairly\"), \",\", porter.stem(\"sportingly\"))\n",
    "print(\"Snowball:\", snowball.stem(\"fairly\"), \",\", snowball.stem(\"sportingly\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf66246",
   "metadata": {},
   "source": [
    "### 🔍 Output\n",
    "\n",
    "| Word | **Porter Stemmer** | **Snowball Stemmer (Porter2)** | ✅ **Explanation** |\n",
    "|:--|:--|:--|:--|\n",
    "| `fairly` | `fairli` | `fair` | Porter replaces *y → i* (old rule); Snowball cleanly removes `-ly` |\n",
    "| `sportingly` | `sportingli` | `sport` | Porter only removes `-ly` (leaving `sportingli`); Snowball removes the full `-ingly` suffix |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Why Porter Gives “fairli” and “sportingli”\n",
    "\n",
    "The **Porter Stemmer** applies a **fixed, rule-based sequence** without understanding grammar or word parts.  \n",
    "It mechanically replaces or trims suffixes based on pattern rules.\n",
    "\n",
    "In **Step 1c** of Porter’s algorithm:\n",
    "\n",
    "> (*v*) Y → I  \n",
    "> If the word contains a vowel and ends with *y*, replace *y* with *i*.\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\text{fairly} \\xrightarrow[\\text{remove } -ly]{} \\text{fairy} \\xrightarrow[\\text{Y→I}]{} \\text{fairli}\n",
    "$$\n",
    "\n",
    "And for *sportingly*:\n",
    "\n",
    "$$\n",
    "\\text{sportingly} \\xrightarrow[\\text{remove } -ly]{} \\text{sportingli}\n",
    "$$\n",
    "\n",
    "✅ Correct per its rules,  \n",
    "❌ but not linguistically meaningful (non-word outputs).\n",
    "\n",
    "---\n",
    "\n",
    "### 🧊 Why Snowball (Porter2) Is Better\n",
    "\n",
    "**Snowball (Porter2)** improves Porter’s logic by:\n",
    "- Recognizing multi-suffix patterns (*-ly*, *-ingly*, *-edly*)  \n",
    "- Avoiding unnecessary *y → i* replacements  \n",
    "- Applying **normalization** steps for smoother stems  \n",
    "\n",
    "Hence:\n",
    "\n",
    "$$\n",
    "\\text{SnowballStemmer(\"fairly\")} = \\text{\"fair\"}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{SnowballStemmer(\"sportingly\")} = \\text{\"sport\"}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Formal Difference\n",
    "\n",
    "$$\n",
    "\\text{PorterStemmer}(w) =\n",
    "\\text{apply\\_rules}(w, \\text{fixed\\_suffixes})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{SnowballStemmer}(w) =\n",
    "\\text{normalize}\\Big(\n",
    "w -\n",
    "\\sum_{i=1}^{n}\n",
    "\\text{suffix}_i \\cdot\n",
    "\\mathbb{1}_{\\text{rule}_i(w)}\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\mathbb{1}_{\\text{rule}_i(w)} = 1$ if linguistic rule *i* applies  \n",
    "- $\\text{normalize}(\\cdot)$ handles cleanup like removing double consonants or restoring vowels\n",
    "\n",
    "---\n",
    "\n",
    "### 📘 Summary\n",
    "\n",
    "| Aspect | **Porter Stemmer** | **Snowball Stemmer (Porter2)** |\n",
    "|:--|:--|:--|\n",
    "| Rule design | Fixed, legacy English rules | Modular, modern rule system |\n",
    "| Handles `-ly` / `-ingly` | Partial (→ “sportingli”) | ✅ Robust (→ “sport”) |\n",
    "| `fairly` → | `fairli` | `fair` |\n",
    "| `sportingly` → | `sportingli` | `sport` |\n",
    "| Accuracy | ❌ Often non-word | ✅ Linguistically cleaner |\n",
    "| Language support | English only | Multilingual (English, German, Spanish, etc.) |\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Takeaway\n",
    "\n",
    "> The **Porter Stemmer** applies old mechanical rules — fast but crude.  \n",
    "> The **Snowball (Porter2)** Stemmer refines these rules for readability, consistency, and multilingual support.  \n",
    "> For **true dictionary words**, the next step is **WordNet Lemmatization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca88be",
   "metadata": {},
   "source": [
    "## ⚡ Lancaster Stemmer — The Aggressive Rule-Based Stemmer\n",
    "\n",
    "The **Lancaster Stemmer** (also known as the **Paice/Husk Stemmer**) is a **very aggressive, iterative rule-based** stemmer.  \n",
    "It was developed by **Chris Paice (1990)** and applies a series of **deletion and substitution rules** until no more rules can be applied.  \n",
    "\n",
    "It’s fast ⚡ and simple, but often **over-stems** words — chopping too much and sometimes merging unrelated terms.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Overview\n",
    "\n",
    "- Developed after Porter, designed to be **simpler and faster**  \n",
    "- Uses a **rule lookup table** of around **120 rules**  \n",
    "- Applies rules **iteratively** (multiple passes) until no further reduction is possible  \n",
    "- Each rule defines:\n",
    "  - a **suffix pattern** to remove  \n",
    "  - a **replacement** (optional)  \n",
    "  - and whether to **continue or stop**\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 How It Works\n",
    "\n",
    "Each rule in Lancaster has the general form:\n",
    "\n",
    "$$\n",
    "\\text{<ending><condition><replacement><continue>}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Example Rules\n",
    "\n",
    "```text\n",
    "tion4>     → remove \"tion\" if word length > 4  \n",
    "ed4>       → remove \"ed\" if word length > 4  \n",
    "y>i        → replace \"y\" with \"i\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Sample Output\n",
    "\n",
    "| Word | **Lancaster Stem** | ✅ **Observation** |\n",
    "|:--|:--|:--|\n",
    "| `connection` | `connect` | ✅ Good |\n",
    "| `connected` | `connect` | ✅ Good |\n",
    "| `connecting` | `connect` | ✅ Good |\n",
    "| `organization` | `organ` | ⚠️ Over-stemmed (lost “ize” meaning) |\n",
    "| `organized` | `organ` | ⚠️ Same — merges with “organ” |\n",
    "| `happiness` | `happy` | ✅ Accurate |\n",
    "| `practically` | `practic` | ⚠️ Slight truncation |\n",
    "| `studies` | `study` | ✅ Correct |\n",
    "| `generalization` | `gener` | ⚠️ Too short, over-stemmed |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Characteristics & Limitations\n",
    "\n",
    "| Aspect | Description |\n",
    "|:--|:--|\n",
    "| ⚡ **Aggressive** | Removes large parts of the word; may over-stem |\n",
    "| 🔁 **Iterative** | Continues applying rules until no further matches |\n",
    "| ❌ **Unstable** | Small changes in input can cause big differences in output |\n",
    "| 🧩 **Short stems** | Often results in very short root forms (e.g., “compute”, “computer” → “comput”) |\n",
    "| 📚 **No context** | Doesn’t distinguish noun/verb forms or semantics |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Mathematical View (KaTeX)\n",
    "\n",
    "$$\n",
    "\\text{LancasterStemmer}(w) =\n",
    "\\text{Iterate}\\Big(\n",
    "w - \\sum_{i=1}^{n}\n",
    "\\text{rule}_i(w)\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- Each $\\text{rule}_i$ is a transformation (delete/replace suffix)  \n",
    "- The process **continues until convergence** (no rule applies)\n",
    "\n",
    "That is:\n",
    "\n",
    "$$\n",
    "w_{t+1} = \\text{apply\\_rule}(w_t)\n",
    "\\quad \\text{until} \\quad\n",
    "w_{t+1} = w_t\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Strengths vs Weaknesses\n",
    "\n",
    "| ✅ Strengths | ⚠️ Weaknesses |\n",
    "|:--|:--|\n",
    "| Very fast and simple | Over-stemming is common |\n",
    "| Small code footprint | Can merge unrelated words |\n",
    "| Iterative and deterministic | Not linguistically aware |\n",
    "| Handles many English suffixes | Poor accuracy for complex derivations |\n",
    "\n",
    "---\n",
    "\n",
    "### 📘 Summary\n",
    "\n",
    "| Feature | Description |\n",
    "|:--|:--|\n",
    "| Algorithm | Rule-based, iterative |\n",
    "| Developer | Chris Paice (1990) |\n",
    "| Aggressiveness | 🔥 Very high |\n",
    "| Accuracy | ⚠️ Moderate |\n",
    "| Iterative | Yes |\n",
    "| Output validity | Often non-dictionary stems |\n",
    "| Example | `organization → organ` |\n",
    "| Use case | When **speed > accuracy** (e.g., keyword compression) |\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Takeaway\n",
    "\n",
    "> The **Lancaster Stemmer** is the fastest and most aggressive among classical stemmers.  \n",
    "> It’s suitable for quick **indexing or keyword matching**, but **not recommended for semantic NLP tasks**.  \n",
    ">  \n",
    "> For balanced results — use **Snowball (Porter2)**.  \n",
    "> For linguistically valid roots — use **WordNet Lemmatizer** next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc77dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Lancaster Stemmer Results (Aggressive):\n",
      "\n",
      "        connect  →  connect\n",
      "      connected  →  connect\n",
      "     connection  →  connect\n",
      "    connections  →  connect\n",
      "     connecting  →  connect\n",
      "          study  →  study\n",
      "        studies  →  study\n",
      "       studying  →  study\n",
      "        studied  →  study\n",
      "          happy  →  happy\n",
      "      happiness  →  happy\n",
      "      unhappily  →  unhappy\n",
      "   organization  →  org\n",
      "       organize  →  org\n",
      "      organized  →  org\n",
      "     organizing  →  org\n",
      "     generalize  →  gen\n",
      "    generalized  →  gen\n",
      " generalization  →  gen\n",
      "             go  →  go\n",
      "          going  →  going\n",
      "           goes  →  goe\n",
      "           gone  →  gon\n",
      "         better  →  bet\n",
      "           best  →  best\n",
      "     relational  →  rel\n",
      "         relate  →  rel\n",
      "        related  →  rel\n",
      "       relating  →  rel\n",
      "           cats  →  cat\n",
      "          boxes  →  box\n",
      "           mice  →  mic\n",
      "    practically  →  pract\n",
      "      practical  →  pract\n",
      "    practicable  →  pract\n",
      "        writing  →  writ\n",
      "         writes  →  writ\n",
      "    programming  →  program\n",
      "       programs  →  program\n",
      "        history  →  hist\n",
      "        finally  →  fin\n",
      "      finalized  →  fin\n"
     ]
    }
   ],
   "source": [
    "# ⚠️ 3. Lancaster Stemmer — very aggressive\n",
    "#     Often chops too much, merging unrelated words.\n",
    "#     But it’s extremely fast and compact.\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "print(\"\\n🔹 Lancaster Stemmer Results (Aggressive):\\n\")\n",
    "for w in tokens:\n",
    "    stem = lancaster.stem(w)\n",
    "    print(f\"{w:>15}  →  {stem}\")\n",
    "\n",
    "# 🧠 Notes:\n",
    "# - Often shorter stems than Porter/Snowball (e.g., 'connection' → 'connect')\n",
    "# - Can over-stem: 'practically' → 'practic' or even 'prac'\n",
    "# - Useful when you want maximum vocabulary compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e06a3",
   "metadata": {},
   "source": [
    "## ⚠️ Why Some Lancaster Stemmer Results Look Incorrect\n",
    "\n",
    "The **Lancaster Stemmer** is extremely **aggressive** and **iterative**, which makes it *fast* but often **linguistically inaccurate**.  \n",
    "Unlike Porter or Snowball, it doesn’t just remove a suffix — it applies **a chain of truncation and replacement rules repeatedly**,  \n",
    "until the word can no longer be shortened.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Example Output\n",
    "\n",
    "| Word | **Lancaster Stem** | ✅ **Expected** | ⚙️ **Observation** |\n",
    "|:--|:--|:--|:--|\n",
    "| `organization` | `org` | `organize` | Over-stemmed — repeatedly truncated |\n",
    "| `generalization` | `gen` | `generalize` | Over-stemmed; multiple suffix rules applied |\n",
    "| `unhappily` | `unhappy` | `unhappy` | ✅ Correct — stripped “-ily” |\n",
    "| `finally` | `fin` | `final` | Over-stemmed; lost meaningful suffix |\n",
    "| `gone` | `gon` | `go` | Partial removal of “e” only |\n",
    "| `mice` | `mic` | `mouse` | ❌ Wrong — no understanding of irregular plurals |\n",
    "| `better` | `bet` | `good` | ❌ Not aware of comparative/synonymic meaning |\n",
    "| `writing` | `writ` | `write` | ✅ Acceptable stem |\n",
    "| `organization` | `org` | `organize` | ⚠️ Over-stemmed — intended for text compression |\n",
    "| `practical` | `pract` | `practical` | ⚠️ Truncated aggressively |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Why It Happens\n",
    "\n",
    "The **Lancaster Stemmer** uses a compact **rule table of about 120 transformation rules**.  \n",
    "Each rule defines:\n",
    "- a **suffix** to remove,\n",
    "- a **minimum stem length**, and  \n",
    "- whether to **continue or stop** processing.\n",
    "\n",
    "For example, some of its rules look like:\n",
    "\n",
    "```text\n",
    "ion4>      → remove \"ion\" if length > 4\n",
    "ize3>      → remove \"ize\" if length > 3\n",
    "al4>       → remove \"al\" if length > 4\n",
    "e>         → remove \"e\"\n",
    "y>i        → replace \"y\" with \"i\"\n",
    "```\n",
    "\n",
    "These are applied **iteratively**.  \n",
    "So a word like **“organization”** goes through multiple transformations:\n",
    "\n",
    "```text\n",
    "organization \n",
    "→ organize\n",
    "→ organ\n",
    "→ org\n",
    "```\n",
    "\n",
    "✅ The algorithm stops **only when no more rules match**,  \n",
    "hence the stems are often **shorter than expected**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Mathematical Representation \n",
    "\n",
    "$$\n",
    "w_{t+1} = f(w_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "f(w) = \n",
    "\\begin{cases}\n",
    "w - \\text{suffix}, & \\text{if a rule applies} \\\\\n",
    "\\text{replace}(w), & \\text{if replacement condition met} \\\\\n",
    "w, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The process continues until convergence:\n",
    "\n",
    "$$\n",
    "\\text{LancasterStemmer}(w) = \\lim_{t \\to T} f^{(t)}(w)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 Key Differences from Porter / Snowball\n",
    "\n",
    "| Feature | **Lancaster** | **Porter** | **Snowball (Porter2)** |\n",
    "|:--|:--|:--|:--|\n",
    "| Rule type | Iterative table lookup | Sequential suffix removal | Structured rule system |\n",
    "| Aggressiveness | 🔥 Very high | Medium | Controlled |\n",
    "| Iterative passes | ✅ Yes | ❌ No | ❌ No |\n",
    "| Output validity | Often non-word | Sometimes non-word | Usually readable |\n",
    "| `organization` | `org` | `organ` | `organ` |\n",
    "| `generalization` | `gen` | `general` | `general` |\n",
    "| `practically` | `pract` | `practic` | `practic` |\n",
    "| Speed | ⚡ Fastest | ⚡ Fast | ⚡ Fast |\n",
    "| Use case | IR compression | General NLP | Balanced NLP |\n",
    "\n",
    "---\n",
    "\n",
    "### 📘 When Results Are “Incorrect”\n",
    "\n",
    "They look incorrect because:\n",
    "\n",
    "1. The algorithm has **no linguistic understanding** — it only applies string patterns.  \n",
    "2. Rules can **cascade**, meaning multiple suffix rules can apply in sequence.  \n",
    "3. It doesn’t check whether the **resulting stem** is a valid English word.  \n",
    "4. It was designed for **Information Retrieval (IR)**, not grammatical correctness.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ When It’s Still Useful\n",
    "\n",
    "| Use Case | Reason |\n",
    "|:--|:--|\n",
    "| **Search indexing** | Fewer unique stems → faster lookups |\n",
    "| **Text deduplication** | Groups related word forms aggressively |\n",
    "| **Keyword extraction** | Reduces inflectional variations |\n",
    "| **Count-based NLP models** | Smaller vocabulary → more efficient vectorization |\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Summary\n",
    "\n",
    "> The **Lancaster Stemmer** is extremely fast but highly aggressive.  \n",
    "> It’s ideal for **text retrieval** and **index compression**,  \n",
    "> but unsuitable for linguistically sensitive NLP applications.  \n",
    ">\n",
    "> ✅ Use **Snowball (Porter2)** for balanced stemming.  \n",
    "> ✅ Use **WordNet Lemmatizer** for meaningful dictionary words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bead5004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKEN           PORTER     SNOWBALL   LANCASTER  REGEXP        \n",
      "---------------------------------------------------------------\n",
      "connect         connect    connect    connect    connect       \n",
      "connected       connect    connect    connect    connect       \n",
      "connection      connect    connect    connect    connection    \n",
      "connections     connect    connect    connect    connection    \n",
      "connecting      connect    connect    connect    connect       \n",
      "study           studi      studi      study      study         \n",
      "studies         studi      studi      study      stud          \n",
      "studying        studi      studi      study      study         \n",
      "studied         studi      studi      study      studi         \n",
      "happy           happi      happi      happy      happy         \n",
      "happiness       happi      happi      happy      happines      \n",
      "unhappily       unhappili  unhappili  unhappy    unhappi       \n",
      "organization    organ      organ      org        organization  \n",
      "organize        organ      organ      org        organize      \n",
      "organized       organ      organ      org        organiz       \n",
      "organizing      organ      organ      org        organiz       \n",
      "generalize      gener      general    gen        generalize    \n",
      "generalized     gener      general    gen        generaliz     \n",
      "generalization  gener      general    gen        generalization\n",
      "go              go         go         go         go            \n",
      "going           go         go         going      go            \n",
      "goes            goe        goe        goe        go            \n",
      "gone            gone       gone       gon        gone          \n",
      "better          better     better     bet        better        \n",
      "best            best       best       best       best          \n",
      "relational      relat      relat      rel        relational    \n",
      "relate          relat      relat      rel        relate        \n",
      "related         relat      relat      rel        relat         \n",
      "relating        relat      relat      rel        relat         \n",
      "cats            cat        cat        cat        cat           \n",
      "boxes           box        box        box        box           \n",
      "mice            mice       mice       mic        mice          \n",
      "practically     practic    practic    pract      practical     \n",
      "practical       practic    practic    pract      practical     \n",
      "practicable     practic    practic    pract      practicable   \n",
      "writing         write      write      writ       writ          \n",
      "writes          write      write      writ       writ          \n",
      "programming     program    program    program    programm      \n",
      "programs        program    program    program    program       \n",
      "history         histori    histori    hist       history       \n",
      "finally         final      final      fin        final         \n",
      "finalized       final      final      fin        finaliz       \n"
     ]
    }
   ],
   "source": [
    "# 🧮 Compare outputs of all stemmers side by side for quick analysis\n",
    "\n",
    "def compare_stemmers(word):\n",
    "    return {\n",
    "        \"token\": word,\n",
    "        \"Porter\": porter.stem(word),\n",
    "        \"Snowball\": snowball.stem(word),\n",
    "        \"Lancaster\": lancaster.stem(word),\n",
    "        \"Regexp\": regexp.stem(word),\n",
    "    }\n",
    "\n",
    "results = [compare_stemmers(w) for w in tokens]\n",
    "\n",
    "# Compute column widths for nice console alignment\n",
    "cols = [\"token\", \"Porter\", \"Snowball\", \"Lancaster\", \"Regexp\"]\n",
    "widths = {c: max(len(c), max(len(str(r[c])) for r in results)) for c in cols}\n",
    "\n",
    "# Header\n",
    "header = \"  \".join(c.upper().ljust(widths[c]) for c in cols)\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# Rows\n",
    "for row in results:\n",
    "    print(\"  \".join(str(row[c]).ljust(widths[c]) for c in cols))\n",
    "\n",
    "# 💡 Alternative visualization:\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(results)\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95412902",
   "metadata": {},
   "source": [
    "## 📊 Observations and Comparison\n",
    "\n",
    "| Word | Porter | Snowball | Lancaster | Regexp | Remarks |\n",
    "|:----|:-------|:---------|:-----------|:--------|:--------|\n",
    "| connected | connect | connect | connect | connect | ✅ Consistent |\n",
    "| studies | studi | studi | study | study | ⚠️ Minor differences |\n",
    "| happiness | happi | happi | happy | happi | ✅ Close results |\n",
    "| organizing | organ | organ | organ | organiz | ⚠️ Regexp retains ‘z’ |\n",
    "| practical | practic | practic | pract | practic | ⚠️ Lancaster over-stems |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Over-stemming vs Under-stemming\n",
    "\n",
    "- **Over-stemming:** unrelated words merge into one root  \n",
    "  e.g., “practicable” and “practical” → both → “practic”\n",
    "- **Under-stemming:** related words fail to merge  \n",
    "  e.g., “relational” and “relate” remain different stems\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Best Practices\n",
    "\n",
    "- Use **Porter** or **Snowball** for English text preprocessing pipelines.  \n",
    "- Use **Lancaster** only for **aggressive vocabulary compression** (like search engines).  \n",
    "- Use **Regexp** for **custom domains** — especially if you know common suffix patterns.  \n",
    "- For linguistically valid roots → switch to **lemmatization**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Mathematical Recap\n",
    "\n",
    "$$\n",
    "\\text{Stem: } \\mathcal{W} \\to \\mathcal{S}, \\quad s = \\text{stem}(w)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Word Family: } \\{\\text{connect}, \\text{connected}, \\text{connections}, \\text{connecting}\\} \n",
    "\\xrightarrow{\\text{stem}} \\{\\text{connect}\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e91063",
   "metadata": {},
   "source": [
    "## 🧩 WordNet Lemmatization — Getting True Dictionary Words\n",
    "\n",
    "Unlike **stemming**, which just chops off suffixes,  \n",
    "**lemmatization** uses **vocabulary + grammar rules** to find the *actual dictionary root* (lemma) of a word.\n",
    "\n",
    "It considers:\n",
    "- Part of Speech (POS) — noun, verb, adjective, adverb  \n",
    "- Morphological rules (e.g., *studies → study*, *better → good*)  \n",
    "- WordNet lexical database for valid words\n",
    "\n",
    "---\n",
    "\n",
    "### 💻 Example Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ce0f917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/psundara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/psundara/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download WordNet if not already done\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b095d2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Lemmatization Examples:\n",
      "\n",
      "   connect → connect\n",
      " connected → connected\n",
      "connection → connection\n",
      "connections → connection\n",
      "connecting → connecting\n",
      "     study → study\n",
      "   studies → study\n",
      "  studying → studying\n",
      "   studied → studied\n",
      "     happy → happy\n",
      " happiness → happiness\n",
      " unhappily → unhappily\n",
      "organization → organization\n",
      "  organize → organize\n",
      " organized → organized\n",
      "organizing → organizing\n",
      "generalize → generalize\n",
      "generalized → generalized\n",
      "generalization → generalization\n",
      "        go → go\n",
      "     going → going\n",
      "      goes → go\n",
      "      gone → gone\n",
      "    better → better\n",
      "      best → best\n",
      "relational → relational\n",
      "    relate → relate\n",
      "   related → related\n",
      "  relating → relating\n",
      "      cats → cat\n",
      "     boxes → box\n",
      "      mice → mouse\n",
      "practically → practically\n",
      " practical → practical\n",
      "practicable → practicable\n",
      "   writing → writing\n",
      "    writes → writes\n",
      "programming → programming\n",
      "  programs → program\n",
      "   history → history\n",
      "   finally → finally\n",
      " finalized → finalized\n",
      "\n",
      "🔹 With POS Tags:\n",
      "\n",
      "connect (verb): connect\n",
      "connect (adjective): connect\n",
      "connected (verb): connect\n",
      "connected (adjective): connected\n",
      "connection (verb): connection\n",
      "connection (adjective): connection\n",
      "connections (verb): connections\n",
      "connections (adjective): connections\n",
      "connecting (verb): connect\n",
      "connecting (adjective): connecting\n",
      "study (verb): study\n",
      "study (adjective): study\n",
      "studies (verb): study\n",
      "studies (adjective): studies\n",
      "studying (verb): study\n",
      "studying (adjective): studying\n",
      "studied (verb): study\n",
      "studied (adjective): studied\n",
      "happy (verb): happy\n",
      "happy (adjective): happy\n",
      "happiness (verb): happiness\n",
      "happiness (adjective): happiness\n",
      "unhappily (verb): unhappily\n",
      "unhappily (adjective): unhappily\n",
      "organization (verb): organization\n",
      "organization (adjective): organization\n",
      "organize (verb): organize\n",
      "organize (adjective): organize\n",
      "organized (verb): organize\n",
      "organized (adjective): organized\n",
      "organizing (verb): organize\n",
      "organizing (adjective): organizing\n",
      "generalize (verb): generalize\n",
      "generalize (adjective): generalize\n",
      "generalized (verb): generalize\n",
      "generalized (adjective): generalized\n",
      "generalization (verb): generalization\n",
      "generalization (adjective): generalization\n",
      "go (verb): go\n",
      "go (adjective): go\n",
      "going (verb): go\n",
      "going (adjective): going\n",
      "goes (verb): go\n",
      "goes (adjective): goes\n",
      "gone (verb): go\n",
      "gone (adjective): gone\n",
      "better (verb): better\n",
      "better (adjective): good\n",
      "best (verb): best\n",
      "best (adjective): best\n",
      "relational (verb): relational\n",
      "relational (adjective): relational\n",
      "relate (verb): relate\n",
      "relate (adjective): relate\n",
      "related (verb): relate\n",
      "related (adjective): related\n",
      "relating (verb): relate\n",
      "relating (adjective): relating\n",
      "cats (verb): cat\n",
      "cats (adjective): cats\n",
      "boxes (verb): box\n",
      "boxes (adjective): boxes\n",
      "mice (verb): mice\n",
      "mice (adjective): mice\n",
      "practically (verb): practically\n",
      "practically (adjective): practically\n",
      "practical (verb): practical\n",
      "practical (adjective): practical\n",
      "practicable (verb): practicable\n",
      "practicable (adjective): practicable\n",
      "writing (verb): write\n",
      "writing (adjective): writing\n",
      "writes (verb): write\n",
      "writes (adjective): writes\n",
      "programming (verb): program\n",
      "programming (adjective): programming\n",
      "programs (verb): program\n",
      "programs (adjective): programs\n",
      "history (verb): history\n",
      "history (adjective): history\n",
      "finally (verb): finally\n",
      "finally (adjective): finally\n",
      "finalized (verb): finalize\n",
      "finalized (adjective): finalized\n",
      "studying (verb): study\n",
      "better (adjective): good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"studies\", \"studying\", \"better\", \"organized\", \"cats\", \"went\"]\n",
    "\n",
    "print(\"🔹 Lemmatization Examples:\\n\")\n",
    "for w in tokens:\n",
    "    print(f\"{w:>10} → {lemmatizer.lemmatize(w)}\")\n",
    "\n",
    "# Using POS (Part of Speech) for better accuracy\n",
    "print(\"\\n🔹 With POS Tags:\\n\")\n",
    "for w in tokens:\n",
    "    print(f\"{w} (verb): {lemmatizer.lemmatize(w, pos='v')}\")\n",
    "    print(f\"{w} (adjective): {lemmatizer.lemmatize(w, pos='a')}\")\n",
    "\n",
    "    \n",
    "print(\"studying (verb):\", lemmatizer.lemmatize(\"studying\", pos='v'))\n",
    "print(\"better (adjective):\", lemmatizer.lemmatize(\"better\", pos='a'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
