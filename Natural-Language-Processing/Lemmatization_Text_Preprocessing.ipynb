{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e1ca29",
   "metadata": {},
   "source": [
    "# 📘 WordNet Lemmatization\n",
    "\n",
    "> **Objective:**  \n",
    "> Transform words into their **base or dictionary form (lemma)** using linguistic knowledge of **morphology and part-of-speech (POS)** tags.  \n",
    "> Unlike *stemming*, which crudely chops off word endings, **lemmatization** ensures the result is a valid, meaningful word found in a lexicon (WordNet).\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 1. What is Lemmatization?\n",
    "\n",
    "- Lemmatizzation technique is like stemming. The output we will get after lemmatization is called **lemma**\n",
    "- Lemmatization finds the **canonical (dictionary) form** of a word — called the **lemma**.\n",
    "- It’s **context-aware** and **POS-sensitive**, using grammatical category (noun, verb, adjective, adverb) to find the correct base form.\n",
    "- Lemmatization relies on **WordNet**, a large lexical database of English maintained by Princeton University.\n",
    "- After Lemmatization, we will be getting a valid word that means the same thing\n",
    "\n",
    "💡 **Example Difference vs Stemming**\n",
    "\n",
    "| Word | Porter Stemmer | Lemmatizer | Comment |\n",
    "|------|----------------|-------------|----------|\n",
    "| studies | studi | study | Returns valid word |\n",
    "| better | better | good | Handles irregular adjective |\n",
    "| mice | mice | mouse | Handles plural-to-singular |\n",
    "| gone | gone | go | Handles past participle |\n",
    "| running | run | run | Similar, but via POS check |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 2. Theoretical Foundation (Mathematical View)\n",
    "\n",
    "The lemmatization function can be mathematically defined as:\n",
    "\n",
    "$$\n",
    "\\text{Lemma}(w, \\text{POS}) = \\operatorname{lookup}_{\\text{WordNet}}\\Big(\\text{morph}(w), \\text{POS}\\Big)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ w $ → input word token  \n",
    "- $ \\text{POS} $ → part-of-speech tag  \n",
    "- $ \\text{morph}(w) $ → morphological normalization (e.g., removing inflectional suffixes like *-ing*, *-ed*)  \n",
    "- $ \\operatorname{lookup}_{\\text{WordNet}} $ → dictionary-based retrieval of lemma\n",
    "\n",
    "If no match is found:\n",
    "\n",
    "$$\n",
    "\\text{Lemma}(w, \\text{POS}) =\n",
    "\\begin{cases}\n",
    "\\text{headword in WordNet}, & \\text{if found}\\\\[3pt]\n",
    "\\text{morph}(w), & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "📘 **Simplified Intuition:**\n",
    "\n",
    "$$\n",
    "\\text{Lemma}(w,\\text{POS}) = \\text{dictionary lookup based on morphology and POS}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 3. Implementation with NLTK’s WordNet Lemmatizer\n",
    "\n",
    "NLTK provides a built-in **`WordNetLemmatizer`** class that uses the **WordNet** lexical database to return the lemma (dictionary base form) of a word.\n",
    "\n",
    "> 💡 **Key Detail:**  \n",
    "> The `WordNetLemmatizer` is a **thin wrapper** around NLTK’s internal `WordNetCorpusReader` class.  \n",
    "> Under the hood, it calls the **`morph()`** function of `WordNetCorpusReader` to perform **morphological analysis** and find the lemma.\n",
    "\n",
    "This means:\n",
    "- It first applies **morphological normalization** to remove inflectional suffixes (e.g., *-ing*, *-ed*, *-s*).  \n",
    "- Then it performs a **dictionary lookup** within WordNet to find the canonical form of the word based on its **part of speech (POS)**.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Import and Setup\n",
    "\n",
    "Before using the lemmatizer, we must download required NLTK corpora and initialize the tools.\n",
    "\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55a6530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/psundara/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5071a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293d118",
   "metadata": {},
   "source": [
    "Once initialized, the `lemmatizer.lemmatize()` method can be used as:\n",
    "\n",
    "$$\n",
    "\\text{lemma} = \\text{lemmatizer.lemmatize}(w, \\text{pos})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ w $: input word  \n",
    "- $ \\text{pos} $: optional POS tag (`'n'`, `'v'`, `'a'`, `'r'`) corresponding to **noun**, **verb**, **adjective**, or **adverb**.\n",
    "\n",
    "If `pos` is not provided, it defaults to **noun (`'n'`)**, which can lead to incorrect results for verbs or adjectives — hence POS tagging is essential.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e054e2d1",
   "metadata": {},
   "source": [
    "## 🧩 4. Why POS Tagging is Crucial\n",
    "\n",
    "Lemmatization is **part-of-speech (POS) sensitive**.  \n",
    "Without knowing whether a word is a *noun*, *verb*, *adjective*, or *adverb*, the lemmatizer cannot accurately determine its correct base form.\n",
    "\n",
    "For example:\n",
    "- `running` as a **noun** → `running`\n",
    "- `running` as a **verb** → `run`\n",
    "\n",
    "Hence, we must first assign **POS tags** (using Penn Treebank tags), and then **map them** to WordNet POS tags understood by the lemmatizer.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 POS Tag Mapping (Penn → WordNet)\n",
    "\n",
    "| Word Type | Penn Tag Prefix | WordNet Constant | Example |\n",
    "|------------|-----------------|------------------|----------|\n",
    "| Noun | `N` | `wn.NOUN` | mice → mouse |\n",
    "| Verb | `V` | `wn.VERB` | gone → go |\n",
    "| Adjective | `J` | `wn.ADJ` | better → good |\n",
    "| Adverb | `R` | `wn.ADV` | finally → finally |\n",
    "\n",
    "---\n",
    "\n",
    "The mapping function can be defined as:\n",
    "\n",
    "$$\n",
    "\\text{map}(t) =\n",
    "\\begin{cases}\n",
    "wn.ADJ, & \\text{if } t \\text{ starts with } J \\\\[4pt]\n",
    "wn.VERB, & \\text{if } t \\text{ starts with } V \\\\[4pt]\n",
    "wn.NOUN, & \\text{if } t \\text{ starts with } N \\\\[4pt]\n",
    "wn.ADV, & \\text{if } t \\text{ starts with } R \\\\[4pt]\n",
    "wn.NOUN, & \\text{otherwise (default)}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226587be",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### ⚙️ The Internal Flow\n",
    "\n",
    "```text\n",
    "WordNetLemmatizer.lemmatize()\n",
    "        ↓\n",
    "calls  nltk.corpus.reader.wordnet._morphy()\n",
    "        ↓\n",
    "uses   WordNetCorpusReader.morph()\n",
    "        ↓\n",
    "accesses WordNet dictionary entries and morphological rules\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3a360",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "### 💡 Visualization: Internal Flow Diagram\n",
    "```text\n",
    "WordNetLemmatizer\n",
    "   │\n",
    "   └──> wordnet._morphy(word, pos)\n",
    "           │\n",
    "           └──> WordNetCorpusReader.morph(form, pos)\n",
    "                    │\n",
    "                    └──> Applies suffix rules + dictionary lookup\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d7d47",
   "metadata": {},
   "source": [
    "## 🧩 Understanding Penn Treebank POS Tags\n",
    "\n",
    "When we run NLTK’s `pos_tag()` function on tokens,  \n",
    "it returns **Penn Treebank POS tags** such as `NN`, `VBD`, `JJ`, `RBR`, etc.\n",
    "\n",
    "These are **fine-grained grammatical tags** used in NLP pipelines to describe\n",
    "each word’s syntactic role (noun, verb, adjective, adverb, etc.).\n",
    "\n",
    "For example:\n",
    "\n",
    "| Tag | Full Form | Example | Description |\n",
    "|------|-------------|----------|--------------|\n",
    "| **NN** | Noun, singular | dog | singular noun |\n",
    "| **NNS** | Noun, plural | dogs | plural noun |\n",
    "| **NNP** | Proper noun, singular | India | proper name |\n",
    "| **VB** | Verb, base form | go | base verb |\n",
    "| **VBD** | Verb, past tense | went | past tense |\n",
    "| **VBG** | Verb, gerund/present participle | running | `-ing` form |\n",
    "| **VBN** | Verb, past participle | gone | past participle |\n",
    "| **VBP** | Verb, non-3rd person singular present | run | present tense |\n",
    "| **VBZ** | Verb, 3rd person singular present | runs | present tense |\n",
    "| **JJ** | Adjective | quick | adjective |\n",
    "| **JJR** | Adjective, comparative | quicker | comparative adjective |\n",
    "| **JJS** | Adjective, superlative | quickest | superlative adjective |\n",
    "| **RB** | Adverb | quickly | adverb |\n",
    "| **RBR** | Adverb, comparative | faster | comparative adverb |\n",
    "| **RBS** | Adverb, superlative | fastest | superlative adverb |\n",
    "| **PRP** | Personal pronoun | he, she | pronouns |\n",
    "| **DT** | Determiner | the, a | articles |\n",
    "| **IN** | Preposition / subordinating conjunction | in, on, after | connector |\n",
    "| **CC** | Coordinating conjunction | and, but | conjunction |\n",
    "| **TO** | \"to\" | to go | infinitive marker |\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ffd50f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('better', 'JJR'), ('mice', 'NN'), ('were', 'VBD'), ('running', 'VBG'), ('faster', 'RBR'), ('than', 'IN'), ('others', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "text = \"The better mice were running faster than others.\"\n",
    "print(pos_tag(word_tokenize(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2836016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def penn_to_wn(penn_tag):\n",
    "    \"\"\"Convert Penn Treebank POS tags to WordNet POS tags.\"\"\"\n",
    "    if penn_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif penn_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif penn_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif penn_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    return wn.NOUN  # default fallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6e2ed",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 🧩 How It Connects to Lemmatization\n",
    "\n",
    "Each Penn tag must be mapped to one of the four WordNet POS categories:\n",
    "\n",
    "| Penn Prefix | WordNet POS | Meaning   |\n",
    "| ----------- | ----------- | --------- |\n",
    "| `N`         | `wn.NOUN`   | Noun      |\n",
    "| `V`         | `wn.VERB`   | Verb      |\n",
    "| `J`         | `wn.ADJ`    | Adjective |\n",
    "| `R`         | `wn.ADV`    | Adverb    |\n",
    "\n",
    "for instance:\n",
    "- `NNS` → starts with `N` → `wn.NOUN`\n",
    "`VBG` → starts with `V` → `wn.VERB`\n",
    "`JJR` → starts with `J` → `wn.ADJ`\n",
    "`RBR` → starts with `R` → `wn.ADV`\n",
    "\n",
    "This mapping ensures the WordNetLemmatizer applies correct rules to find the right lemma.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 **Code Cell \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73e6b42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Penn Tag</th>\n",
       "      <th>Mapped WordNet POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NNS</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VBD</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VBG</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JJR</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RBR</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VBZ</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NNP</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RBS</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Penn Tag Mapped WordNet POS\n",
       "0      NNS                  n\n",
       "1      VBD                  v\n",
       "2      VBG                  v\n",
       "3      JJR                  a\n",
       "4      RBR                  r\n",
       "5      VBZ                  v\n",
       "6      NNP                  n\n",
       "7      RBS                  r"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "penn_tags = [\"NNS\", \"VBD\", \"VBG\", \"JJR\", \"RBR\", \"VBZ\", \"NNP\", \"RBS\"]\n",
    "mapped = [(t, penn_to_wn(t)) for t in penn_tags]\n",
    "pd.DataFrame(mapped, columns=[\"Penn Tag\", \"Mapped WordNet POS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a4f12",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### 💬 Quick Recap\n",
    "- `pos_tag()` → outputs Penn Treebank tags (fine-grained syntax categories).\n",
    "- You → use `penn_to_wn()` → to map to WordNet POS tags (`n`, `v`, `a`, `r`).\n",
    "- `WordNetLemmatizer` → then uses these to perform accurate dictionary-based lemmatization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f1e38",
   "metadata": {},
   "source": [
    "## 🧮 5. Lemmatization in Action\n",
    "\n",
    "Now let’s observe how **WordNet Lemmatization** behaves on real examples — with and without POS tagging.\n",
    "\n",
    "Below are sample words demonstrating plural, tense, and irregular forms:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d46d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   studies (NNS) → study → Mapped WordNet POS → n\n",
      "    better (RBR) → well → Mapped WordNet POS → r\n",
      "      gone (VBN) → go → Mapped WordNet POS → v\n",
      "       was (VBD) → be → Mapped WordNet POS → v\n",
      "   running (VBG) → run → Mapped WordNet POS → v\n",
      " organized (VBN) → organize → Mapped WordNet POS → v\n",
      "      mice (NN) → mouse → Mapped WordNet POS → n\n",
      "   finally (RB) → finally → Mapped WordNet POS → r\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "words = [\"studies\", \"better\", \"gone\", \"was\", \"running\", \"organized\", \"mice\", \"finally\"]\n",
    "\n",
    "for w in words:\n",
    "    pos = pos_tag([w])[0][1]          # Penn POS tag\n",
    "    wn_pos = penn_to_wn(pos)          # Convert to WordNet POS\n",
    "    lemma = lemmatizer.lemmatize(w, pos=wn_pos)\n",
    "    print(f\"{w:>10} ({pos}) → {lemma} → Mapped WordNet POS → {wn_pos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58246ce1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### 💡 Expected Output Explanation\n",
    "\n",
    "| Word | POS (Penn) | Lemma | Description |\n",
    "|------|-------------|--------|-------------|\n",
    "| studies | NNS | study | plural → singular |\n",
    "| better | JJR | good | adjective comparative |\n",
    "| gone | VBN | go | past participle |\n",
    "| was | VBD | be | irregular verb |\n",
    "| running | VBG | run | gerund form reduced |\n",
    "| organized | VBD | organize | past tense normalized |\n",
    "| mice | NNS | mouse | plural noun |\n",
    "| finally | RB | finally | adverb unchanged |\n",
    "\n",
    "---\n",
    "\n",
    "🧮 **Lemmatization Equation with POS:**\n",
    "\n",
    "$$\n",
    "\\text{Lemma}(w, \\text{POS}) = \\operatorname{lookup}_{\\text{WordNet}}\\big(\\text{morph}(w), \\text{POS}\\big)\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c09e7f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['The', 'mice', 'were', 'running', 'faster', 'and', 'the', 'better', 'runner', 'was', 'finally', 'organized', '.']\n",
      "POS Tags: [('The', 'DT'), ('mice', 'NN'), ('were', 'VBD'), ('running', 'VBG'), ('faster', 'RBR'), ('and', 'CC'), ('the', 'DT'), ('better', 'JJR'), ('runner', 'NN'), ('was', 'VBD'), ('finally', 'RB'), ('organized', 'VBN'), ('.', '.')]\n",
      "Lemmas: ['The', 'mouse', 'be', 'run', 'faster', 'and', 'the', 'good', 'runner', 'be', 'finally', 'organize', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "sentence = \"The mice were running faster and the better runner was finally organized.\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    \"\"\"Lemmatize a sentence using POS tagging.\"\"\"\n",
    "    lemmas = []\n",
    "    for word, pos in tagged:\n",
    "        wn_pos = penn_to_wn(pos)\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"POS Tags:\", tagged)\n",
    "print(\"Lemmas:\", lemmatize_sentence(tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce85c37",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Let’s visualize the transformation process for the sentence:\n",
    "\n",
    "> “The mice were running faster and the better runner was finally organized.”\n",
    "\n",
    "```text\n",
    "┌──────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         Raw Input Text                                       │ \n",
    "│  \"The mice were running faster and the better runner was finally organized.\" │\n",
    "└──────────────────────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "                  🧩 Tokenization (word_tokenize)\n",
    "                              │\n",
    "                              ▼\n",
    "[\"The\", \"mice\", \"were\", \"running\", \"faster\", \"and\",\n",
    " \"the\", \"better\", \"runner\", \"was\", \"finally\", \"organized\", \".\"]\n",
    "                              │\n",
    "                              ▼\n",
    "              ⚙️ Stemming (Porter / Snowball / etc.)\n",
    "                              │\n",
    "                              ▼\n",
    "[\"the\", \"mic\", \"were\", \"run\", \"faster\", \"and\",\n",
    " \"the\", \"better\", \"run\", \"was\", \"final\", \"organ\", \".\"]\n",
    "       ↑                     ↑                        ↑\n",
    "  (non-word)          (root form)               (over-stemmed)\n",
    "                              │\n",
    "                              ▼\n",
    "          📘 Lemmatization (WordNet + POS Awareness)\n",
    "                              │\n",
    "                              ▼\n",
    "[\"the\", \"mouse\", \"be\", \"run\", \"fast\", \"and\",\n",
    " \"the\", \"good\", \"runner\", \"be\", \"finally\", \"organize\", \".\"]\n",
    "       ↑          ↑          ↑            ↑\n",
    "  plural→sing.   tense→base  comp→base   adj→verb (semantic lemma)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f25f3",
   "metadata": {},
   "source": [
    "## 🧩 7. Lemmatizer vs. Stemmers — Comparative Study\n",
    "\n",
    "| Word | Porter | Snowball | Lancaster | Regexp | Lemmatizer |\n",
    "|------|---------|-----------|-------------|----------|-------------|\n",
    "| studies | studi | studi | study | studi | study |\n",
    "| better | better | better | bet | better | good |\n",
    "| gone | gone | gone | gon | gone | go |\n",
    "| was | wa | wa | was | was | be |\n",
    "| running | run | run | run | run | run |\n",
    "| organized | organ | organiz | organ | organiz | organize |\n",
    "| mice | mic | mic | mic | mice | mouse |\n",
    "| finally | final | final | fin | final | finally |\n",
    "\n",
    "🧩 **Observations:**\n",
    "- Lemmatizer produces valid **dictionary words**.  \n",
    "- Stemmers often output non-words (`organiz`, `organ`).  \n",
    "- Lemmatization correctly handles **irregular** forms and **POS** sensitivity.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 8. Lemmatization Pipeline (Mathematical Model)\n",
    "\n",
    "$$[\n",
    "\\text{Lemma}(w) = \\text{Lemma}\\big(w,\\; \\text{map}(\\text{tag}(w))\\big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Lemma}(w, \\text{POS}) = \\operatorname{lookup}_{\\text{WordNet}}\\big(\\text{morph}(w), \\text{POS}\\big)\n",
    "$$\n",
    "\n",
    "Thus,  \n",
    "$$\n",
    "\\boxed{\\text{Lemmatization} = \\text{POS Tagging} + \\text{Morphology} + \\text{Dictionary Lookup}}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf8ef3",
   "metadata": {},
   "source": [
    "## ⚠️ 9. Caveats and Best Practices\n",
    "\n",
    "⚠️ **Caveats:**\n",
    "- Requires accurate **POS tagging** for correctness.\n",
    "- Coverage limited to **WordNet vocabulary**.\n",
    "- Does not perform **contextual disambiguation** (e.g., *saw* can mean `see` or `cut`).\n",
    "- Slower than stemmers due to lookups and tagging.\n",
    "\n",
    "✅ **Best Practices:**\n",
    "- Always combine with a POS tagger before lemmatization.\n",
    "- Prefer for **semantic NLP tasks**: question answering, summarization, chatbot NLU.\n",
    "- Use stemmers for **speed-critical** IR or indexing tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 10. Final Comparison — All Normalization Techniques\n",
    "\n",
    "| Technique | Output Type | POS-Aware | Handles Irregulars | Speed | Output Validity | Typical Use |\n",
    "|------------|--------------|------------|---------------------|--------|------------------|---------------|\n",
    "| **Porter Stemmer** | Truncated root (non-word) | ❌ | ❌ | ⚡ Fast | ⚠️ | IR baseline |\n",
    "| **Snowball Stemmer** | Balanced stem | ❌ | ❌ | ⚡ Fast | ⚠️ | General stemming |\n",
    "| **Lancaster Stemmer** | Aggressive | ❌ | ❌ | ⚡ Fast | ❌ | Noisy text |\n",
    "| **Regexp Stemmer** | Pattern-based | ❌ | ❌ | ⚡ Fast | ⚠️ Custom only | Controlled suffix removal |\n",
    "| **WordNet Lemmatizer** | Real word | ✅ | ✅ | 🐢 Slower | ✅ | Semantic, context-aware NLP |\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 TL;DR\n",
    "\n",
    "- **Stemming** → rule-based truncation (fast, but rough).  \n",
    "- **Lemmatization** → linguistically accurate (slow, but meaningful).  \n",
    "- Use **Lemmatizer + POS Tagging** for all **meaning-driven** NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13bdd5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## 📊 11. Lemmatization Visualization Pipeline\n",
    "\n",
    "To make lemmatization results more intuitive, let's visualize how each **token** transforms through the process:\n",
    "\n",
    "1. **Tokenization** → breaking the sentence into words  \n",
    "2. **POS Tagging** → assigning grammatical roles  \n",
    "3. **POS Conversion** → mapping Penn POS to WordNet POS  \n",
    "4. **Lemmatization** → finding dictionary base forms\n",
    "\n",
    "This table helps you observe how **POS affects the final lemma**, especially for verbs, adjectives, and irregular forms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "854be8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 🔍 Input Sentence:\n",
       "> *The mice were running faster and the better runner was finally organized.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Penn POS</th>\n",
       "      <th>WordNet POS</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>DT</td>\n",
       "      <td>n</td>\n",
       "      <td>The</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mice</td>\n",
       "      <td>NN</td>\n",
       "      <td>n</td>\n",
       "      <td>mouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>were</td>\n",
       "      <td>VBD</td>\n",
       "      <td>v</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>running</td>\n",
       "      <td>VBG</td>\n",
       "      <td>v</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>faster</td>\n",
       "      <td>RBR</td>\n",
       "      <td>r</td>\n",
       "      <td>faster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>n</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>n</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>better</td>\n",
       "      <td>JJR</td>\n",
       "      <td>a</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>runner</td>\n",
       "      <td>NN</td>\n",
       "      <td>n</td>\n",
       "      <td>runner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>was</td>\n",
       "      <td>VBD</td>\n",
       "      <td>v</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>finally</td>\n",
       "      <td>RB</td>\n",
       "      <td>r</td>\n",
       "      <td>finally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>organized</td>\n",
       "      <td>VBN</td>\n",
       "      <td>v</td>\n",
       "      <td>organize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>n</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Token Penn POS WordNet POS     Lemma\n",
       "0         The       DT           n       The\n",
       "1        mice       NN           n     mouse\n",
       "2        were      VBD           v        be\n",
       "3     running      VBG           v       run\n",
       "4      faster      RBR           r    faster\n",
       "5         and       CC           n       and\n",
       "6         the       DT           n       the\n",
       "7      better      JJR           a      good\n",
       "8      runner       NN           n    runner\n",
       "9         was      VBD           v        be\n",
       "10    finally       RB           r   finally\n",
       "11  organized      VBN           v  organize\n",
       "12          .        .           n         ."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def visualize_lemmatization(sentence):\n",
    "    \"\"\"Display a DataFrame showing Token → POS → WordNet POS → Lemma.\"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    data = []\n",
    "    for word, pos in tagged:\n",
    "        wn_pos = penn_to_wn(pos)\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "        data.append({\n",
    "            \"Token\": word,\n",
    "            \"Penn POS\": pos,\n",
    "            \"WordNet POS\": wn_pos if wn_pos else \"-\",\n",
    "            \"Lemma\": lemma\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    display(Markdown(f\"### 🔍 Input Sentence:\\n> *{sentence}*\"))\n",
    "    display(df)\n",
    "\n",
    "# Example usage\n",
    "visualize_lemmatization(\"The mice were running faster and the better runner was finally organized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ce169",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 💡 Observations from Visualization\n",
    "\n",
    "| Insight | Explanation |\n",
    "|----------|--------------|\n",
    "| `mice → mouse` | Lemmatizer correctly handles plural nouns. |\n",
    "| `were → be` | Verb inflection (past tense) normalized to root. |\n",
    "| `running → run` | Gerund form reduced to base form using POS. |\n",
    "| `better → good` | Adjective comparative handled via WordNet semantics. |\n",
    "| `finally → finally` | Adverbs usually remain unchanged. |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Lemmatization Flow Summary\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Tokenized Words} \n",
    "\\;\\xrightarrow{\\text{POS Tagger}}\\;\n",
    "\\text{Tagged Pairs (w, t)}\n",
    "\\;\\xrightarrow{\\text{map}(t)}\\;\n",
    "\\text{WordNet POS}\n",
    "\\;\\xrightarrow{\\text{lookup}}\\;\n",
    "\\text{Lemma}(w, \\text{POS})\n",
    "$$\n",
    "\n",
    "\n",
    "or in short:\n",
    "\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{Lemma} = \\text{dictionary lookup based on morphology and POS}}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Wrap-Up\n",
    "\n",
    "- Lemmatization = **POS tagging + Morphological normalization + Dictionary lookup**  \n",
    "- Produces linguistically valid words — crucial for **semantic NLP tasks** like:\n",
    "  - Chatbots 🤖  \n",
    "  - Information Retrieval 🔍  \n",
    "  - Text Summarization 🧾  \n",
    "  - Question Answering 💬  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b0945bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Input Text: Studies were better when mice were running and finally gone.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### 🔍 Input Sentence:\n",
       "> *Studies were better when mice were running and finally gone.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Penn POS</th>\n",
       "      <th>WordNet POS</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Studies</td>\n",
       "      <td>NNS</td>\n",
       "      <td>n</td>\n",
       "      <td>Studies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>were</td>\n",
       "      <td>VBD</td>\n",
       "      <td>v</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better</td>\n",
       "      <td>JJR</td>\n",
       "      <td>a</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when</td>\n",
       "      <td>WRB</td>\n",
       "      <td>n</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mice</td>\n",
       "      <td>NN</td>\n",
       "      <td>n</td>\n",
       "      <td>mouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>were</td>\n",
       "      <td>VBD</td>\n",
       "      <td>v</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>running</td>\n",
       "      <td>VBG</td>\n",
       "      <td>v</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>n</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>finally</td>\n",
       "      <td>RB</td>\n",
       "      <td>r</td>\n",
       "      <td>finally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gone</td>\n",
       "      <td>VBN</td>\n",
       "      <td>v</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>n</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Token Penn POS WordNet POS    Lemma\n",
       "0   Studies      NNS           n  Studies\n",
       "1      were      VBD           v       be\n",
       "2    better      JJR           a     good\n",
       "3      when      WRB           n     when\n",
       "4      mice       NN           n    mouse\n",
       "5      were      VBD           v       be\n",
       "6   running      VBG           v      run\n",
       "7       and       CC           n      and\n",
       "8   finally       RB           r  finally\n",
       "9      gone      VBN           v       go\n",
       "10        .        .           n        ."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lemmatize_text_pipeline(text):\n",
    "    \"\"\"End-to-end text normalization visualization.\"\"\"\n",
    "    print(\"📥 Input Text:\", text)\n",
    "    visualize_lemmatization(text)\n",
    "\n",
    "# Try it\n",
    "lemmatize_text_pipeline(\"Studies were better when mice were running and finally gone.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233472be",
   "metadata": {},
   "source": [
    "# 🧠 NLP Text Normalization Summary\n",
    "\n",
    "You’ve now learned the **core preprocessing trilogy** of NLP:\n",
    "\n",
    "> **Tokenization → Stemming → Lemmatization**\n",
    "\n",
    "These three stages form the foundation of almost every **Natural Language Processing** workflow — from search engines to chatbots, sentiment analyzers, and summarizers.\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 1. Conceptual Overview\n",
    "\n",
    "| Stage | Purpose | Output Example | Notes |\n",
    "|--------|----------|----------------|-------|\n",
    "| **Tokenization** | Splits raw text into tokens (words/punctuations) | `\"The cats are running\"` → `[\"The\", \"cats\", \"are\", \"running\"]` | Foundation of NLP preprocessing |\n",
    "| **Stemming** | Removes suffixes to get the root form | `\"running\"` → `\"run\"` | Rule-based, can produce non-words |\n",
    "| **Lemmatization** | Converts to dictionary (lemma) form | `\"better\"` → `\"good\"` | Uses WordNet + POS awareness |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧮 2. Mathematical Summary\n",
    "\n",
    "\\[\n",
    "\\text{Normalization Pipeline:}\n",
    "\\quad\n",
    "\\text{Raw Text}\n",
    "\\xrightarrow{\\text{tokenize}}\n",
    "\\text{Tokens}\n",
    "\\xrightarrow{\\text{stem/lemma}}\n",
    "\\text{Normalized Tokens}\n",
    "\\]\n",
    "\n",
    "Formally, for each token \\( w \\):\n",
    "\n",
    "\\[\n",
    "\\text{Lemma}(w, \\text{POS}) = \\operatorname{lookup}_{\\text{WordNet}}\\big(\\text{morph}(w), \\text{POS}\\big)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 3. ASCII Pipeline Visualization\n",
    "\n",
    "```text\n",
    "         ┌────────────────────────────────────┐\n",
    "         │        Raw Input Text              │\n",
    "         │   \"The mice were running fast.\"    │\n",
    "         └────────────────────────────────────┘\n",
    "                          │\n",
    "                          ▼\n",
    "             🧩 Tokenization (word_tokenize)\n",
    "                          │\n",
    "                          ▼\n",
    "      [\"The\", \"mice\", \"were\", \"running\", \"fast\", \".\"]\n",
    "                          │\n",
    "                          ▼\n",
    "           ⚙️ Stemming (Porter / Snowball / etc.)\n",
    "                          │\n",
    "                          ▼\n",
    "   [\"the\", \"mice\", \"were\", \"run\", \"fast\", \".\"]   ← root-like forms\n",
    "                          │\n",
    "                          ▼\n",
    "      📘 Lemmatization (WordNet + POS Awareness)\n",
    "                          │\n",
    "                          ▼\n",
    "[\"the\", \"mouse\", \"be\", \"run\", \"fast\", \".\"]   ← valid dictionary words\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50573a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 💡 4. Key Takeaways\n",
    "\n",
    "### ✅ **Tokenization**\n",
    "- Splits raw text into individual tokens (words, punctuation, etc.).\n",
    "- It’s the **first step** in every NLP pipeline.\n",
    "- Output forms the base for further processing.\n",
    "\n",
    "### ⚙️ **Stemming**\n",
    "- Applies **rule-based truncation** to get root-like forms.\n",
    "- Fast, but often produces **non-words** (e.g., *organiz*).\n",
    "- Ideal for **search engines** and **information retrieval (IR)** where exact meaning isn’t critical.\n",
    "\n",
    "### 📘 **Lemmatization**\n",
    "- Produces **valid dictionary words (lemmas)**.\n",
    "- Depends on **POS tagging** and **morphological rules**.\n",
    "- Slower but **more accurate** and **meaningful**.\n",
    "- Preferred for **semantic NLP**, **chatbots**, and **language understanding** tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 5. TL;DR Visual Summary\n",
    "\n",
    "| Stage | Algorithm | Example Transformation | Output Valid | POS-Aware | Best Used For |\n",
    "|--------|------------|------------------------|---------------|------------|----------------|\n",
    "| **Tokenization** | `word_tokenize` | `\"cats are running\"` → `[\"cats\",\"are\",\"running\"]` | ✅ | ❌ | Text segmentation |\n",
    "| **Porter Stemmer** | Rule-based suffix removal | `\"running\"` → `\"run\"` | ⚠️ Sometimes | ❌ | Quick baseline tasks |\n",
    "| **Snowball Stemmer** | Improved Porter | `\"studies\"` → `\"studi\"` | ❌ | ❌ | General stemming |\n",
    "| **Lancaster Stemmer** | Aggressive | `\"connection\"` → `\"connect\"` | ⚠️ | ❌ | Noisy data cleanup |\n",
    "| **Regexp Stemmer** | Regex-based suffix trim | `\"organizing\"` → `\"organ\"` | ⚠️ | ❌ | Controlled suffix removal |\n",
    "| **WordNet Lemmatizer** | Morph + POS + Dictionary | `\"better\"` → `\"good\"` | ✅ | ✅ | Semantic NLP, NLU tasks |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 6. Visual Flow Recap\n",
    "\n",
    "```text\n",
    "Text\n",
    "  │\n",
    "  ▼\n",
    "Tokenization\n",
    "  │\n",
    "  ▼\n",
    "Stemming / Lemmatization\n",
    "  │\n",
    "  ▼\n",
    "Feature Extraction (BoW / TF-IDF / Embeddings)\n",
    "  │\n",
    "  ▼\n",
    "Model Input → Training / Inference\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
