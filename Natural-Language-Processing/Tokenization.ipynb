{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e326af1c",
   "metadata": {},
   "source": [
    "# 🧠 Tokenization in NLP\n",
    "\n",
    "Natural Language Processing (NLP) deals with how computers understand and process human language.  \n",
    "Before any text can be analyzed by a model, it must be **tokenized** — i.e., broken into smaller pieces called **tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 Topics Overview\n",
    "\n",
    "| 🔢 | **Concept** | **Represents** |\n",
    "|:--:|:-------------|:----------------|\n",
    "| 1️⃣ | **Corpus** | A collection of *paragraphs* or the entire text dataset |\n",
    "| 2️⃣ | **Documents** | A *single paragraph* or *sentence* within the corpus |\n",
    "| 3️⃣ | **Vocabulary** | The set of *unique words* present in the corpus |\n",
    "| 4️⃣ | **Words** | The *individual tokens* or *terms* in each sentence |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Detailed Explanation\n",
    "\n",
    "### 1️⃣ Corpus — *The Entire Text Collection*\n",
    "📌 A **corpus** is a large and structured set of texts used for training or evaluating NLP models.  \n",
    "It may contain multiple documents, paragraphs, or sentences.\n",
    "\n",
    "$$\n",
    "Corpus = \\{Document_1, Document_2, ..., Document_n\\}\n",
    "$$\n",
    "\n",
    "🧠 **Example:**\n",
    "> “Natural Language Processing is amazing. NLP enables machines to understand humans.”\n",
    "\n",
    "Here, the entire text above is our **corpus**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Documents — *Smaller Text Units*\n",
    "📌 A **document** is a subset of the corpus.  \n",
    "It could be a **paragraph**, **article**, or **sentence** depending on context.\n",
    "\n",
    "$$\n",
    "Corpus = \\{D_1, D_2, D_3, ...\\}\n",
    "$$\n",
    "\n",
    "🧠 **Example:**\n",
    "1. “Natural Language Processing is amazing.”  \n",
    "2. “NLP enables machines to understand humans.”\n",
    "\n",
    "Both are separate **documents** within the same corpus.\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ Vocabulary — *Unique Words Collection*\n",
    "📌 The **vocabulary** of a corpus is the set of all **unique tokens** (distinct words) appearing across documents.\n",
    "\n",
    "$$\n",
    "Vocabulary = \\{w_1, w_2, ..., w_n\\}\n",
    "$$\n",
    "\n",
    "✅ **Example:**\n",
    "From the corpus above:\n",
    "> Vocabulary = {Natural, Language, Processing, is, amazing, NLP, enables, machines, to, understand, humans}\n",
    "\n",
    "---\n",
    "\n",
    "### 4️⃣ Words — *Individual Tokens*\n",
    "📌 The **words** (or **tokens**) are the **smallest textual units** in a sentence — usually split by spaces or punctuation.\n",
    "\n",
    "$$\n",
    "Sentence = \\{w_1, w_2, w_3, ...\\}\n",
    "$$\n",
    "\n",
    "🧠 **Example:**\n",
    "> “NLP enables machines to understand humans.”\n",
    "\n",
    "➡ Tokens = [‘NLP’, ‘enables’, ‘machines’, ‘to’, ‘understand’, ‘humans’]\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Why Tokenization Matters\n",
    "\n",
    "✅ Converts raw text into a form that algorithms can understand  \n",
    "✅ Helps in building vocabulary, frequency counts, embeddings, and context models  \n",
    "✅ Serves as the **foundation** for further NLP tasks such as:\n",
    "- Text Classification  \n",
    "- Sentiment Analysis  \n",
    "- Named Entity Recognition  \n",
    "- Language Modeling\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Summary Table\n",
    "\n",
    "| Concept | Description | Example |\n",
    "|:--------|:-------------|:---------|\n",
    "| **Corpus** | Entire text dataset | All articles in Wikipedia |\n",
    "| **Document** | One paragraph/sentence | “Natural Language Processing is amazing.” |\n",
    "| **Vocabulary** | Unique words in corpus | {‘Natural’, ‘Language’, …} |\n",
    "| **Words** | Individual tokens | [‘NLP’, ‘enables’, …] |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11a0b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from nltk) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cc2d7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/psundara/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626c8650",
   "metadata": {},
   "source": [
    "# 🧠 Tokenization in NLP — Practical Implementation with NLTK\n",
    "\n",
    "Natural Language Processing (NLP) involves preparing raw text so that machines can process it.  \n",
    "One of the very first steps in any NLP pipeline is **Tokenization** — the process of splitting text into **sentences**, **words**, or **subwords**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 What You’ll Learn\n",
    "\n",
    "🔹 Understanding **Corpus**, **Documents**, **Vocabulary**, and **Words**  \n",
    "🔹 Performing **Sentence Tokenization** using `sent_tokenize()`  \n",
    "🔹 Performing **Word Tokenization** using `word_tokenize()`, `wordpunct_tokenize()`, and `TreebankWordTokenizer()`  \n",
    "🔹 Seeing the difference between each tokenizer  \n",
    "🔹 Observing how punctuation and contractions are handled  \n",
    "\n",
    "---\n",
    "\n",
    "📌 **Mathematical Representation**\n",
    "\n",
    "- Sentence Tokenization:  \n",
    "  $$ P \\rightarrow [S_1, S_2, S_3, \\dots, S_m] $$\n",
    "  where each $S_i$ is a sentence.\n",
    "\n",
    "- Word Tokenization:  \n",
    "  $$ S_i \\rightarrow [w_{i1}, w_{i2}, \\dots, w_{ik}] $$\n",
    "\n",
    "- Vocabulary Extraction:  \n",
    "  $$ V = \\bigcup_{i=1}^{m} \\{ w_{ij} \\mid j = 1, 2, \\dots, k_i \\} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea46ca43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Corpus Text:\n",
      "\n",
      "Hello Welcome, to Prasanna Sundaram's NLP Tutorials.\n",
      "Please go through the entire content! to become expert in NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 📦 Define a sample corpus for tokenization demonstration\n",
    "\n",
    "corpus = \"\"\"Hello Welcome, to Prasanna Sundaram's NLP Tutorials.\n",
    "Please go through the entire content! to become expert in NLP.\n",
    "\"\"\"\n",
    "\n",
    "# Display the raw corpus\n",
    "print(\"🧾 Corpus Text:\\n\")\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e8bba5",
   "metadata": {},
   "source": [
    "## ✂️ Sentence Tokenization (Paragraph → Sentences)\n",
    "\n",
    "Sentence Tokenization splits a paragraph or document into **individual sentences**.  \n",
    "This helps models process text meaningfully one sentence at a time.\n",
    "\n",
    "---\n",
    "\n",
    "📌 **Concept**\n",
    "$$\n",
    "\\text{sent\\_tokenize}(Paragraph) \\rightarrow [Sentence_1, Sentence_2, \\dots, Sentence_n]\n",
    "$$\n",
    "\n",
    "🔹 We’ll use **`nltk.sent_tokenize`**, which internally relies on the **Punkt** model trained on English data.  \n",
    "🔹 It intelligently handles:\n",
    "- Abbreviations (`Dr.` or `Mr.`)\n",
    "- Question marks (`?`)\n",
    "- Exclamations (`!`)\n",
    "- Periods (`.`) in normal sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2deea59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Type: <class 'list'>\n",
      "\n",
      "🧩 Tokenized Sentences:\n",
      "\n",
      "1. Hello Welcome, to Prasanna Sundaram's NLP Tutorials.\n",
      "2. Please go through the entire content!\n",
      "3. to become expert in NLP.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Sentence Tokenization Example\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Perform sentence tokenization on the corpus\n",
    "documents = sent_tokenize(corpus)\n",
    "\n",
    "# Display type and output\n",
    "print(\"📂 Type:\", type(documents))\n",
    "print(\"\\n🧩 Tokenized Sentences:\\n\")\n",
    "for idx, sentence in enumerate(documents, start=1):\n",
    "    print(f\"{idx}. {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f959c79",
   "metadata": {},
   "source": [
    "## 🔤 Word Tokenization (Sentence → Words)\n",
    "\n",
    "Now that we have sentences, we can further break them into **individual words (tokens)**.  \n",
    "Word tokenization is critical for tasks like:\n",
    "- Building a **vocabulary**\n",
    "- Counting word **frequency**\n",
    "- Creating **embeddings**\n",
    "\n",
    "---\n",
    "\n",
    "📘 **Concept**\n",
    "$$\n",
    "\\text{word\\_tokenize}(Sentence) \\rightarrow [w_1, w_2, ..., w_n]\n",
    "$$\n",
    "\n",
    "We’ll experiment with **three tokenizers** from NLTK:\n",
    "1. `word_tokenize()` — general-purpose tokenizer  \n",
    "2. `wordpunct_tokenize()` — splits on all non-word characters  \n",
    "3. `TreebankWordTokenizer()` — follows Penn Treebank rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46177a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 Word Tokenization using word_tokenize():\n",
      "\n",
      "All Tokens (Corpus Level): ['Hello', 'Welcome', ',', 'to', 'Prasanna', 'Sundaram', \"'s\", 'NLP', 'Tutorials', '.', 'Please', 'go', 'through', 'the', 'entire', 'content', '!', 'to', 'become', 'expert', 'in', 'NLP', '.'] \n",
      "\n",
      "Sentence 1 Tokens: ['Hello', 'Welcome', ',', 'to', 'Prasanna', 'Sundaram', \"'s\", 'NLP', 'Tutorials', '.']\n",
      "Sentence 2 Tokens: ['Please', 'go', 'through', 'the', 'entire', 'content', '!']\n",
      "Sentence 3 Tokens: ['to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "# ✅ Word Tokenization using word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(\"🧩 Word Tokenization using word_tokenize():\\n\")\n",
    "\n",
    "# Tokenize the entire corpus\n",
    "word_tokens = word_tokenize(corpus)\n",
    "print(\"All Tokens (Corpus Level):\", word_tokens, \"\\n\")\n",
    "\n",
    "# Tokenize each sentence separately\n",
    "for idx, sentence in enumerate(documents, start=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    print(f\"Sentence {idx} Tokens: {tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22f84fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧩 Word Tokenization using wordpunct_tokenize():\n",
      "\n",
      "['Hello', 'Welcome', ',', 'to', 'Prasanna', 'Sundaram', \"'\", 's', 'NLP', 'Tutorials', '.', 'Please', 'go', 'through', 'the', 'entire', 'content', '!', 'to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Word Tokenization using wordpunct_tokenize\n",
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "print(\"\\n🧩 Word Tokenization using wordpunct_tokenize():\\n\")\n",
    "\n",
    "# This splits words on every non-alphanumeric character\n",
    "# Example: \"Sundaram's\" -> ['Sundaram', \"'\", 's']\n",
    "tokens_punct = wordpunct_tokenize(corpus)\n",
    "print(tokens_punct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44df7f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧩 Word Tokenization using TreebankWordTokenizer():\n",
      "\n",
      "['Hello', 'Welcome', ',', 'to', 'Prasanna', 'Sundaram', \"'s\", 'NLP', 'Tutorials.', 'Please', 'go', 'through', 'the', 'entire', 'content', '!', 'to', 'become', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "# 🔹 Word Tokenization using \n",
    "# 📝 TreebankWordTokenizer follows Penn Treebank conventions:\n",
    "# - Splits contractions: \"can't\" → ['ca', \"n't\"]\n",
    "# - Handles ending punctuation carefully\n",
    "# - Keeps internal punctuation attached unless sentence-ending\n",
    "# if you check the result fullstop will not treated as sperate word, for the last full stop only it will consider as seperate word\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "print(\"\\n🧩 Word Tokenization using TreebankWordTokenizer():\\n\")\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "treebank_tokens = tokenizer.tokenize(corpus)\n",
    "print(treebank_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cac37b",
   "metadata": {},
   "source": [
    "## 🧩 Tokenizer Comparison Summary\n",
    "\n",
    "| Tokenizer | Splitting Behavior | Handles Contractions | Handles Punctuation | Notes |\n",
    "|:-----------|:------------------:|:--------------------:|:------------------:|:------|\n",
    "| **`word_tokenize`** | Moderate | ✅ | ✅ | Good default choice |\n",
    "| **`wordpunct_tokenize`** | Aggressive | ❌ (splits `'s` → `'`, `s`) | ✅✅ | Best for punctuation-heavy text |\n",
    "| **`TreebankWordTokenizer`** | Linguistically consistent | ✅ | ✅ | Used in research & corpora preprocessing |\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Key Takeaways\n",
    "✅ Always inspect tokens before using them in downstream tasks.  \n",
    "✅ Maintain the same tokenizer during both **training** and **inference**.  \n",
    "✅ Combine tokenization with **lowercasing**, **stopword removal**, and **lemmatization** as preprocessing steps.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧮 Mathematical Summary\n",
    "\n",
    "- **Sentence Tokenization**\n",
    "  $$\n",
    "  P \\mapsto [S_1, S_2, \\dots, S_m]\n",
    "  $$\n",
    "- **Word Tokenization**\n",
    "  $$\n",
    "  S_i \\mapsto [w_{i1}, w_{i2}, \\dots, w_{ik}]\n",
    "  $$\n",
    "- **Vocabulary Extraction**\n",
    "  $$\n",
    "  V = \\bigcup_{i=1}^{m} \\{ w_{ij} \\mid j = 1, 2, \\dots, k_i \\}\n",
    "  $$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
