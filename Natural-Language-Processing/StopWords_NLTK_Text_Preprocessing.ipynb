{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db12fcc",
   "metadata": {},
   "source": [
    "# ğŸ§¹ Stop Words Removal using NLTK\n",
    "\n",
    "> **Objective:**  \n",
    "> Remove common words (like *the*, *is*, *in*, *and*) that usually carry **little or no semantic meaning** in NLP tasks.  \n",
    "> These words are called **Stop Words**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“˜ 1. What are Stop Words?\n",
    "\n",
    "- **Stop words** are high-frequency words that appear in most texts but donâ€™t contribute much to meaning.  \n",
    "  Examples: *a, an, the, in, on, is, was, were, of, for, and, to, at*.\n",
    "- Removing them helps:\n",
    "  - Reduce dataset size.\n",
    "  - Improve model focus on **meaningful tokens**.\n",
    "  - Increase efficiency of vectorization (BoW / TF-IDF).\n",
    "\n",
    "However, the decision to remove stop words depends on **task context** â€”  \n",
    "for example, you may *not* remove them in **sentiment analysis** (\"not good\" vs \"good\").\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© 2. NLTK Stopword Corpus\n",
    "\n",
    "NLTK provides a ready-made list of English stop words via:\n",
    "\n",
    "$$\n",
    "\\text{stopwords.words('english')}\n",
    "$$\n",
    "\n",
    "> âœ… Pipeline:\n",
    "> **Sentence Tokenization â†’ Word Tokenization â†’ Stopword Removal â†’ Lemmatization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ecf07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When you step into any challenge, you must bring one thing above all â€” consistency in your actions.\n",
      "Success is not a sudden peak â€” itâ€™s a steady climb built on daily habits.\n",
      "You donâ€™t wake up one morning and find youâ€™re great; you become great because you kept showing up, kept trying, and kept learning.\n",
      "Mistakes will happen â€” thatâ€™s inevitable.\n",
      "What matters is whether you let them define you or refine you.\n",
      "Stay calm under pressure, focus only on what you can control â€” your preparation, your intent, and your effort.\n",
      "The scoreboard may shift, the crowd may silence, but your character stays intact if youâ€™ve done the right things.\n",
      "Let your purpose drive your performance, not the applause.\n",
      "Because in the end, itâ€™s not about trophies or titles â€” itâ€™s about the process, the grind, and the faith that carried you through.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ MS Dhoni motivational paragraph\n",
    "paragraph = \"\"\"\n",
    "When you step into any challenge, you must bring one thing above all â€” consistency in your actions.\n",
    "Success is not a sudden peak â€” itâ€™s a steady climb built on daily habits.\n",
    "You donâ€™t wake up one morning and find youâ€™re great; you become great because you kept showing up, kept trying, and kept learning.\n",
    "Mistakes will happen â€” thatâ€™s inevitable.\n",
    "What matters is whether you let them define you or refine you.\n",
    "Stay calm under pressure, focus only on what you can control â€” your preparation, your intent, and your effort.\n",
    "The scoreboard may shift, the crowd may silence, but your character stays intact if youâ€™ve done the right things.\n",
    "Let your purpose drive your performance, not the applause.\n",
    "Because in the end, itâ€™s not about trophies or titles â€” itâ€™s about the process, the grind, and the faith that carried you through.\n",
    "\"\"\"\n",
    "print(paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a17b90",
   "metadata": {},
   "source": [
    "## ğŸ§© Step 1 â€” Sentence Tokenization\n",
    "\n",
    "**Goal:**  \n",
    "Break a paragraph into individual **sentences** using NLTKâ€™s `sent_tokenize()`.\n",
    "\n",
    "$$\n",
    "\\text{Sentences} = \\text{sent\\_tokenize}(paragraph)\n",
    "$$\n",
    "\n",
    "This step helps process long documents efficiently and enables per-sentence analysis for later stages like POS tagging or sentiment scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26aa4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sentence Tokenization\n",
    "sentences = sent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
