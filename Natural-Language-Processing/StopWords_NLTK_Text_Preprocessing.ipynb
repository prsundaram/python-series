{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db12fcc",
   "metadata": {},
   "source": [
    "# üßπ Stop Words Removal using NLTK\n",
    "\n",
    "> **Objective:**  \n",
    "> Remove common words (like *the*, *is*, *in*, *and*) that usually carry **little or no semantic meaning** in NLP tasks.  \n",
    "> These words are called **Stop Words**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò 1. What are Stop Words?\n",
    "\n",
    "- **Stop words** are high-frequency words that appear in most texts but don‚Äôt contribute much to meaning.  \n",
    "  Examples: *a, an, the, in, on, is, was, were, of, for, and, to, at*.\n",
    "- Removing them helps:\n",
    "  - Reduce dataset size.\n",
    "  - Improve model focus on **meaningful tokens**.\n",
    "  - Increase efficiency of vectorization (BoW / TF-IDF).\n",
    "\n",
    "However, the decision to remove stop words depends on **task context** ‚Äî  \n",
    "for example, you may *not* remove them in **sentiment analysis** (\"not good\" vs \"good\").\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 2. NLTK Stopword Corpus\n",
    "\n",
    "NLTK provides a ready-made list of English stop words via:\n",
    "\n",
    "$$\n",
    "\\text{stopwords.words('english')}\n",
    "$$\n",
    "\n",
    "> ‚úÖ Pipeline:\n",
    "> **Sentence Tokenization ‚Üí Word Tokenization ‚Üí Stopword Removal ‚Üí Lemmatization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ecf07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "When you step into any challenge, you must bring one thing above all ‚Äî consistency in your actions.\n",
      "Success is not a sudden peak ‚Äî it‚Äôs a steady climb built on daily habits.\n",
      "You don‚Äôt wake up one morning and find you‚Äôre great; you become great because you kept showing up, kept trying, and kept learning.\n",
      "Mistakes will happen ‚Äî that‚Äôs inevitable.\n",
      "What matters is whether you let them define you or refine you.\n",
      "Stay calm under pressure, focus only on what you can control ‚Äî your preparation, your intent, and your effort.\n",
      "The scoreboard may shift, the crowd may silence, but your character stays intact if you‚Äôve done the right things.\n",
      "Let your purpose drive your performance, not the applause.\n",
      "Because in the end, it‚Äôs not about trophies or titles ‚Äî it‚Äôs about the process, the grind, and the faith that carried you through.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üèè MS Dhoni motivational paragraph\n",
    "paragraph = \"\"\"\n",
    "When you step into any challenge, you must bring one thing above all ‚Äî consistency in your actions.\n",
    "Success is not a sudden peak ‚Äî it‚Äôs a steady climb built on daily habits.\n",
    "You don‚Äôt wake up one morning and find you‚Äôre great; you become great because you kept showing up, kept trying, and kept learning.\n",
    "Mistakes will happen ‚Äî that‚Äôs inevitable.\n",
    "What matters is whether you let them define you or refine you.\n",
    "Stay calm under pressure, focus only on what you can control ‚Äî your preparation, your intent, and your effort.\n",
    "The scoreboard may shift, the crowd may silence, but your character stays intact if you‚Äôve done the right things.\n",
    "Let your purpose drive your performance, not the applause.\n",
    "Because in the end, it‚Äôs not about trophies or titles ‚Äî it‚Äôs about the process, the grind, and the faith that carried you through.\n",
    "\"\"\"\n",
    "print(paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a17b90",
   "metadata": {},
   "source": [
    "## üß© Step 1 ‚Äî Sentence Tokenization\n",
    "\n",
    "**Goal:**  \n",
    "Break a paragraph into individual **sentences** using NLTK‚Äôs `sent_tokenize()`.\n",
    "\n",
    "$$\n",
    "\\text{Sentences} = \\text{sent\\_tokenize}(paragraph)\n",
    "$$\n",
    "\n",
    "This step helps process long documents efficiently and enables per-sentence analysis for later stages like POS tagging or sentiment scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26aa4f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ad3e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences: 9\n",
      "\n",
      "1. When you step into any challenge, you must bring one thing above all ‚Äî consistency in your actions.\n",
      "2. Success is not a sudden peak ‚Äî it‚Äôs a steady climb built on daily habits.\n",
      "3. You don‚Äôt wake up one morning and find you‚Äôre great; you become great because you kept showing up, kept trying, and kept learning.\n",
      "4. Mistakes will happen ‚Äî that‚Äôs inevitable.\n",
      "5. What matters is whether you let them define you or refine you.\n",
      "6. Stay calm under pressure, focus only on what you can control ‚Äî your preparation, your intent, and your effort.\n",
      "7. The scoreboard may shift, the crowd may silence, but your character stays intact if you‚Äôve done the right things.\n",
      "8. Let your purpose drive your performance, not the applause.\n",
      "9. Because in the end, it‚Äôs not about trophies or titles ‚Äî it‚Äôs about the process, the grind, and the faith that carried you through.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sentence Tokenization\n",
    "sentences = sent_tokenize(paragraph)\n",
    "print(f\"Total Sentences: {len(sentences)}\\n\")\n",
    "\n",
    "for i, s in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {s.strip()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11056018",
   "metadata": {},
   "source": [
    "## üß© Step 2 ‚Äî Word Tokenization (Per Sentence)\n",
    "\n",
    "After splitting into sentences, we tokenize each sentence into words.\n",
    "\n",
    "$$\n",
    "\\text{Tokens}(S_i) = \\text{word\\_tokenize}(S_i)\n",
    "$$\n",
    "\n",
    "This hierarchical tokenization (sentence ‚Üí word) gives us better control over context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce5a852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1 Tokens (20):\n",
      "['When', 'you', 'step', 'into', 'any', 'challenge', ',', 'you', 'must', 'bring', 'one', 'thing', 'above', 'all', '‚Äî', 'consistency', 'in', 'your', 'actions', '.']\n",
      "\n",
      "Sentence 2 Tokens (18):\n",
      "['Success', 'is', 'not', 'a', 'sudden', 'peak', '‚Äî', 'it', '‚Äô', 's', 'a', 'steady', 'climb', 'built', 'on', 'daily', 'habits', '.']\n",
      "\n",
      "Sentence 3 Tokens (31):\n",
      "['You', 'don', '‚Äô', 't', 'wake', 'up', 'one', 'morning', 'and', 'find', 'you', '‚Äô', 're', 'great', ';', 'you', 'become', 'great', 'because', 'you', 'kept', 'showing', 'up', ',', 'kept', 'trying', ',', 'and', 'kept', 'learning', '.']\n",
      "\n",
      "Sentence 4 Tokens (9):\n",
      "['Mistakes', 'will', 'happen', '‚Äî', 'that', '‚Äô', 's', 'inevitable', '.']\n",
      "\n",
      "Sentence 5 Tokens (13):\n",
      "['What', 'matters', 'is', 'whether', 'you', 'let', 'them', 'define', 'you', 'or', 'refine', 'you', '.']\n",
      "\n",
      "Sentence 6 Tokens (23):\n",
      "['Stay', 'calm', 'under', 'pressure', ',', 'focus', 'only', 'on', 'what', 'you', 'can', 'control', '‚Äî', 'your', 'preparation', ',', 'your', 'intent', ',', 'and', 'your', 'effort', '.']\n",
      "\n",
      "Sentence 7 Tokens (24):\n",
      "['The', 'scoreboard', 'may', 'shift', ',', 'the', 'crowd', 'may', 'silence', ',', 'but', 'your', 'character', 'stays', 'intact', 'if', 'you', '‚Äô', 've', 'done', 'the', 'right', 'things', '.']\n",
      "\n",
      "Sentence 8 Tokens (11):\n",
      "['Let', 'your', 'purpose', 'drive', 'your', 'performance', ',', 'not', 'the', 'applause', '.']\n",
      "\n",
      "Sentence 9 Tokens (32):\n",
      "['Because', 'in', 'the', 'end', ',', 'it', '‚Äô', 's', 'not', 'about', 'trophies', 'or', 'titles', '‚Äî', 'it', '‚Äô', 's', 'about', 'the', 'process', ',', 'the', 'grind', ',', 'and', 'the', 'faith', 'that', 'carried', 'you', 'through', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each sentence into words\n",
    "\n",
    "tokenized_sentences = [word_tokenize(s) for s in sentences]\n",
    "\n",
    "for i, tokens in enumerate(tokenized_sentences, 1):\n",
    "    print(f\"\\nSentence {i} Tokens ({len(tokens)}):\\n{tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af2a8ba",
   "metadata": {},
   "source": [
    "## üßπ Step 3 ‚Äî Stopword Removal (Per Sentence)\n",
    "\n",
    "Now that we have **word tokens per sentence**, we‚Äôll remove **stop words** from each sentence independently.\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Given a sentence token list \\( T_i = [w_1,\\dots,w_m] \\) and a stopword set \\( S \\),\n",
    "\n",
    "$$\n",
    "T_i' = \\{\\, w \\in T_i \\mid w.\\mathrm{lower}() \\notin S \\ \\land\\ w.\\mathrm{isalpha}() \\,\\}\n",
    "$$\n",
    "\n",
    "This keeps **alphabetic content words** and drops common function words (the, is, and‚Ä¶).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36aa50d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Ensure resources\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2d22001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1:\n",
      "Original tokens: ['When', 'you', 'step', 'into', 'any', 'challenge', ',', 'you', 'must', 'bring', 'one', 'thing', 'above', 'all', '‚Äî', 'consistency', 'in', 'your', 'actions', '.']\n",
      "After stopword removal: ['step', 'challenge', 'must', 'bring', 'one', 'thing', 'consistency', 'actions']\n",
      "\n",
      "Sentence 2:\n",
      "Original tokens: ['Success', 'is', 'not', 'a', 'sudden', 'peak', '‚Äî', 'it', '‚Äô', 's', 'a', 'steady', 'climb', 'built', 'on', 'daily', 'habits', '.']\n",
      "After stopword removal: ['Success', 'sudden', 'peak', 'steady', 'climb', 'built', 'daily', 'habits']\n",
      "\n",
      "Sentence 3:\n",
      "Original tokens: ['You', 'don', '‚Äô', 't', 'wake', 'up', 'one', 'morning', 'and', 'find', 'you', '‚Äô', 're', 'great', ';', 'you', 'become', 'great', 'because', 'you', 'kept', 'showing', 'up', ',', 'kept', 'trying', ',', 'and', 'kept', 'learning', '.']\n",
      "After stopword removal: ['wake', 'one', 'morning', 'find', 'great', 'become', 'great', 'kept', 'showing', 'kept', 'trying', 'kept', 'learning']\n",
      "\n",
      "Sentence 4:\n",
      "Original tokens: ['Mistakes', 'will', 'happen', '‚Äî', 'that', '‚Äô', 's', 'inevitable', '.']\n",
      "After stopword removal: ['Mistakes', 'happen', 'inevitable']\n",
      "\n",
      "Sentence 5:\n",
      "Original tokens: ['What', 'matters', 'is', 'whether', 'you', 'let', 'them', 'define', 'you', 'or', 'refine', 'you', '.']\n",
      "After stopword removal: ['matters', 'whether', 'let', 'define', 'refine']\n",
      "\n",
      "Sentence 6:\n",
      "Original tokens: ['Stay', 'calm', 'under', 'pressure', ',', 'focus', 'only', 'on', 'what', 'you', 'can', 'control', '‚Äî', 'your', 'preparation', ',', 'your', 'intent', ',', 'and', 'your', 'effort', '.']\n",
      "After stopword removal: ['Stay', 'calm', 'pressure', 'focus', 'control', 'preparation', 'intent', 'effort']\n",
      "\n",
      "Sentence 7:\n",
      "Original tokens: ['The', 'scoreboard', 'may', 'shift', ',', 'the', 'crowd', 'may', 'silence', ',', 'but', 'your', 'character', 'stays', 'intact', 'if', 'you', '‚Äô', 've', 'done', 'the', 'right', 'things', '.']\n",
      "After stopword removal: ['scoreboard', 'may', 'shift', 'crowd', 'may', 'silence', 'character', 'stays', 'intact', 'done', 'right', 'things']\n",
      "\n",
      "Sentence 8:\n",
      "Original tokens: ['Let', 'your', 'purpose', 'drive', 'your', 'performance', ',', 'not', 'the', 'applause', '.']\n",
      "After stopword removal: ['Let', 'purpose', 'drive', 'performance', 'applause']\n",
      "\n",
      "Sentence 9:\n",
      "Original tokens: ['Because', 'in', 'the', 'end', ',', 'it', '‚Äô', 's', 'not', 'about', 'trophies', 'or', 'titles', '‚Äî', 'it', '‚Äô', 's', 'about', 'the', 'process', ',', 'the', 'grind', ',', 'and', 'the', 'faith', 'that', 'carried', 'you', 'through', '.']\n",
      "After stopword removal: ['end', 'trophies', 'titles', 'process', 'grind', 'faith', 'carried']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopword set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# tokens per sentence are assumed in `tokenized_sentences`\n",
    "# 'tokenized_sentences' is a list of lists.\n",
    "# Example:\n",
    "# [\n",
    "#   ['When', 'you', 'step', 'into', 'any', 'challenge', ','],\n",
    "#   ['Success', 'is', 'not', 'a', 'sudden', 'peak', ...],\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "# We want to remove stopwords ('the', 'is', 'and', etc.)\n",
    "# and keep only alphabetic words (ignore punctuation, numbers).\n",
    "\n",
    "filtered_sentences = [\n",
    "    \n",
    "    # üëá Inner list comprehension: process one sentence (list of tokens)\n",
    "    [\n",
    "        w                                   # keep this word\n",
    "        for w in tokens                     # iterate over all words in this sentence\n",
    "        if w.lower() not in stop_words      # condition 1: skip stop words (case-insensitive)\n",
    "        and w.isalpha()                     # condition 2: skip non-alphabetic tokens (e.g., ',', '.', '‚Äî')\n",
    "    ]\n",
    "    # üëá Outer loop: repeat for every sentence in tokenized_sentences\n",
    "    for tokens in tokenized_sentences\n",
    "    \n",
    "]\n",
    "\n",
    "for i, (orig, filt) in enumerate(zip(tokenized_sentences, filtered_sentences), 1):\n",
    "    print(f\"\\nSentence {i}:\")\n",
    "    print(\"Original tokens:\", orig)\n",
    "    print(\"After stopword removal:\", filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d7670",
   "metadata": {},
   "source": [
    "## üìò Step 4 ‚Äî Lemmatization with POS (Per Sentence)\n",
    "\n",
    "We now lemmatize the **filtered tokens** of each sentence using **WordNet Lemmatizer** with **POS tags**.\n",
    "\n",
    "**POS Mapping**\n",
    "\n",
    "$$\n",
    "\\text{map}(t)=\n",
    "\\begin{cases}\n",
    "\\text{wn.ADJ},& t\\text{ starts with }J\\\\\n",
    "\\text{wn.VERB},& t\\text{ starts with }V\\\\\n",
    "\\text{wn.NOUN},& t\\text{ starts with }N\\\\\n",
    "\\text{wn.ADV},& t\\text{ starts with }R\\\\\n",
    "\\text{wn.NOUN},& \\text{otherwise (default)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "$$\n",
    "\\text{Lemma}(w,\\text{POS})=\\operatorname{lookup}_{\\text{WordNet}}\\big(\\text{morph}(w),\\text{POS}\\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6494f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def penn_to_wn(tag: str):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    if tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    if tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    if tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    return wn.NOUN\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"POS-aware lemmatization for a list of tokens.\"\"\"\n",
    "    tagged = pos_tag(tokens)\n",
    "    lemmas = []\n",
    "    for w, t in tagged:\n",
    "        wn_pos = penn_to_wn(t)\n",
    "        lemmas.append(lemmatizer.lemmatize(w.lower(), pos=wn_pos))\n",
    "    return lemmas, tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c67b68c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence 1:\n",
      "After stopword removal: ['step', 'challenge', 'must', 'bring', 'one', 'thing', 'consistency', 'actions']\n",
      "Lemmas: ['step', 'challenge', 'must', 'bring', 'one', 'thing', 'consistency', 'action']\n",
      "\n",
      "Sentence 2:\n",
      "After stopword removal: ['Success', 'sudden', 'peak', 'steady', 'climb', 'built', 'daily', 'habits']\n",
      "Lemmas: ['success', 'sudden', 'peak', 'steady', 'climb', 'build', 'daily', 'habit']\n",
      "\n",
      "Sentence 3:\n",
      "After stopword removal: ['wake', 'one', 'morning', 'find', 'great', 'become', 'great', 'kept', 'showing', 'kept', 'trying', 'kept', 'learning']\n",
      "Lemmas: ['wake', 'one', 'morning', 'find', 'great', 'become', 'great', 'kept', 'show', 'keep', 'try', 'keep', 'learn']\n",
      "\n",
      "Sentence 4:\n",
      "After stopword removal: ['Mistakes', 'happen', 'inevitable']\n",
      "Lemmas: ['mistake', 'happen', 'inevitable']\n",
      "\n",
      "Sentence 5:\n",
      "After stopword removal: ['matters', 'whether', 'let', 'define', 'refine']\n",
      "Lemmas: ['matter', 'whether', 'let', 'define', 'refine']\n",
      "\n",
      "Sentence 6:\n",
      "After stopword removal: ['Stay', 'calm', 'pressure', 'focus', 'control', 'preparation', 'intent', 'effort']\n",
      "Lemmas: ['stay', 'calm', 'pressure', 'focus', 'control', 'preparation', 'intent', 'effort']\n",
      "\n",
      "Sentence 7:\n",
      "After stopword removal: ['scoreboard', 'may', 'shift', 'crowd', 'may', 'silence', 'character', 'stays', 'intact', 'done', 'right', 'things']\n",
      "Lemmas: ['scoreboard', 'may', 'shift', 'crowd', 'may', 'silence', 'character', 'stay', 'intact', 'do', 'right', 'thing']\n",
      "\n",
      "Sentence 8:\n",
      "After stopword removal: ['Let', 'purpose', 'drive', 'performance', 'applause']\n",
      "Lemmas: ['let', 'purpose', 'drive', 'performance', 'applause']\n",
      "\n",
      "Sentence 9:\n",
      "After stopword removal: ['end', 'trophies', 'titles', 'process', 'grind', 'faith', 'carried']\n",
      "Lemmas: ['end', 'trophy', 'title', 'process', 'grind', 'faith', 'carry']\n"
     ]
    }
   ],
   "source": [
    "# Apply per sentence\n",
    "lemma_sentences = []\n",
    "tagged_sentences = []\n",
    "\n",
    "for filt in filtered_sentences:\n",
    "    lemmas, tagged = lemmatize_tokens(filt)\n",
    "    lemma_sentences.append(lemmas)\n",
    "    tagged_sentences.append(tagged)\n",
    "\n",
    "for i, (filt, lem) in enumerate(zip(filtered_sentences, lemma_sentences), 1):\n",
    "    print(f\"\\nSentence {i}:\")\n",
    "    print(\"After stopword removal:\", filt)\n",
    "    print(\"Lemmas:\", lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d31c7",
   "metadata": {},
   "source": [
    "## üìä Step 5 ‚Äî Per-Sentence Summary Table\n",
    "\n",
    "We‚Äôll summarize each sentence with:\n",
    "- Original sentence\n",
    "- Token counts (original ‚Üí filtered)\n",
    "- Lemma list (joined for readability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19656dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tokens (orig)</th>\n",
       "      <th>Tokens (filtered)</th>\n",
       "      <th>Lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\\nWhen you step into any challenge, you must b...</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>step, challenge, must, bring, one, thing, cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Success is not a sudden peak ‚Äî it‚Äôs a steady c...</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>success, sudden, peak, steady, climb, build, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>You don‚Äôt wake up one morning and find you‚Äôre ...</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>wake, one, morning, find, great, become, great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mistakes will happen ‚Äî that‚Äôs inevitable.</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>mistake, happen, inevitable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What matters is whether you let them define yo...</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>matter, whether, let, define, refine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Stay calm under pressure, focus only on what y...</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>stay, calm, pressure, focus, control, preparat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>The scoreboard may shift, the crowd may silenc...</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>scoreboard, may, shift, crowd, may, silence, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Let your purpose drive your performance, not t...</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>let, purpose, drive, performance, applause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Because in the end, it‚Äôs not about trophies or...</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>end, trophy, title, process, grind, faith, carry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #                                           Sentence  \\\n",
       "0           1  \\nWhen you step into any challenge, you must b...   \n",
       "1           2  Success is not a sudden peak ‚Äî it‚Äôs a steady c...   \n",
       "2           3  You don‚Äôt wake up one morning and find you‚Äôre ...   \n",
       "3           4          Mistakes will happen ‚Äî that‚Äôs inevitable.   \n",
       "4           5  What matters is whether you let them define yo...   \n",
       "5           6  Stay calm under pressure, focus only on what y...   \n",
       "6           7  The scoreboard may shift, the crowd may silenc...   \n",
       "7           8  Let your purpose drive your performance, not t...   \n",
       "8           9  Because in the end, it‚Äôs not about trophies or...   \n",
       "\n",
       "   Tokens (orig)  Tokens (filtered)  \\\n",
       "0             20                  8   \n",
       "1             18                  8   \n",
       "2             31                 13   \n",
       "3              9                  3   \n",
       "4             13                  5   \n",
       "5             23                  8   \n",
       "6             24                 12   \n",
       "7             11                  5   \n",
       "8             32                  7   \n",
       "\n",
       "                                              Lemmas  \n",
       "0  step, challenge, must, bring, one, thing, cons...  \n",
       "1  success, sudden, peak, steady, climb, build, d...  \n",
       "2  wake, one, morning, find, great, become, great...  \n",
       "3                        mistake, happen, inevitable  \n",
       "4               matter, whether, let, define, refine  \n",
       "5  stay, calm, pressure, focus, control, preparat...  \n",
       "6  scoreboard, may, shift, crowd, may, silence, c...  \n",
       "7         let, purpose, drive, performance, applause  \n",
       "8   end, trophy, title, process, grind, faith, carry  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for i, (s, orig_tokens, filt_tokens, lemmas) in enumerate(\n",
    "    zip(sentences, tokenized_sentences, filtered_sentences, lemma_sentences), 1\n",
    "):\n",
    "    rows.append({\n",
    "        \"Sentence #\": i,\n",
    "        \"Sentence\": s,\n",
    "        \"Tokens (orig)\": len(orig_tokens),\n",
    "        \"Tokens (filtered)\": len(filt_tokens),\n",
    "        \"Lemmas\": \", \".join(lemmas)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"Sentence #\", \"Sentence\", \"Tokens (orig)\", \"Tokens (filtered)\", \"Lemmas\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3259aac",
   "metadata": {},
   "source": [
    "## üí° Observations\n",
    "\n",
    "- **Stopword removal** reduces noise and keeps content words.\n",
    "- **POS-aware lemmatization** normalizes tense, plurality, and irregular forms (e.g., *running ‚Üí run*, *carried ‚Üí carry*).\n",
    "- Working **per sentence** preserves structure and enables sentence-level analytics (e.g., sentiment by sentence, clause-level features).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Optional: Paragraph-Level Lemma Stats\n",
    "\n",
    "Quickly view the **most frequent lemmas** across the whole paragraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9cf52c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Lemmas:\n",
      "            one : 2\n",
      "          thing : 2\n",
      "          great : 2\n",
      "           keep : 2\n",
      "            let : 2\n",
      "           stay : 2\n",
      "            may : 2\n",
      "           step : 1\n",
      "      challenge : 1\n",
      "           must : 1\n",
      "          bring : 1\n",
      "    consistency : 1\n",
      "         action : 1\n",
      "        success : 1\n",
      "         sudden : 1\n",
      "           peak : 1\n",
      "         steady : 1\n",
      "          climb : 1\n",
      "          build : 1\n",
      "          daily : 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_lemmas = [lemma for sent in lemma_sentences for lemma in sent]\n",
    "freq = Counter(all_lemmas).most_common(20)\n",
    "\n",
    "print(\"Top 20 Lemmas:\")\n",
    "for w, c in freq:\n",
    "    print(f\"{w:>15} : {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf0a85",
   "metadata": {},
   "source": [
    "## üéØ What You Now Have\n",
    "\n",
    "A complete, production-style normalization pipeline:\n",
    "\n",
    "> **sent_tokenize ‚Üí word_tokenize ‚Üí stopword filter ‚Üí POS-aware lemmatize ‚Üí summarize**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28df2d4",
   "metadata": {},
   "source": [
    "# üßÆ Stopword Removal & Lemmatization Flowchart\n",
    "\n",
    "This flow summarizes our entire preprocessing sequence applied to the **MS Dhoni motivational paragraph** üèè  \n",
    "\n",
    "Each stage transforms the text gradually from a raw paragraph to clean, lemmatized tokens ready for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "\\boxed{\\text{Raw Paragraph}} \\\\[6pt]\n",
    "\\Downarrow\\ \\text{sent\\_tokenize()} \\\\[6pt]\n",
    "\\boxed{\\text{Sentence List } \\{S_1, S_2, \\dots, S_n\\}} \\\\[6pt]\n",
    "\\Downarrow\\ \\text{word\\_tokenize()} \\\\[6pt]\n",
    "\\boxed{\\text{Word Tokens per Sentence}} \\\\[6pt]\n",
    "\\Downarrow\\ \\text{Lowercase + Remove Non-Alpha} \\\\[6pt]\n",
    "\\Downarrow\\ \\text{Stopword Filter} \\\\[6pt]\n",
    "\\boxed{\\text{Filtered Tokens } T_i' = \\{w \\mid w \\notin S\\}} \\\\[6pt]\n",
    "\\Downarrow\\ \\text{POS Tagger } \\text{tag}(w) \\\\[6pt]\n",
    "\\Downarrow\\ \\text{Map Penn‚ÜíWordNet POS} \\\\[6pt]\n",
    "\\boxed{\\text{POS-aware Lemmatizer}} \\\\[6pt]\n",
    "\\Downarrow \\\\[6pt]\n",
    "\\boxed{\\text{Lemmatized Sentences } L_i} \\\\[6pt]\n",
    "\\Downarrow\\ \\text{Summarize per sentence / paragraph} \\\\[6pt]\n",
    "\\boxed{\\text{Structured DataFrame or Clean Text Output}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Mathematical View\n",
    "\n",
    "For each sentence $ S_i $:\n",
    "\n",
    "$$\n",
    "S_i \\xrightarrow{\\text{word\\_tokenize}} T_i\n",
    "\\xrightarrow{\\text{filter}} T_i' = \\{w \\in T_i \\mid w \\notin S\\}\n",
    "\\xrightarrow{\\text{lemmatize}} L_i\n",
    "$$\n",
    "\n",
    "and finally summarized as:\n",
    "\n",
    "$$\n",
    "\\text{Paragraph Summary} = \\bigcup_i L_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Summary:**\n",
    "- `sent_tokenize()` ‚Üí separates sentences  \n",
    "- `word_tokenize()` ‚Üí breaks each sentence into words  \n",
    "- Stopword filter ‚Üí removes frequent, low-value words  \n",
    "- POS-aware Lemmatizer ‚Üí normalizes meaningful words  \n",
    "- Summarize ‚Üí combines clean lemmas for NLP analysis (BoW, TF-IDF, etc.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
