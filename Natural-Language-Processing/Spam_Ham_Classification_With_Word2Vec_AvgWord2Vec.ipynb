{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e858b015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages rows: 5572\n",
      "sentence rows after cleaning: 10713\n",
      "avg_vectors.shape: (10713, 100)\n",
      "y.shape: (10713,)\n",
      "kept_texts: 10713\n",
      "label mapping: {np.str_('ham'): np.int64(0), np.str_('spam'): np.int64(1)}\n",
      "Accuracy: 0.918338777414839\n",
      "Confusion matrix:\n",
      " [[1681   18]\n",
      " [ 157  287]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.91      0.99      0.95      1699\n",
      "        spam       0.94      0.65      0.77       444\n",
      "\n",
      "    accuracy                           0.92      2143\n",
      "   macro avg       0.93      0.82      0.86      2143\n",
      "weighted avg       0.92      0.92      0.91      2143\n",
      "\n",
      "text: way back call\n",
      "pred: HAM actual: HAM\n",
      "------------------------------------------------------------\n",
      "text: sms\n",
      "pred: SPAM actual: SPAM\n",
      "------------------------------------------------------------\n",
      "text: okie thanx\n",
      "pred: HAM actual: HAM\n",
      "------------------------------------------------------------\n",
      "text: speak live operator claim call pm\n",
      "pred: SPAM actual: SPAM\n",
      "------------------------------------------------------------\n",
      "text: put stuff first\n",
      "pred: HAM actual: HAM\n",
      "------------------------------------------------------------\n",
      "text: last lt gt\n",
      "pred: HAM actual: HAM\n",
      "------------------------------------------------------------\n",
      "text: enter ur mobile personal details prompts\n",
      "pred: SPAM actual: SPAM\n",
      "------------------------------------------------------------\n",
      "text: theory test\n",
      "pred: HAM actual: HAM\n",
      "------------------------------------------------------------\n",
      "text: could work reach consensus next meeting\n",
      "pred: HAM actual: HAM\n",
      "------------------------------------------------------------\n",
      "text: congratulations\n",
      "pred: SPAM actual: SPAM\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# SMS -> Sentence -> Word2Vec\n",
    "# Classification pipeline\n",
    "# ---------------------------\n",
    "\n",
    "# --- Standard library imports ---\n",
    "import re                       # regular expressions for cleaning text\n",
    "import numpy as np              # numerical arrays and vector math\n",
    "import pandas as pd             # DataFrame for tidy outputs and inspection\n",
    "\n",
    "# --- Third-party NLP / ML imports ---\n",
    "from gensim.utils import simple_preprocess     # light tokenizer + filtering (lowercase, short tokens)\n",
    "import gensim                                   # for Word2Vec training\n",
    "from nltk.tokenize import sent_tokenize         # sentence splitter (preserves sentence boundaries)\n",
    "from nltk.corpus import stopwords               # english stopwords list\n",
    "from tqdm import tqdm                           # progress bars (handy for longer corpora)\n",
    "from sklearn.preprocessing import LabelEncoder  # convert string labels to integers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Load the raw dataset\n",
    "# ---------------------------\n",
    "# The dataset file SMSSpamCollection.txt is expected to be tab-separated (label \\t message)\n",
    "# and contains lines like: \"ham\\tGo until jurong point, ...\".\n",
    "messages = pd.read_csv('SMSSpamCollection.txt', sep='\\t', names=['label', 'message'])\n",
    "# `messages` is now a DataFrame with two columns: 'label' (ham/spam) and 'message' (raw SMS text).\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Prepare stopwords and containers\n",
    "# ---------------------------\n",
    "# Convert NLTK stopwords list to a set for O(1) average-time membership checks.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# We'll collect cleaned sentence-level representations here:\n",
    "sentence_texts = []   # readable cleaned sentence strings (e.g., \"free win prize\")\n",
    "sentence_tokens = []  # token lists per sentence (e.g., [\"free\",\"win\",\"prize\"])\n",
    "sentence_labels = []  # the original message-level label repeated for each sentence\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Sentence-level preprocessing\n",
    "# ---------------------------\n",
    "# We iterate each message, split it into sentences, clean tokens and keep only non-empty sentences.\n",
    "for idx, raw_msg in enumerate(messages['message']):\n",
    "    # sent_tokenize works on the raw message to preserve sentence boundaries\n",
    "    # (do NOT pre-clean punctuation before sentence-splitting ‚Äî that can break boundary detection).\n",
    "    for sent in sent_tokenize(raw_msg):\n",
    "        # Remove any non-alphabet characters and lowercase the sentence.\n",
    "        # This removes numbers, punctuation, URLs, etc., leaving only letters and spaces.\n",
    "        cleaned = re.sub('[^a-zA-Z]', ' ', sent).lower()\n",
    "        # Use gensim.simple_preprocess to tokenize, lowercase (already lower), and remove too-short tokens.\n",
    "        toks = simple_preprocess(cleaned)  # example: [\"free\", \"entry\", \"win\"]\n",
    "        # Remove stopwords to keep the most informative tokens\n",
    "        toks = [w for w in toks if w not in stop_words]\n",
    "        if len(toks) == 0:\n",
    "            # Skip sentences that become empty after cleaning & stopword removal.\n",
    "            # Important: when we skip, we do NOT append a label ‚Äî this preserves alignment.\n",
    "            continue\n",
    "        # Keep a readable sentence text (joined tokens) for later inspection\n",
    "        sentence_texts.append(' '.join(toks))\n",
    "        # Keep the token list for Word2Vec training and embedding creation\n",
    "        sentence_tokens.append(toks)\n",
    "        # Repeat the original parent message label for this sentence so labels stay aligned\n",
    "        sentence_labels.append(messages['label'].iloc[idx])\n",
    "\n",
    "# Quick sanity assertion: all three lists must be of equal length.\n",
    "assert len(sentence_texts) == len(sentence_tokens) == len(sentence_labels), \\\n",
    "    f\"Lengths mismatch: {len(sentence_texts)}, {len(sentence_tokens)}, {len(sentence_labels)}\"\n",
    "\n",
    "# Print counts to confirm how many sentences we obtained vs original messages\n",
    "print(\"messages rows:\", len(messages))\n",
    "print(\"sentence rows after cleaning:\", len(sentence_texts))\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Train Word2Vec on sentence tokens\n",
    "# ---------------------------\n",
    "# Use min_count=1 to include rare words in embedding vocab (avoids many empty sentence vectors).\n",
    "# vector_size=100 sets embedding dimensionality; window=5 controls context window size.\n",
    "# workers uses multiple cores for faster training; epochs controls number of passes.\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentence_tokens,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,   # include rare tokens; change to >=2 to ignore very rare words\n",
    "    workers=4,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Build averaged sentence vectors (skip empties)\n",
    "# ---------------------------\n",
    "def avg_word2vec_from_tokens(tokens, model):\n",
    "    \"\"\"\n",
    "    Given a list of tokens and a gensim Word2Vec model,\n",
    "    return the averaged word vector (mean of in-vocab token vectors).\n",
    "    If none of the tokens are in the model vocab, return None to indicate skip.\n",
    "    \"\"\"\n",
    "    # Collect vectors only for tokens present in model vocabulary\n",
    "    vecs = [model.wv[w] for w in tokens if w in model.wv.key_to_index]\n",
    "    if not vecs:\n",
    "        # Return None to signal that this sentence has no in-vocab words\n",
    "        return None\n",
    "    # Return the mean vector (shape: (vector_size,))\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# We'll build lists and keep only sentences that produced a real vector\n",
    "X_list = []      # will hold 1D numpy arrays (sentence vectors)\n",
    "y_list = []      # corresponding labels (strings at this point)\n",
    "kept_texts = []  # readable texts for the sentences we kept\n",
    "\n",
    "# Iterate aligned triples and compute averaged vectors\n",
    "for toks, label, text in zip(sentence_tokens, sentence_labels, sentence_texts):\n",
    "    v = avg_word2vec_from_tokens(toks, model)\n",
    "    if v is None:\n",
    "        # Skip sentences that yield no in-vocabulary vectors (rare after min_count=1)\n",
    "        # This keeps X/y aligned: we only append label/text when v is available.\n",
    "        continue\n",
    "    X_list.append(v)\n",
    "    y_list.append(label)\n",
    "    kept_texts.append(text)\n",
    "\n",
    "# Stack the list of 1D vectors into a 2D array (n_samples, vector_size)\n",
    "X = np.vstack(X_list)\n",
    "# Convert labels into a NumPy array (strings for now)\n",
    "y = np.array(y_list)\n",
    "\n",
    "# Print shapes so we can debug alignment problems quickly\n",
    "print(\"avg_vectors.shape:\", X.shape)   # expected (n_kept_sentences, vector_size)\n",
    "print(\"y.shape:\", y.shape)\n",
    "print(\"kept_texts:\", len(kept_texts))\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Label encoding (ham/spam -> 0/1)\n",
    "# ---------------------------\n",
    "le = LabelEncoder()\n",
    "# Fit the encoder on string labels then transform to integers\n",
    "y_encoded = le.fit_transform(y)   # e.g., ['ham','spam'] -> [0,1] mapping shown below\n",
    "print(\"label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Train / Test split\n",
    "# ---------------------------\n",
    "# We split X and y_encoded into train and test sets.\n",
    "# We also keep the sample indices (np.arange(len(X))) so we can retrieve original texts for inspection.\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X,\n",
    "    y_encoded,\n",
    "    np.arange(len(X)),       # indices to map back to kept_texts\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded       # maintain class balance between train and test\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Train classifier (RandomForest)\n",
    "# ---------------------------\n",
    "# RandomForest is a robust, non-linear classifier. n_estimators sets number of trees.\n",
    "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "clf.fit(X_train, y_train)  # learn mapping from averaged embeddings -> labels\n",
    "\n",
    "# ---------------------------\n",
    "# 9) Evaluate on test set\n",
    "# ---------------------------\n",
    "y_pred = clf.predict(X_test)  # predicted integer labels for test set\n",
    "\n",
    "# Accuracy (overall fraction correct)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix (rows=true, cols=pred) to inspect false positives / negatives\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# A full classification report (precision, recall, f1) with readable class names\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# ---------------------------\n",
    "# 10) Print sample predictions with readable text\n",
    "# ---------------------------\n",
    "# Use idx_test to map test rows back to their original cleaned sentence text in kept_texts\n",
    "for i in range(10):\n",
    "    # kept_texts[idx_test[i]] is the cleaned sentence corresponding to sample i in the test split\n",
    "    print(\"text:\", kept_texts[idx_test[i]])\n",
    "    print(\"pred:\", 'SPAM' if y_pred[i] == 1 else 'HAM', \"actual:\", 'SPAM' if y_test[i] == 1 else 'HAM')\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fbc212be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# New Text Prediction: SPAM üö®\n"
     ]
    }
   ],
   "source": [
    "# ---- Test with a new message (like TF-IDF style) ----\n",
    "new_text = \"WIN a FREE prize! Click here now.\"\n",
    "tokens = [w for w in simple_preprocess(re.sub('[^a-zA-Z]', ' ', new_text).lower()) if w not in stop_words]\n",
    "new_vec = np.mean([model.wv[w] for w in tokens if w in model.wv.key_to_index], axis=0).reshape(1, -1)\n",
    "pred = clf.predict(new_vec)[0]\n",
    "print(\"\\n# New Text Prediction:\", \"SPAM üö®\" if pred == 1 else \"HAM ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "65425345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  pred_label pred_name  \\\n",
      "0                  WIN a FREE prize! Click here now.           1      spam   \n",
      "1            Hey, are we meeting for lunch tomorrow?           0       ham   \n",
      "2               Claim your reward: http://bit.ly/xyz           0       ham   \n",
      "3  Free entry in 2 a weekly competition to win ti...           1      spam   \n",
      "\n",
      "   prob_spam  \n",
      "0   0.510000  \n",
      "1   0.205000  \n",
      "2   0.301667  \n",
      "3   0.762500  \n"
     ]
    }
   ],
   "source": [
    "# --- Import required libraries ---\n",
    "import re                     # Regular expressions for text cleaning (e.g., removing punctuation/numbers)\n",
    "import numpy as np             # For numerical computations and handling vectors\n",
    "import pandas as pd            # For creating DataFrames (nice tabular results)\n",
    "from gensim.utils import simple_preprocess   # Lightweight tokenizer & cleaner from gensim\n",
    "from nltk.corpus import stopwords            # Stopword list from NLTK (common useless words like 'the', 'is')\n",
    "\n",
    "# --- Prepare reusable stopword set ---\n",
    "# Convert to a Python set for O(1) membership lookup time (much faster than list)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 1Ô∏è‚É£  TEXT PREPROCESSING FUNCTION\n",
    "# ===============================\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes a given text string.\n",
    "    Removes non-alphabetic characters, lowercases everything,\n",
    "    tokenizes into words, and removes English stopwords.\n",
    "    \"\"\"\n",
    "    # Replace all non-letter characters with a space (e.g., numbers, punctuation)\n",
    "    cleaned = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
    "    \n",
    "    # simple_preprocess() ‚Üí tokenizes, lowercases, and removes very short/long tokens automatically\n",
    "    toks = simple_preprocess(cleaned)\n",
    "    \n",
    "    # Remove stopwords to keep only meaningful words\n",
    "    toks = [w for w in toks if w not in stop_words]\n",
    "    \n",
    "    # Return the final list of clean tokens (words)\n",
    "    return toks\n",
    "\n",
    "\n",
    "# ================================================\n",
    "# 2Ô∏è‚É£  FUNCTION: CONVERT TEXTS ‚Üí AVG WORD2VEC VECTORS\n",
    "# ================================================\n",
    "def texts_to_avgvecs(texts, model, fallback='zero'):\n",
    "    \"\"\"\n",
    "    Converts a list of text strings into their average Word2Vec embeddings.\n",
    "\n",
    "    Parameters:\n",
    "        texts (list[str]): Raw text strings to convert.\n",
    "        model (gensim.models.Word2Vec): Trained Word2Vec model with word embeddings.\n",
    "        fallback (str): Strategy when no valid word vectors found:\n",
    "                        'zero' = use zero-vector\n",
    "                        'skip' = mark as None (some rows may be dropped later)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray of shape (n_texts, vector_size): Each row is an averaged word embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    vecs = []  # To store the resulting sentence vectors\n",
    "\n",
    "    # Loop through every input text\n",
    "    for t in texts:\n",
    "        # Step 1: Clean and tokenize the text\n",
    "        toks = preprocess_text(t)\n",
    "\n",
    "        # Step 2: Collect the Word2Vec vectors for words that exist in model vocabulary\n",
    "        word_vecs = [model.wv[w] for w in toks if w in model.wv.key_to_index]\n",
    "\n",
    "        # Step 3: Handle case where none of the words exist in vocabulary\n",
    "        if len(word_vecs) == 0:\n",
    "            if fallback == 'zero':\n",
    "                # Append a zero-vector (so dimensions remain consistent)\n",
    "                vecs.append(np.zeros(model.vector_size))\n",
    "            elif fallback == 'skip':\n",
    "                # Append None (may cause alignment issues, so rarely used)\n",
    "                vecs.append(None)\n",
    "        else:\n",
    "            # Step 4: Average all word vectors to form a single vector for this text\n",
    "            vecs.append(np.mean(word_vecs, axis=0))\n",
    "\n",
    "    # Step 5: Replace None values (if any) with zero-vectors to ensure consistent stacking\n",
    "    final_vecs = [v if v is not None else np.zeros(model.vector_size) for v in vecs]\n",
    "\n",
    "    # Step 6: Stack all vectors vertically to form a 2D NumPy array\n",
    "    # Shape: (n_texts, vector_size)\n",
    "    return np.vstack(final_vecs)\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# 3Ô∏è‚É£  FUNCTION: BATCH PREDICT TEXTS\n",
    "# ===================================\n",
    "def batch_predict_texts(texts, model, clf, return_proba=True):\n",
    "    \"\"\"\n",
    "    Predicts SPAM/HAM for a batch of texts using trained Word2Vec model and classifier.\n",
    "\n",
    "    Parameters:\n",
    "        texts (list[str]): Raw input messages to classify.\n",
    "        model: Trained gensim Word2Vec model (used to compute embeddings).\n",
    "        clf: Trained classifier (e.g., RandomForest, LogisticRegression).\n",
    "        return_proba (bool): Whether to include probability scores (if model supports it).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Table containing text, predicted label, label name, and probability (if available).\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert input texts to averaged Word2Vec vectors\n",
    "    X_batch = texts_to_avgvecs(texts, model, fallback='zero')\n",
    "\n",
    "    # Step 2: Predict class labels (0/1) using the trained classifier\n",
    "    preds = clf.predict(X_batch)\n",
    "\n",
    "    # Step 3: (Optional) Predict probabilities if classifier supports it and user requested it\n",
    "    # - predict_proba() returns an array of shape (n_samples, n_classes)\n",
    "    #   where each column = probability for that class\n",
    "    probs = clf.predict_proba(X_batch) if return_proba and hasattr(clf, \"predict_proba\") else None\n",
    "\n",
    "    # Step 4: Create a DataFrame to hold predictions and corresponding raw text\n",
    "    df = pd.DataFrame({'text': texts, 'pred_label': preds})\n",
    "\n",
    "    # Step 5: (Optional) Map numeric predictions back to string labels if LabelEncoder `le` exists\n",
    "    try:\n",
    "        # le.classes_ contains ['ham', 'spam'] and numeric mapping [0, 1]\n",
    "        df['pred_name'] = df['pred_label'].map({i: name for i, name in enumerate(le.classes_)})\n",
    "    except Exception:\n",
    "        # If label encoder isn't defined, skip silently\n",
    "        pass\n",
    "\n",
    "    # Step 6: Add probability columns if they were computed\n",
    "    if probs is not None:\n",
    "        # Binary classification ‚Üí show only probability for class 1 (SPAM)\n",
    "        if probs.shape[1] == 2:\n",
    "            df['prob_spam'] = probs[:, 1]\n",
    "        else:\n",
    "            # Multi-class ‚Üí create one probability column per class\n",
    "            for i in range(probs.shape[1]):\n",
    "                df[f'prob_class_{i}'] = probs[:, i]\n",
    "\n",
    "    # Step 7: Return the final result DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 4Ô∏è‚É£  EXAMPLE USAGE / DEMO\n",
    "# ========================\n",
    "batch = [\n",
    "    \"WIN a FREE prize! Click here now.\",                  # obvious spam\n",
    "    \"Hey, are we meeting for lunch tomorrow?\",            # normal message\n",
    "    \"Claim your reward: http://bit.ly/xyz\",               # spam-like\n",
    "    \"Free entry in 2 a weekly competition to win tickets\" # spam again\n",
    "]\n",
    "\n",
    "# Step 1: Predict labels + probabilities for this batch\n",
    "result_df = batch_predict_texts(batch, model, clf)\n",
    "\n",
    "# Step 2: Print the resulting table (each row = one input text)\n",
    "print(result_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
