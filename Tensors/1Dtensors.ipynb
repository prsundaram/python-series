{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b55e66a",
   "metadata": {},
   "source": [
    "# üìå Understanding Tensors ‚Äî The Foundation of PyTorch\n",
    "\n",
    "Before we start coding, let's first **understand what a Tensor actually is**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What is a Tensor?\n",
    "\n",
    "A **Tensor** is a **multi-dimensional array**, a generalization of scalars, vectors, and matrices.\n",
    "\n",
    "| Object Type | Dimensions | Example | PyTorch Shape |\n",
    "|--------------|-------------|----------|----------------|\n",
    "| **Scalar**   | 0D | $x = 5$ | `torch.Size([])` |\n",
    "| **Vector**   | 1D | $\\mathbf{v} = [1, 2, 3]$ | `torch.Size([3])` |\n",
    "| **Matrix**   | 2D | $$ A = \\begin{bmatrix} 1 & 2 & 3 \\\\\\\\ 4 & 5 & 6 \\end{bmatrix}$$ | `torch.Size([2, 3])` |\n",
    "| **Tensor (3D+)** | 3D+ | Think of it as a cube or higher-dimensional block | `torch.Size([depth, height, width])` |\n",
    "\n",
    "Formally, a tensor is an element of a **tensor space**, written as:\n",
    "\n",
    "$$\n",
    "\\mathbb{R}^{n_1 \\times n_2 \\times \\dots \\times n_k}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Are Tensors Useful?\n",
    "\n",
    "‚úÖ **Universal Data Structure** ‚Äî Tensors can represent almost any kind of data:\n",
    "- A grayscale image ‚Üí 2D tensor `[height, width]`\n",
    "- An RGB image ‚Üí 3D tensor `[channels, height, width]`\n",
    "- A batch of images ‚Üí 4D tensor `[batch, channels, height, width]`\n",
    "\n",
    "‚úÖ **Hardware Accelerated** ‚Äî PyTorch Tensors run on **GPU** using CUDA for massive speedups.  \n",
    "A tensor operation (like addition, dot product, etc.) can run thousands of times faster on GPU.\n",
    "\n",
    "‚úÖ **Automatic Differentiation** ‚Äî When `requires_grad=True`, PyTorch automatically tracks operations to compute **gradients** during training (key for Deep Learning).\n",
    "\n",
    "‚úÖ **Mathematical Expressiveness** ‚Äî Tensors make it easy to express:\n",
    "- Linear algebra operations  \n",
    "- Convolutions  \n",
    "- Attention mechanisms (in Transformers)  \n",
    "- Gradient computations\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Tensor Analogy\n",
    "\n",
    "| Concept | Intuition |\n",
    "|----------|------------|\n",
    "| **Scalar** | A single number ‚Äî point on a line |\n",
    "| **Vector (1D tensor)** | Direction + magnitude ‚Äî point in 1D/2D/3D space |\n",
    "| **Matrix (2D tensor)** | Grid of numbers ‚Äî transformation in space |\n",
    "| **Tensor (3D+)** | Stack of matrices ‚Äî complex data like images or volumes |\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Visualization Idea\n",
    "\n",
    "Imagine stacking sheets of paper (each a 2D matrix):  \n",
    "The whole stack becomes a **3D tensor**.  \n",
    "If you stack multiple such stacks, you get **4D and beyond**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "- A **Tensor** is the core data structure of PyTorch.  \n",
    "- It‚Äôs a **multi-dimensional array** with GPU acceleration and gradient tracking.  \n",
    "- All deep learning models (CNNs, RNNs, Transformers) process tensors as inputs, weights, and outputs.  \n",
    "- In this notebook, we‚Äôll begin with **1D tensors (vectors)** ‚Äî the simplest and most fundamental form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8367d41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (2.9.0)\n",
      "Requirement already satisfied: filelock in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/psundara/learn/python/python-series/.conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479111dc",
   "metadata": {},
   "source": [
    "# üìò Step 1: Creating 1D Tensors in PyTorch\n",
    "\n",
    "Now that we know what tensors are, let‚Äôs start **creating 1D tensors** (vectors) ‚Äî the simplest form of tensor.\n",
    "\n",
    "A **1D Tensor** is like a mathematical vector:\n",
    "$$\n",
    "\\mathbf{x} = [x_0, x_1, \\dots, x_{n-1}]\n",
    "$$\n",
    "\n",
    "In PyTorch:\n",
    "- 1D tensors have shape `(n,)`\n",
    "- They store data of a specific **dtype** (`float32`, `int64`, etc.)\n",
    "- They can live on different **devices** (CPU, GPU)\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 1. Create from Python List\n",
    "\n",
    "You can create a tensor directly from a Python list using `torch.tensor()`.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 2. Factory Functions\n",
    "\n",
    "PyTorch provides convenient tensor **factory functions**:\n",
    "\n",
    "| Function | Description | Example |\n",
    "|-----------|--------------|----------|\n",
    "| `torch.zeros(n)` | n zeros | `[0, 0, 0]` |\n",
    "| `torch.ones(n)` | n ones | `[1, 1, 1]` |\n",
    "| `torch.arange(start, end, step)` | Range sequence | `[0, 2, 4, 6, 8]` |\n",
    "| `torch.linspace(start, end, steps)` | Evenly spaced values | `[0.0, 0.25, 0.5, 0.75, 1.0]` |\n",
    "| `torch.randn(n)` | Random normal values | e.g., `[0.24, -0.89, 1.02]` |\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ 3. Dtype & Device\n",
    "\n",
    "You can specify:\n",
    "- `dtype=torch.float32` or `torch.int64`\n",
    "- `device='cpu'` or `device='cuda'` (if GPU available)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example Summary\n",
    "\n",
    "We'll:\n",
    "1. Create tensors from lists  \n",
    "2. Use factory functions  \n",
    "3. Explore dtype and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb3db029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "01909239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "## Check torch version \n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5d34eedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1: tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "Shape: torch.Size([8]) | dType: torch.int64 | type: torch.LongTensor\n",
      "v2: tensor([1.0000, 2.1000, 3.0000, 4.7000, 5.2000, 6.9000, 7.0000, 8.0000, 6.0000])\n",
      "Shape: torch.Size([9]) | dType: torch.float32 | type: torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ From Python lists \n",
    "# 1.1 From Integer List\n",
    "list_of_integer = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "v1 = torch.tensor(list_of_integer)\n",
    "print(f\"v1: {v1}\")\n",
    "print(f\"Shape: {v1.shape} | dType: {v1.dtype} | type: {v1.type()}\")\n",
    "\n",
    "# 1.2 From Float List\n",
    "list_of_float = [1.0, 2.1, 3.0, 4.7, 5.2, 6.9, 7.0, 8,6]\n",
    "v2 = torch.tensor(list_of_float)\n",
    "print(f\"v2: {v2}\")\n",
    "print(f\"Shape: {v2.shape} | dType: {v2.dtype} | type: {v2.type()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c9676266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2: tensor([1.2000, 2.3000, 3.4000, 5.0000])\n",
      "Shape: torch.Size([4]) | dType: torch.float32 | type: torch.FloatTensor\n",
      "v3: tensor([1, 2, 3, 4, 5, 6])\n",
      "Shape: torch.Size([6]) | dType: torch.int64 | type: torch.LongTensor\n",
      "v4: tensor([1.2000, 2.5000, 3.7000, 4.3000])\n",
      "Shape: torch.Size([4]) | dType: torch.float32 | type: torch.FloatTensor\n",
      "v4: tensor([1, 2, 3, 4, 5], dtype=torch.int32)\n",
      "Shape: torch.Size([5]) | dType: torch.int32 | type: torch.IntTensor\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ with explict type\n",
    "v2 = torch.tensor([1.2, 2.3, 3.4, 5.0], dtype=torch.float32)\n",
    "print(f\"v2: {v2}\")\n",
    "print(f\"Shape: {v2.shape} | dType: {v2.dtype} | type: {v2.type()}\")\n",
    "\n",
    "v3 = torch.tensor([1, 2, 3, 4, 5, 6], dtype=torch.int64) \n",
    "print(f\"v3: {v3}\")\n",
    "print(f\"Shape: {v3.shape} | dType: {v3.dtype} | type: {v3.type()}\")\n",
    "\n",
    "# 3Ô∏è‚É£ with explict type (FloatTensor)\n",
    "v4 = torch.FloatTensor([1.2, 2.5, 3.7, 4.3])\n",
    "print(f\"v4: {v4}\")\n",
    "print(f\"Shape: {v4.shape} | dType: {v4.dtype} | type: {v4.type()}\")\n",
    "\n",
    "v5 = torch.IntTensor([1, 2 ,3, 4, 5])\n",
    "print(f\"v4: {v5}\")\n",
    "print(f\"Shape: {v5.shape} | dType: {v5.dtype} | type: {v5.type()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "64577c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v5: tensor([1, 2, 3])\n",
      "v6: tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 4Ô∏è‚É£ Convering float to Int\n",
    "\n",
    "list_of_floats = [1.2, 2.0, 3.2]\n",
    "v5 = torch.tensor(list_of_floats, dtype=torch.int64)\n",
    "print(f\"v5: {v5}\")\n",
    "\n",
    "# Another Method to convert float to Int tesnor and vice versa\n",
    "\n",
    "v6 = torch.IntTensor(list_of_floats)\n",
    "print(f\"v6: {v6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5799efb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the tensor object: torch.Size([4])\n",
      "Dimensions of the tensor object: 1\n"
     ]
    }
   ],
   "source": [
    "# 5Ô∏è‚É£ tensor_obj.size() & tensor_object.ndimension() methods\n",
    "\n",
    "tensor_obj = torch.tensor([1, 2, 3, 4], dtype=torch.int64)\n",
    "\n",
    "print(f\"Size of the tensor object: {tensor_obj.size()}\")\n",
    "print(f\"Dimensions of the tensor object: {tensor_obj.ndimension()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e6df3122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zeros: tensor([0., 0., 0., 0., 0.])\n",
      "ones: tensor([1., 1., 1., 1.])\n",
      "arange: tensor([0, 2, 4, 6, 8])\n",
      "linspace: tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
      "linspace_1: tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n",
      "randn: tensor([-0.5177,  1.2713, -0.0881,  0.2428,  0.7371,  0.2405])\n"
     ]
    }
   ],
   "source": [
    "# 6Ô∏è‚É£ Factory functions ‚Äî quick ways to create tensors\n",
    "\n",
    "# üîπ Create a 1D tensor of all zeros (length = 5)\n",
    "zeros = torch.zeros(5)   # [0., 0., 0., 0., 0.]\n",
    "\n",
    "# üîπ Create a 1D tensor of all ones (length = 4)\n",
    "ones = torch.ones(4)     # [1., 1., 1., 1.]\n",
    "\n",
    "# üîπ Create evenly spaced values within a range using step size\n",
    "#    torch.arange(start, end, step)\n",
    "#    end is *exclusive* (like Python range)\n",
    "arange = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\n",
    "\n",
    "\n",
    "\n",
    "# üîπ Create evenly spaced values between two numbers (inclusive)\n",
    "#    torch.linspace(start, end, steps)\n",
    "linspace = torch.linspace(0, 1, 5)  # [0.0000, 0.2500, 0.5000, 0.7500, 1.0000]\n",
    "\n",
    "linspace_1 = torch.linspace(0, 1, steps=5)\n",
    "\n",
    "# üîπ Create random values from a standard normal distribution (mean=0, std=1)\n",
    "randn = torch.randn(6)   # e.g., [0.12, -1.45, 0.73, ...]\n",
    "\n",
    "# ‚úÖ Print results\n",
    "print(\"\\nzeros:\", zeros)\n",
    "print(\"ones:\", ones)\n",
    "print(\"arange:\", arange)\n",
    "print(\"linspace:\", linspace)\n",
    "print(\"linspace_1:\", linspace_1)\n",
    "print(\"randn:\", randn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6761ab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 7Ô∏è‚É£ Device demonstration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "v_device = torch.tensor([10., 20., 30.], device=device)\n",
    "print(\"\\nCreated on device:\", v_device.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25dacf9",
   "metadata": {},
   "source": [
    "# üìò Step 2: Indexing, Slicing, and Views\n",
    "\n",
    "Now that we can create tensors, let‚Äôs learn how to **access**, **slice**, and **manipulate** parts of them.\n",
    "\n",
    "Just like lists and NumPy arrays, PyTorch tensors support **Pythonic indexing and slicing**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Indexing\n",
    "\n",
    "For a 1D tensor $\\mathbf{t} = [t_0, t_1, t_2, \\dots, t_{n-1}]$:\n",
    "\n",
    "- `t[i]` ‚Üí gives the *i-th element* (0-based)\n",
    "- `t[-1]` ‚Üí gives the *last element*\n",
    "\n",
    "‚úÖ **Example:**  \n",
    "If  \n",
    "$$\n",
    "t = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "$$  \n",
    "then  \n",
    "$t[0] = 0$, $t[3] = 3$, $t[-1] = 9$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Slicing\n",
    "\n",
    "You can extract **sub-tensors** using:\n",
    "```python\n",
    "t[start:end:step]\n",
    "```\n",
    "- `start` ‚Üí index to begin (default = 0)\n",
    "- `end` ‚Üí index to stop before (excluded)\n",
    "- `step` ‚Üí stride length (default = 1)\n",
    "\n",
    "### ‚úÖ Examples\n",
    "```python\n",
    "t[2:6]    # elements 2,3,4,5\n",
    "t[:5]     # first 5 elements\n",
    "t[5:]     # elements from index 5 to end\n",
    "t[::2]    # every second element\n",
    "```\n",
    "---\n",
    "## üîπ 3. Negative Indexing\n",
    "- t[-1] ‚Üí last element\n",
    "- t[-3:] ‚Üí last 3 elements\n",
    "- t[:-3] ‚Üí all except last 3\n",
    "---\n",
    "\n",
    "## üîπ 4. Views vs Copies\n",
    "Slicing in PyTorch returns a view, not a copy ‚Äî meaning the sliced tensor shares memory with the original tensor.\n",
    "\n",
    "If you modify the view, the original tensor will change too.\n",
    " - To make a copy, use `.clone()`.\n",
    "---\n",
    "\n",
    "## ‚úÖ Quick Recap\n",
    "\n",
    "| Operation   | Description    | Example            | Result      |\n",
    "| ----------- | -------------- | ------------------ | ----------- |\n",
    "| `t[i]`      | Single element | `t[3]`             | scalar      |\n",
    "| `t[a:b]`    | Slice          | `t[2:5]`           | `[2, 3, 4]` |\n",
    "| `t[::-1]`   | Reverse        | `[9,8,7,...,0]`    |             |\n",
    "| `t.clone()` | Copy           | Independent tensor |             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "42bae93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "t[0] -> tensor(0)\n",
      "t[3] -> tensor(3)\n",
      "t[-1] -> tensor(9)\n",
      "\n",
      "t[2:6] -> tensor([2, 3, 4, 5])\n",
      "t[:5]  -> tensor([0, 1, 2, 3, 4])\n",
      "t[5:]  -> tensor([5, 6, 7, 8, 9])\n",
      "t[::2] -> tensor([0, 2, 4, 6, 8])\n",
      "torch.flip -> tensor([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n",
      "\n",
      "t[-3:]  -> tensor([7, 8, 9])\n",
      "t[:-3]  -> tensor([0, 1, 2, 3, 4, 5, 6])\n",
      "\n",
      "orig before modifying view: tensor([0., 1., 2., 3., 4., 5.])\n",
      "orig after modifying view: tensor([ 0., 99.,  2.,  3.,  4.,  5.])\n",
      "\n",
      "orig after modifying copy: tensor([ 0., 99.,  2.,  3.,  4.,  5.])\n",
      "copy: tensor([-1.,  2.,  3.])\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Create a simple 1D tensor\n",
    "t = torch.arange(10)\n",
    "print(\"Original tensor:\", t)\n",
    "\n",
    "# 2Ô∏è‚É£ Indexing examples\n",
    "print(\"\\nt[0] ->\", t[0])       # first element\n",
    "print(\"t[3] ->\", t[3])         # 4th element\n",
    "print(\"t[-1] ->\", t[-1])       # last element\n",
    "\n",
    "# 3Ô∏è‚É£ Slicing examples\n",
    "print(\"\\nt[2:6] ->\", t[2:6])   # elements from index 2 to 5\n",
    "print(\"t[:5]  ->\", t[:5])      # first 5 elements\n",
    "print(\"t[5:]  ->\", t[5:])      # elements from 5 to end\n",
    "print(\"t[::2] ->\", t[::2])     # every 2nd element\n",
    "print(\"torch.flip ->\", torch.flip(t, dims=[0]))   # reverse the tensor\n",
    "\n",
    "# 4Ô∏è‚É£ Negative slicing\n",
    "print(\"\\nt[-3:]  ->\", t[-3:])  # last 3 elements\n",
    "print(\"t[:-3]  ->\", t[:-3])    # all except last 3\n",
    "\n",
    "# 5Ô∏è‚É£ Views vs copies demonstration\n",
    "orig = torch.arange(6, dtype=torch.float32)\n",
    "view = orig[1:4]  # this is a view (shares memory)\n",
    "print(\"\\norig before modifying view:\", orig)\n",
    "view[0] = 99.0\n",
    "print(\"orig after modifying view:\", orig)  # changed!\n",
    "\n",
    "# To make a copy, use clone()\n",
    "copy = orig[1:4].clone()\n",
    "copy[0] = -1.0\n",
    "print(\"\\norig after modifying copy:\", orig)  # unchanged\n",
    "print(\"copy:\", copy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f48a4",
   "metadata": {},
   "source": [
    "# üìò Step 3: Tensor Functions and Reductions\n",
    "\n",
    "Now that we know how to create and slice tensors,  \n",
    "let‚Äôs explore the **mathematical operations** and **summary (reduction) functions** that PyTorch provides.  \n",
    "These are essential for deep-learning computations and transformations.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1Ô∏è‚É£ Element-wise Mathematical Functions\n",
    "\n",
    "These functions operate **individually on each element** of a tensor ‚Äî  \n",
    "just like NumPy‚Äôs universal functions (ufuncs).\n",
    "\n",
    "| Function | Description | Example (Math) |\n",
    "|-----------|--------------|----------------|\n",
    "| `torch.add(x, y)` | Addition | $z_i = x_i + y_i$ |\n",
    "| `torch.sub(x, y)` | Subtraction | $z_i = x_i - y_i$ |\n",
    "| `torch.mul(x, y)` | Multiplication | $z_i = x_i \\times y_i$ |\n",
    "| `torch.div(x, y)` | Division | $z_i = x_i / y_i$ |\n",
    "| `torch.pow(x, 2)` | Power | $x_i^2$ |\n",
    "| `torch.sqrt(x)` | Square root | $\\sqrt{x_i}$ |\n",
    "| `torch.exp(x)` | Exponential | $e^{x_i}$ |\n",
    "| `torch.log(x)` | Natural log | $\\ln(x_i)$ |\n",
    "| `torch.sin(x)` / `torch.cos(x)` | Trigonometric ops | $\\sin(x_i)$, $\\cos(x_i)$ |\n",
    "\n",
    "üß† **Note:** These are **vectorized** ‚Äî they act on all elements without loops and run efficiently on GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2Ô∏è‚É£ Reduction Functions\n",
    "\n",
    "Reduction functions **aggregate** values of a tensor into a single result  \n",
    "(e.g., mean, sum, or max).  \n",
    "They‚Äôre used to compute statistics, metrics, or losses.\n",
    "\n",
    "| Function | Description | Formula |\n",
    "|-----------|--------------|----------|\n",
    "| `t.sum()` | Sum of all elements | $\\sum_i t_i$ |\n",
    "| `t.mean()` | Mean value | $\\frac{1}{n}\\sum_i t_i$ |\n",
    "| `t.median()` | Median value | middle element (sorted) |\n",
    "| `t.max()` / `t.min()` | Maximum / Minimum | $\\max(t_i)$, $\\min(t_i)$ |\n",
    "| `t.std()` / `t.var()` | Standard deviation / Variance | $\\sqrt{\\mathrm{Var}(t)}$, $\\mathrm{Var}(t)$ |\n",
    "\n",
    "You can also get the **index** of the max/min value with `t.argmax()` and `t.argmin()`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3Ô∏è‚É£ Mixed Example ‚Äî Element-wise + Reduction\n",
    "\n",
    "Sometimes we apply a **mathematical operation first**,  \n",
    "then a **reduction operation** to summarize the result.  \n",
    "This is common in deep-learning pipelines (for example, activation ‚Üí loss).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "‚Üí Compute the mean of sine values across a tensor:  \n",
    "`torch.sin(x).mean()`\n",
    "\n",
    "‚Üí Compute the sum of exponentials:  \n",
    "`torch.exp(x).sum()`\n",
    "\n",
    "**Mathematical Form:**\n",
    "\n",
    "$\\text{mean}(\\sin(x)) = \\frac{1}{n}\\sum_{i=1}^{n}\\sin(x_i)$\n",
    "\n",
    "üß© This pattern is common in neural networks ‚Äî apply a non-linear transformation, then aggregate (like average loss or normalization).\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4Ô∏è‚É£ Functions with Dimensions (`dim=`)\n",
    "\n",
    "Most reduction functions can also work **along a specific dimension**  \n",
    "for multi-dimensional tensors.\n",
    "\n",
    "For a 2D tensor $A \\in \\mathbb{R}^{m \\times n}$:\n",
    "\n",
    "- `A.sum(dim=0)` ‚Üí sums **down the rows**, producing shape `[n]`  \n",
    "- `A.sum(dim=1)` ‚Üí sums **across columns**, producing shape `[m]`\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Given:  \n",
    "$A = \\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6\\end{bmatrix}$  \n",
    "\n",
    "Then:  \n",
    "$A.sum(dim=0) = [5, 7, 9]$  \n",
    "$A.sum(dim=1) = [6, 15]$\n",
    "\n",
    "This **dimension-wise control** is critical in ML ‚Äî  \n",
    "e.g., computing loss per batch (`dim=1`) or feature-wise normalization (`dim=0`).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Category | Example | Description |\n",
    "|-----------|----------|-------------|\n",
    "| Element-wise | `torch.sin(x)` | Operates on each tensor element |\n",
    "| Reduction | `t.mean()` | Aggregates tensor into a scalar |\n",
    "| Combined | `torch.exp(x).sum()` | Element-wise + reduction |\n",
    "| Dim-wise | `t.sum(dim=1)` | Reduces along a specific axis |\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Key Takeaway:**  \n",
    "Tensor functions make PyTorch expressive and fast ‚Äî all these computations are vectorized, GPU-optimized, and fundamental for ML & DL pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0f8629f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([1., 2., 3., 4., 5.])\n",
      "\n",
      "üîπ Element-wise Mathematical Operations:\n",
      "Square (t ** 2): tensor([ 1.,  4.,  9., 16., 25.])\n",
      "Square Root: tensor([1.0000, 1.4142, 1.7321, 2.0000, 2.2361])\n",
      "Exponential: tensor([  2.7183,   7.3891,  20.0855,  54.5982, 148.4132])\n",
      "Logarithm: tensor([0.0000, 0.6931, 1.0986, 1.3863, 1.6094])\n",
      "Sine: tensor([ 0.8415,  0.9093,  0.1411, -0.7568, -0.9589])\n",
      "Cosine: tensor([ 0.5403, -0.4161, -0.9900, -0.6536,  0.2837])\n",
      "\n",
      "Addition (t + a): tensor([11., 22., 33., 44., 55.])\n",
      "Subtraction (a - t): tensor([ 9., 18., 27., 36., 45.])\n",
      "Multiplication (t * 2): tensor([ 2.,  4.,  6.,  8., 10.])\n",
      "Division (a / t): tensor([10., 10., 10., 10., 10.])\n",
      "\n",
      "üîπ Reduction Functions:\n",
      "Sum: tensor(15.)\n",
      "Mean: tensor(3.)\n",
      "Median: tensor(3.)\n",
      "Min: tensor(1.)\n",
      "Max: tensor(5.)\n",
      "Variance: tensor(2.5000)\n",
      "Standard Deviation: tensor(1.5811)\n",
      "Index of Max (argmax): tensor(4)\n",
      "Index of Min (argmin): tensor(0)\n",
      "\n",
      "üîπ Mixed Example ‚Äî Element-wise + Reduction:\n",
      "x: tensor([0.0000, 0.7854, 1.5708, 2.3562, 3.1416])\n",
      "Mean of sin(x): tensor(0.4828)\n",
      "Sum of exp(x): tensor(41.6952)\n",
      "\n",
      "üîπ Functions with Dimensions (dim=):\n",
      "Matrix A:\n",
      " tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "\n",
      "Sum along dim=0 (column-wise): tensor([5., 7., 9.])\n",
      "Sum along dim=1 (row-wise): tensor([ 6., 15.])\n",
      "\n",
      "Mean along dim=0 (column-wise): tensor([2.5000, 3.5000, 4.5000])\n",
      "Mean along dim=1 (row-wise): tensor([2., 5.])\n"
     ]
    }
   ],
   "source": [
    "# üìò Step 3: Tensor Functions and Reductions ‚Äî Code Practice\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# 1Ô∏è‚É£ Create a simple 1D tensor for demonstration\n",
    "t = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "print(\"Tensor:\", t)\n",
    "\n",
    "# ===========================================================\n",
    "# üîπ 1. Element-wise Mathematical Functions\n",
    "# ===========================================================\n",
    "\n",
    "print(\"\\nüîπ Element-wise Mathematical Operations:\")\n",
    "\n",
    "# Square each element\n",
    "print(\"Square (t ** 2):\", torch.pow(t, 2))\n",
    "\n",
    "# Square root of each element\n",
    "print(\"Square Root:\", torch.sqrt(t))\n",
    "\n",
    "# Exponential function e^x\n",
    "print(\"Exponential:\", torch.exp(t))\n",
    "\n",
    "# Natural logarithm (ln x)\n",
    "print(\"Logarithm:\", torch.log(t))\n",
    "\n",
    "# Trigonometric functions\n",
    "print(\"Sine:\", torch.sin(t))\n",
    "print(\"Cosine:\", torch.cos(t))\n",
    "\n",
    "# Arithmetic operations between tensors\n",
    "a = torch.tensor([10.0, 20.0, 30.0, 40.0, 50.0])\n",
    "print(\"\\nAddition (t + a):\", torch.add(t, a))\n",
    "print(\"Subtraction (a - t):\", torch.sub(a, t))\n",
    "print(\"Multiplication (t * 2):\", torch.mul(t, 2))\n",
    "print(\"Division (a / t):\", torch.div(a, t))\n",
    "\n",
    "# ===========================================================\n",
    "# üîπ 2. Reduction Functions\n",
    "# ===========================================================\n",
    "\n",
    "print(\"\\nüîπ Reduction Functions:\")\n",
    "\n",
    "# Sum of all elements\n",
    "print(\"Sum:\", t.sum())\n",
    "\n",
    "# Mean (average) of all elements\n",
    "print(\"Mean:\", t.mean())\n",
    "\n",
    "# Median value\n",
    "print(\"Median:\", t.median())\n",
    "\n",
    "# Minimum and maximum values\n",
    "print(\"Min:\", t.min())\n",
    "print(\"Max:\", t.max())\n",
    "\n",
    "# Variance and standard deviation\n",
    "print(\"Variance:\", t.var())\n",
    "print(\"Standard Deviation:\", t.std())\n",
    "\n",
    "# Index of max and min\n",
    "print(\"Index of Max (argmax):\", t.argmax())\n",
    "print(\"Index of Min (argmin):\", t.argmin())\n",
    "\n",
    "# ===========================================================\n",
    "# üîπ 3. Mixed Example ‚Äî Element-wise + Reduction\n",
    "# ===========================================================\n",
    "\n",
    "print(\"\\nüîπ Mixed Example ‚Äî Element-wise + Reduction:\")\n",
    "\n",
    "# Create a tensor of evenly spaced values between 0 and œÄ\n",
    "x = torch.linspace(0, torch.pi, 5)\n",
    "print(\"x:\", x)\n",
    "\n",
    "# Mean of sine values\n",
    "print(\"Mean of sin(x):\", torch.sin(x).mean())\n",
    "\n",
    "# Sum of exponential values\n",
    "print(\"Sum of exp(x):\", torch.exp(x).sum())\n",
    "\n",
    "# ===========================================================\n",
    "# üîπ 4. Functions with Dimensions (dim=)\n",
    "# ===========================================================\n",
    "\n",
    "print(\"\\nüîπ Functions with Dimensions (dim=):\")\n",
    "\n",
    "# Create a 2D tensor (matrix)\n",
    "A = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]], dtype=torch.float32)\n",
    "print(\"Matrix A:\\n\", A)\n",
    "\n",
    "# Sum along dimension 0 (down the rows)\n",
    "print(\"\\nSum along dim=0 (column-wise):\", A.sum(dim=0))\n",
    "\n",
    "# Sum along dimension 1 (across columns)\n",
    "print(\"Sum along dim=1 (row-wise):\", A.sum(dim=1))\n",
    "\n",
    "# Mean along dim=0 and dim=1\n",
    "print(\"\\nMean along dim=0 (column-wise):\", A.mean(dim=0))\n",
    "print(\"Mean along dim=1 (row-wise):\", A.mean(dim=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97207dfe",
   "metadata": {},
   "source": [
    "# üìò Step 4: Tensor Operations\n",
    "\n",
    "Now that we‚Äôve explored individual tensor functions and reductions,  \n",
    "let‚Äôs move to **Tensor Operations** ‚Äî how tensors interact, combine, and transform.  \n",
    "\n",
    "Tensor operations are the heart of PyTorch‚Äôs numerical engine ‚Äî powering everything from vector math to deep neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1Ô∏è‚É£ Arithmetic Operations (Tensor ‚Üî Tensor and Tensor ‚Üî Scalar)\n",
    "\n",
    "PyTorch supports all **basic arithmetic operations** both between tensors  \n",
    "and between tensors and scalars.\n",
    "\n",
    "| Operation | Example | Description | Math |\n",
    "|------------|----------|--------------|------|\n",
    "| Addition | `a + b` or `torch.add(a,b)` | Element-wise addition | $c_i = a_i + b_i$ |\n",
    "| Subtraction | `a - b` or `torch.sub(a,b)` | Element-wise subtraction | $c_i = a_i - b_i$ |\n",
    "| Multiplication | `a * b` or `torch.mul(a,b)` | Element-wise multiplication | $c_i = a_i \\times b_i$ |\n",
    "| Division | `a / b` or `torch.div(a,b)` | Element-wise division | $c_i = a_i / b_i$ |\n",
    "| Power | `a ** 2` or `torch.pow(a,2)` | Element-wise exponentiation | $a_i^2$ |\n",
    "\n",
    "### üîπ Scalar Operations\n",
    "Tensors can interact with **scalars** directly ‚Äî  \n",
    "the scalar is *broadcasted* to match the tensor shape.\n",
    "\n",
    "Examples:\n",
    "- `a + 5` ‚Üí adds 5 to every element  \n",
    "- `a * 10` ‚Üí multiplies every element by 10  \n",
    "- `a / 2` ‚Üí divides each element by 2  \n",
    "\n",
    "Mathematically:  \n",
    "$$\n",
    "a = [1, 2, 3] \\quad \\Rightarrow \\quad a + 5 = [6, 7, 8]\n",
    "$$\n",
    "\n",
    "üß† **Note:**  \n",
    "All these operations are **vectorized** and GPU-accelerated.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2Ô∏è‚É£ Comparison & Logical Operations\n",
    "\n",
    "PyTorch allows **element-wise comparisons** between tensors,  \n",
    "returning Boolean tensors that can be used for masking or conditions.\n",
    "\n",
    "| Type | Function | Example | Description |\n",
    "|------|-----------|----------|-------------|\n",
    "| Comparison | `torch.eq(a,b)` | Equal | $a_i == b_i$ |\n",
    "| „ÄÉ | `torch.ne(a,b)` | Not equal | $a_i \\ne b_i$ |\n",
    "| „ÄÉ | `torch.gt(a,b)` | Greater than | $a_i > b_i$ |\n",
    "| „ÄÉ | `torch.ge(a,b)` | Greater or equal | $a_i \\ge b_i$ |\n",
    "| „ÄÉ | `torch.lt(a,b)` | Less than | $a_i < b_i$ |\n",
    "| „ÄÉ | `torch.le(a,b)` | Less or equal | $a_i \\le b_i$ |\n",
    "| Logical | `torch.logical_and(x,y)` | AND | $x_i \\land y_i$ |\n",
    "| „ÄÉ | `torch.logical_or(x,y)` | OR | $x_i \\lor y_i$ |\n",
    "| „ÄÉ | `torch.logical_not(x)` | NOT | $\\neg x_i$ |\n",
    "\n",
    "‚úÖ Boolean tensors are crucial for **filtering**, **thresholding**, or **conditional masking** in ML pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3Ô∏è‚É£ Matrix Operations (Linear Algebra)\n",
    "\n",
    "Matrix operations are central to all deep-learning layers.\n",
    "\n",
    "| Operation | Function | Description | Math |\n",
    "|------------|-----------|-------------|------|\n",
    "| Dot Product | `torch.dot(a,b)` | Inner product (1D tensors) | $a \\cdot b = \\sum_i a_i b_i$ |\n",
    "| Matrix Multiplication | `torch.mm(A,B)` or `A @ B` | 2D matrix multiplication | $C = A \\times B$ |\n",
    "| Batch Matrix Mult | `torch.bmm(A,B)` | 3D tensors (batch of matrices) | $C_i = A_i B_i$ |\n",
    "| Transpose | `A.T` or `torch.transpose(A,0,1)` | Swap axes | $A^T$ |\n",
    "| Determinant | `torch.det(A)` | Square matrix determinant | $\\det(A)$ |\n",
    "| Inverse | `torch.inverse(A)` | Matrix inverse | $A^{-1}$ |\n",
    "\n",
    "üßÆ **Example:**\n",
    "$$\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{bmatrix}, \\quad\n",
    "B = \n",
    "\\begin{bmatrix}\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{bmatrix}\n",
    "$$  \n",
    "Then:  \n",
    "$A @ B =\n",
    "\\begin{bmatrix}\n",
    "19 & 22 \\\\\n",
    "43 & 50\n",
    "\\end{bmatrix}$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4Ô∏è‚É£ Broadcasting Rules\n",
    "\n",
    "PyTorch supports **broadcasting** ‚Äî automatic expansion of smaller tensors  \n",
    "to match the shape of larger ones for element-wise operations.\n",
    "\n",
    "Example shapes:\n",
    "- `a` ‚Üí `(3, 1)`\n",
    "- `b` ‚Üí `(1, 4)`\n",
    "\n",
    "Operation ‚Üí `a + b`  \n",
    "Result ‚Üí `(3, 4)`  \n",
    "\n",
    "**Broadcasting Rules:**\n",
    "1. Compare dimensions from **right to left**  \n",
    "2. Dimensions must be **equal** or **one must be 1**  \n",
    "3. Otherwise ‚Üí broadcasting error\n",
    "\n",
    "**Example Visualization:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "10 & 20 & 30 & 40\n",
    "\\end{bmatrix}\n",
    "\\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "11 & 21 & 31 & 41 \\\\\n",
    "12 & 22 & 32 & 42 \\\\\n",
    "13 & 23 & 33 & 43\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 5Ô∏è‚É£ In-place vs Out-of-place Operations (Preview)\n",
    "\n",
    "In PyTorch, **in-place operations** modify the tensor in memory  \n",
    "and are denoted with an underscore `_`, e.g. `t.add_(1)`.\n",
    "\n",
    "| Type | Example | Description |\n",
    "|------|----------|-------------|\n",
    "| Out-of-place | `y = x + 1` | Creates a new tensor |\n",
    "| In-place | `x.add_(1)` | Updates tensor `x` directly |\n",
    "\n",
    "‚ö†Ô∏è **Important:**  \n",
    "Avoid in-place ops on tensors that require gradients,  \n",
    "as they can disrupt the computation graph.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Category | Example | Description |\n",
    "|-----------|----------|-------------|\n",
    "| Arithmetic | `a + b`, `a * 5`, `a / 2` | Element-wise & scalar ops |\n",
    "| Comparison | `torch.eq(a,b)` | Boolean comparisons |\n",
    "| Matrix | `A @ B`, `torch.mm()` | Linear algebra ops |\n",
    "| Broadcasting | Auto shape expansion | Smart dimension matching |\n",
    "| In-place | `x.add_(1)` | Updates tensor directly in memory |\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Key Takeaway:**  \n",
    "Tensor operations (including scalar arithmetic) are the foundation of PyTorch‚Äôs power ‚Äî  \n",
    "fast, GPU-optimized, and seamlessly broadcasted for efficient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f20f4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([1., 2., 3.])\n",
      "b: tensor([10., 20., 30.])\n",
      "a + b  -> tensor([11., 22., 33.])\n",
      "a - b  -> tensor([ -9., -18., -27.])\n",
      "a * b  -> tensor([10., 40., 90.])\n",
      "a / b  -> tensor([0.1000, 0.1000, 0.1000])\n",
      "\n",
      "a + 5  -> tensor([6., 7., 8.])\n",
      "a * 2  -> tensor([2., 4., 6.])\n",
      "a / 2  -> tensor([0.5000, 1.0000, 1.5000])\n",
      "a ** 2 -> tensor([1., 4., 9.])\n",
      "\n",
      "a == 2        -> tensor([False,  True, False])\n",
      "a > 1.5       -> tensor([False,  True,  True])\n",
      "torch.eq(a,b) -> tensor([False, False, False])\n",
      "\n",
      "mask (a > 1.5): tensor([False,  True,  True])\n",
      "masked values (a[mask]): tensor([2., 3.])\n",
      "logical_and mask with (b > 10): tensor([False,  True,  True])\n",
      "\n",
      "r shape: torch.Size([1, 3]) c shape: torch.Size([3, 1])\n",
      "r + a -> torch.Size([1, 3]) \n",
      " tensor([[2., 4., 6.]])\n",
      "c + r -> torch.Size([3, 3]) \n",
      " tensor([[11., 12., 13.],\n",
      "        [21., 22., 23.],\n",
      "        [31., 32., 33.]])\n",
      "\n",
      "Matrix A:\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "Matrix B:\n",
      " tensor([[5., 6.],\n",
      "        [7., 8.]])\n",
      "\n",
      "A @ B =\n",
      " tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "\n",
      "A.T =\n",
      " tensor([[1., 3.],\n",
      "        [2., 4.]])\n",
      "\n",
      "Dot product v1 ¬∑ v2 -> tensor(32.)\n",
      "\n",
      "det(A) -> tensor(-2.)\n",
      "A^{-1} ->\n",
      " tensor([[-2.0000,  1.0000],\n",
      "        [ 1.5000, -0.5000]])\n",
      "A @ A^{-1} ->\n",
      " tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "\n",
      "batchA shape: torch.Size([2, 2, 2]) batchB shape: torch.Size([2, 2, 2])\n",
      "batchC shape: torch.Size([2, 2, 2]) \n",
      " tensor([[[19., 22.],\n",
      "         [43., 50.]],\n",
      "\n",
      "        [[19., 22.],\n",
      "         [43., 50.]]])\n",
      "\n",
      "Before in-place: x = tensor([1., 2., 3.], requires_grad=True)  y = tensor([2., 3., 4.], grad_fn=<AddBackward0>)\n",
      "In-place operation failed (as expected when it breaks autograd): a leaf Variable that requires grad is being used in an in-place operation.\n",
      "Original (x_safe): tensor([1., 2., 3.], requires_grad=True) Clone after in-place: tensor([2., 3., 4.], grad_fn=<AddBackward0>)\n",
      "Gradients (dL/dx2): tensor([1., 1., 1.])\n",
      "\n",
      "t:\n",
      " tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.]])\n",
      "mask:\n",
      " tensor([[False, False],\n",
      "        [False,  True],\n",
      "        [ True,  True]])\n",
      "t[mask] -> tensor([3., 4., 5.])\n",
      "t2 after mask assignment:\n",
      " tensor([[ 0.,  1.],\n",
      "        [ 2., -1.],\n",
      "        [-1., -1.]])\n",
      "\n",
      "‚úÖ Completed tensor operations demo (element-wise, scalar ops, comparisons, broadcasting, matrix ops, and in-place vs out-of-place).\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Step 4 ‚Äî Tensor Operations: examples with clear comments\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 1) Basic tensor-to-tensor arithmetic (element-wise)\n",
    "# --------------------------\n",
    "a = torch.tensor([1.0, 2.0, 3.0])   # 1D float tensor\n",
    "b = torch.tensor([10.0, 20.0, 30.0]) \n",
    "\n",
    "# Element-wise add/sub/mul/div (shapes must match or be broadcastable)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"a + b  ->\", a + b)            # element-wise addition\n",
    "print(\"a - b  ->\", a - b)            # element-wise subtraction\n",
    "print(\"a * b  ->\", a * b)            # element-wise multiplication\n",
    "print(\"a / b  ->\", a / b)            # element-wise division\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 2) Tensor <-> Scalar arithmetic (scalar is broadcast to shape)\n",
    "# --------------------------\n",
    "print(\"a + 5  ->\", a + 5)            # add scalar 5 to every element\n",
    "print(\"a * 2  ->\", a * 2)            # multiply every element by 2\n",
    "print(\"a / 2  ->\", a / 2)            # divide every element by 2\n",
    "print(\"a ** 2 ->\", a ** 2)           # square each element (power)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 3) Comparison & logical ops (produce boolean tensors)\n",
    "# --------------------------\n",
    "print(\"a == 2        ->\", a == 2.0)                      # equality check\n",
    "print(\"a > 1.5       ->\", a > 1.5)                       # greater-than\n",
    "print(\"torch.eq(a,b) ->\", torch.eq(a, b))                # element-wise equality\n",
    "print()\n",
    "\n",
    "# Logical operations (work on boolean tensors)\n",
    "mask = a > 1.5\n",
    "print(\"mask (a > 1.5):\", mask)\n",
    "print(\"masked values (a[mask]):\", a[mask])               # use mask to filter values\n",
    "print(\"logical_and mask with (b > 10):\", torch.logical_and(mask, b > 10))\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 4) Broadcasting examples\n",
    "# --------------------------\n",
    "# a: shape (3,)\n",
    "# row vector r: shape (1,3)\n",
    "# column vector c: shape (3,1)\n",
    "r = torch.tensor([[1.0, 2.0, 3.0]])   # shape (1,3)\n",
    "c = torch.tensor([[10.0], [20.0], [30.0]])  # shape (3,1)\n",
    "\n",
    "print(\"r shape:\", r.shape, \"c shape:\", c.shape)\n",
    "# r + a -> r is (1,3), a is (3,) -> a is treated as (1,3) -> result (1,3)\n",
    "print(\"r + a ->\", (r + a).shape, \"\\n\", r + a)\n",
    "\n",
    "# c + r -> c (3,1) broadcasts with r (1,3) -> result (3,3)\n",
    "print(\"c + r ->\", (c + r).shape, \"\\n\", c + r)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 5) Matrix / Linear algebra operations\n",
    "# --------------------------\n",
    "# Prepare small matrices (float) for linear algebra\n",
    "A = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0]])\n",
    "B = torch.tensor([[5.0, 6.0],\n",
    "                  [7.0, 8.0]])\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Matrix B:\\n\", B)\n",
    "\n",
    "# Matrix multiplication (2D)\n",
    "C = A @ B                   # same as torch.mm(A, B)\n",
    "print(\"\\nA @ B =\\n\", C)\n",
    "\n",
    "# Transpose\n",
    "print(\"\\nA.T =\\n\", A.T)\n",
    "\n",
    "# Dot product (1D vectors)\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "v2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "print(\"\\nDot product v1 ¬∑ v2 ->\", torch.dot(v1, v2))  # scalar\n",
    "\n",
    "# Determinant and inverse (A is invertible)\n",
    "detA = torch.det(A)\n",
    "invA = torch.inverse(A)\n",
    "print(\"\\ndet(A) ->\", detA)\n",
    "print(\"A^{-1} ->\\n\", invA)\n",
    "# Verify A @ invA ~= Identity\n",
    "print(\"A @ A^{-1} ->\\n\", A @ invA)\n",
    "print()\n",
    "\n",
    "# Batch matrix multiplication example (3D tensors)\n",
    "# Create batch of two (2x2) matrices: shape (batch, n, m)\n",
    "batchA = torch.stack([A, A * 2])   # shape (2,2,2)\n",
    "batchB = torch.stack([B, B * 0.5]) # shape (2,2,2)\n",
    "print(\"batchA shape:\", batchA.shape, \"batchB shape:\", batchB.shape)\n",
    "batchC = torch.bmm(batchA, batchB)  # batch-wise matrix multiplication\n",
    "print(\"batchC shape:\", batchC.shape, \"\\n\", batchC)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 6) In-place vs Out-of-place operations (and autograd caution)\n",
    "# --------------------------\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)  # requires grad\n",
    "y = x + 1.0   # out-of-place: creates new tensor (computation graph preserved)\n",
    "print(\"Before in-place: x =\", x, \" y =\", y)\n",
    "\n",
    "# In-place modification (will raise a runtime error if it breaks grad history)\n",
    "# Example: modifying a leaf tensor that requires_grad is unsafe.\n",
    "try:\n",
    "    x.add_(1.0)   # in-place: modifies x directly\n",
    "    print(\"After in-place add_: x =\", x)\n",
    "except RuntimeError as e:\n",
    "    print(\"In-place operation failed (as expected when it breaks autograd):\", e)\n",
    "\n",
    "# Safer approach: operate on a clone if you need to preserve original or avoid graph issues\n",
    "x_safe = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "x_clone = x_safe.clone()\n",
    "x_clone.add_(1.0)   # in-place on the clone is fine; original x_safe unchanged\n",
    "print(\"Original (x_safe):\", x_safe, \"Clone after in-place:\", x_clone)\n",
    "\n",
    "# Use out-of-place ops when you want to keep computation graph intact:\n",
    "x2 = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y2 = x2 + 1.0       # out-of-place\n",
    "loss = y2.sum()\n",
    "loss.backward()\n",
    "print(\"Gradients (dL/dx2):\", x2.grad)   # gradient computed successfully\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 7) Masking + conditional updates (use boolean masks)\n",
    "# --------------------------\n",
    "t = torch.arange(6.).reshape(3,2)   # shape (3,2)\n",
    "print(\"t:\\n\", t)\n",
    "mask = t > 2.0\n",
    "print(\"mask:\\n\", mask)\n",
    "\n",
    "# Select values with mask (1D view of True elements)\n",
    "print(\"t[mask] ->\", t[mask])\n",
    "\n",
    "# Conditional assignment: set masked elements to -1 (in-place on a copy/cloned tensor if needed)\n",
    "t2 = t.clone()\n",
    "t2[mask] = -1.0\n",
    "print(\"t2 after mask assignment:\\n\", t2)\n",
    "\n",
    "# --------------------------\n",
    "# End of demonstrations\n",
    "# --------------------------\n",
    "print(\"\\n‚úÖ Completed tensor operations demo (element-wise, scalar ops, comparisons, broadcasting, matrix ops, and in-place vs out-of-place).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e6754",
   "metadata": {},
   "source": [
    "# üìò Step 5: Tensor Reshaping & Dimension Manipulation\n",
    "\n",
    "Deep learning models often require tensors to be reshaped, flattened, or expanded ‚Äî  \n",
    "for example, converting an image tensor into a vector or adding a batch dimension.  \n",
    "\n",
    "In PyTorch, we use **view**, **reshape**, **squeeze**, **unsqueeze**, **flatten**, **expand**, and **repeat**  \n",
    "to control the *shape* and *dimensionality* of tensors efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1Ô∏è‚É£ Tensor Shape Basics\n",
    "\n",
    "Each tensor has a **shape** (tuple of dimensions).\n",
    "\n",
    "$$\n",
    "\\text{Tensor shape} = (B, C, H, W)\n",
    "$$\n",
    "\n",
    "- **B** ‚Üí Batch size  \n",
    "- **C** ‚Üí Channels  \n",
    "- **H, W** ‚Üí Height and Width  \n",
    "\n",
    "Use:\n",
    "- `t.shape` ‚Üí get shape  \n",
    "- `t.ndim` ‚Üí number of dimensions  \n",
    "- `t.numel()` ‚Üí total number of elements  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2Ô∏è‚É£ Reshape vs View\n",
    "\n",
    "Both `reshape()` and `view()` change the tensor‚Äôs shape **without changing the data**.\n",
    "\n",
    "| Function | Description | Example |\n",
    "|-----------|--------------|----------|\n",
    "| `t.reshape(new_shape)` | Returns a reshaped tensor (may copy data if needed) | `t.reshape(2, 3)` |\n",
    "| `t.view(new_shape)` | Returns a view if possible (shares memory) | `t.view(2, 3)` |\n",
    "\n",
    "**Mathematically:**\n",
    "\n",
    "If tensor $t$ has $m \\times n$ elements,  \n",
    "then `t.reshape(a, b)` must satisfy:\n",
    "\n",
    "$$\n",
    "a \\times b = m \\times n\n",
    "$$\n",
    "\n",
    "üß† **Difference:**  \n",
    "- `view()` requires tensor to be **contiguous** in memory.  \n",
    "- `reshape()` automatically creates a copy if required.\n",
    "\n",
    "**Example Idea:**  \n",
    "If a tensor has shape `(6,)`, we can reshape it into `(2, 3)` or `(3, 2)` ‚Äî  \n",
    "since $2 \\times 3 = 3 \\times 2 = 6$.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3Ô∏è‚É£ Contiguity (`contiguous()`)\n",
    "\n",
    "Some operations like `transpose()` or slicing create **non-contiguous** tensors.  \n",
    "A non-contiguous tensor can‚Äôt be safely used with `.view()`.\n",
    "\n",
    "To fix this, use:\n",
    "\n",
    "- `t = t.contiguous().view(new_shape)`\n",
    "\n",
    "‚úÖ Always call `.contiguous()` before `.view()` after a transpose or slice.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4Ô∏è‚É£ Squeeze and Unsqueeze\n",
    "\n",
    "These are used to **add** or **remove** singleton dimensions (size = 1).\n",
    "\n",
    "| Function | Description | Example | Result |\n",
    "|-----------|--------------|----------|---------|\n",
    "| `t.unsqueeze(dim)` | Adds a new dimension of size 1 | `(3,) ‚Üí (1,3)` | Adds axis |\n",
    "| `t.squeeze(dim)` | Removes dimensions of size 1 | `(1,3,1) ‚Üí (3,)` | Removes axis |\n",
    "\n",
    "**Mathematical Intuition:**\n",
    "\n",
    "If  \n",
    "$$\n",
    "t = [1, 2, 3]\n",
    "$$  \n",
    "then  \n",
    "$$\n",
    "t.unsqueeze(0) \\Rightarrow [[1, 2, 3]]\n",
    "$$\n",
    "\n",
    "- `unsqueeze(0)` adds a new axis at the start ‚Üí shape `(1,3)`  \n",
    "- `unsqueeze(1)` adds a new axis in the middle ‚Üí shape `(3,1)`  \n",
    "- `squeeze()` removes axes with size = 1  \n",
    "\n",
    "Used for:  \n",
    "- Adding batch dimensions ‚Üí `x.unsqueeze(0)`  \n",
    "- Removing redundant dimensions ‚Üí `t.squeeze()`\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 5Ô∏è‚É£ Flatten\n",
    "\n",
    "Flattens a tensor into a **1D vector** or from a specific dimension range.\n",
    "\n",
    "| Function | Example | Description |\n",
    "|-----------|----------|--------------|\n",
    "| `torch.flatten(t)` | `shape: (B, C, H, W) ‚Üí (B, C*H*W)` | Flattens all dims except batch |\n",
    "| `torch.flatten(t, start_dim=1)` | Flattens from a specific dimension | Keeps batch intact |\n",
    "\n",
    "**Example Explanation:**\n",
    "\n",
    "For an image tensor of shape `(B=32, C=3, H=28, W=28)`:\n",
    "\n",
    "$$\n",
    "\\text{Flattened shape} = (32, 3 \\times 28 \\times 28) = (32, 2352)\n",
    "$$\n",
    "\n",
    "This is essential before feeding tensors into a **fully connected (linear) layer**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 6Ô∏è‚É£ Expand vs Repeat\n",
    "\n",
    "Both expand and repeat are used to **replicate tensor data**,  \n",
    "but differ in **memory behavior**.\n",
    "\n",
    "| Function | Example | Copies Data? | Description |\n",
    "|-----------|----------|--------------|--------------|\n",
    "| `t.expand(new_shape)` | `t.expand(3, 4)` | ‚ùå No | Creates a *view* (no real memory copy) |\n",
    "| `t.repeat(repeats)` | `t.repeat(2, 3)` | ‚úÖ Yes | Physically copies data in memory |\n",
    "\n",
    "üß† **Rule of Thumb:**\n",
    "- Use `expand()` when you just want to *broadcast* data.  \n",
    "- Use `repeat()` when you need actual repeated data.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "If  \n",
    "$$\n",
    "t = [1, 2, 3]\n",
    "$$  \n",
    "\n",
    "Then:  \n",
    "- `t.expand(3, 3)` ‚Üí uses broadcasting ‚Üí  \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 2 & 3\n",
    "\\end{bmatrix}\n",
    "$$  \n",
    "(no extra memory)\n",
    "\n",
    "- `t.repeat(2, 1)` ‚Üí copies the data twice ‚Üí  \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 2 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 7Ô∏è‚É£ Practical Example Overview\n",
    "\n",
    "Let‚Äôs take $t = [1, 2, 3]$ and visualize how different operations change its shape:\n",
    "\n",
    "| Operation | Output | Shape |\n",
    "|------------|----------|--------|\n",
    "| `t.unsqueeze(0)` | `[[1, 2, 3]]` | `(1, 3)` |\n",
    "| `t.unsqueeze(1)` | `[[1], [2], [3]]` | `(3, 1)` |\n",
    "| `t.squeeze()` | `[1, 2, 3]` | `(3,)` |\n",
    "| `t.expand(3, 3)` | broadcasted rows | `(3, 3)` |\n",
    "| `t.repeat(2, 1)` | duplicated rows | `(2, 3)` |\n",
    "| `t.view(1, 3)` | reshaped | `(1, 3)` |\n",
    "| `t.reshape(3, 1)` | reshaped copy | `(3, 1)` |\n",
    "| `t.flatten()` | flattened | `(3,)` |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Operation | Purpose | Example |\n",
    "|------------|----------|----------|\n",
    "| `view()` / `reshape()` | Change shape | `t.view(2,3)` |\n",
    "| `contiguous()` | Fix memory layout | `t = t.contiguous()` |\n",
    "| `unsqueeze()` / `squeeze()` | Add / remove axes | `t.unsqueeze(0)` |\n",
    "| `flatten()` | Collapse dims | `torch.flatten(t)` |\n",
    "| `expand()` / `repeat()` | Broadcast or copy | `t.expand(3,3)`, `t.repeat(2,1)` |\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Key Takeaway:**  \n",
    "Mastering reshaping and dimensionality control is essential for deep learning ‚Äî  \n",
    "especially when batching images, flattening outputs, or adapting tensors for linear layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e75e0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 0) Helper printer for clarity\n",
    "# --------------------------\n",
    "def p(name, t):\n",
    "    \"\"\"\n",
    "    Clean tensor printer for notebooks.\n",
    "    Shows: name, shape, dtype, device, and contiguity.\n",
    "    Works for all tensor types (CPU/GPU, int/float).\n",
    "    \"\"\"\n",
    "    print(f\"{name:25} | shape={tuple(t.shape)} | dtype={t.dtype} | device={t.device} | contiguous={t.is_contiguous()}\")\n",
    "    print(\"  ->\", t)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c149435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0\n",
      "\n",
      "original t                | shape=(6,) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([0, 1, 2, 3, 4, 5])\n",
      "\n",
      "t.ndim: 1  t.numel(): 6\n",
      "\n",
      "t.reshape(2,3)            | shape=(2, 3) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "t.view(2,3)               | shape=(2, 3) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "M (3x4)                   | shape=(3, 4) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n",
      "Mt = M.T (transposed)     | shape=(4, 3) | dtype=torch.int64 | device=cpu | contiguous=False\n",
      "  -> tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n",
      "\n",
      "Is Mt contiguous? False\n",
      "Attempting Mt.view(-1) may fail if non-contiguous; use contiguous() first.\n",
      "\n",
      "Mt.view(-1) failed (expected for non-contiguous): RuntimeError - view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "Mt.contiguous().view(-1)  | shape=(12,) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\n",
      "\n",
      "a (orig)                  | shape=(3,) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([1., 2., 3.])\n",
      "\n",
      "a.unsqueeze(0)            | shape=(1, 3) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([[1., 2., 3.]])\n",
      "\n",
      "a.unsqueeze(1)            | shape=(3, 1) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "\n",
      "b (1,3,1)                 | shape=(1, 3, 1) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([[[7.],\n",
      "         [8.],\n",
      "         [9.]]])\n",
      "\n",
      "b.squeeze()               | shape=(3,) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([7., 8., 9.])\n",
      "\n",
      "b.squeeze(0)              | shape=(3, 1) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([[7.],\n",
      "        [8.],\n",
      "        [9.]])\n",
      "\n",
      "img (2,3,4)               | shape=(2, 3, 4) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "\n",
      "torch.flatten(img)        | shape=(24,) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n",
      "\n",
      "torch.flatten(img, start_dim=1) | shape=(2, 12) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n",
      "\n",
      "t_small                   | shape=(3,) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([1., 2., 3.])\n",
      "\n",
      "t_small.unsqueeze(1).expand(3,3) | shape=(3, 3) | dtype=torch.float32 | device=cpu | contiguous=False\n",
      "  -> tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n",
      "\n",
      "t_small.repeat(2,1)       | shape=(2, 3) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "\n",
      "Storage ptrs -> expand.data_ptr: 5392283136 repeat.data_ptr: 5392285568\n",
      "Note: different ptrs for repeat => new memory; expand may reuse original storage (broadcast view).\n",
      "\n",
      "x (24)                    | shape=(24,) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n",
      "\n",
      "x.view(2, -1)             | shape=(2, 12) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n",
      "\n",
      "x.reshape(-1, 6)          | shape=(4, 6) | dtype=torch.int64 | device=cpu | contiguous=True\n",
      "  -> tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23]])\n",
      "\n",
      "img2 (8,3,28,28)          | shape=(8, 3, 28, 28) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([[[[ 0.6958,  1.7900, -1.1804,  ..., -0.5599,  1.4451,  0.0306],\n",
      "          [ 0.7165, -2.1348,  0.3078,  ..., -0.5874,  0.6562,  0.2426],\n",
      "          [ 0.7771, -1.1179, -0.4212,  ..., -1.1196, -0.8263,  1.6516],\n",
      "          ...,\n",
      "          [-1.2375, -1.1116, -0.0349,  ..., -1.0040,  0.9875,  0.3098],\n",
      "          [-0.0915, -0.3628, -1.0270,  ...,  2.2144, -1.4246,  0.7763],\n",
      "          [ 1.5844,  1.6395,  1.0195,  ..., -0.7117, -0.3368,  0.4284]],\n",
      "\n",
      "         [[-1.7110,  1.0963, -1.7119,  ..., -0.1076,  0.6720, -0.5031],\n",
      "          [ 0.7449, -0.5473, -0.1483,  ...,  0.2495,  0.1861, -1.3287],\n",
      "          [ 0.2416, -2.4569, -0.4355,  ..., -0.8975,  0.1153,  2.8533],\n",
      "          ...,\n",
      "          [ 0.1823,  0.2939,  0.7517,  ..., -0.1154,  0.6901,  0.1813],\n",
      "          [ 0.9881, -1.4924,  1.5847,  ..., -1.0902,  1.1740,  0.5007],\n",
      "          [-0.3102, -1.1519,  0.3958,  ...,  0.4279,  0.2939, -0.3045]],\n",
      "\n",
      "         [[ 0.9854,  0.8651,  1.1666,  ..., -1.6936,  2.6372, -1.0191],\n",
      "          [-1.7529, -1.3249, -0.8904,  ..., -0.6217, -1.0936, -0.9247],\n",
      "          [-0.9436, -1.3781, -0.2133,  ..., -0.2977,  0.9406,  1.1455],\n",
      "          ...,\n",
      "          [-0.9628, -0.3637, -2.8107,  ..., -0.8807, -1.1439, -1.2355],\n",
      "          [-0.8818, -0.0798,  0.5583,  ..., -0.2070,  1.0954,  0.9384],\n",
      "          [ 0.6365,  0.9365, -1.0918,  ..., -0.4297,  0.7812, -2.5030]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1779, -0.6192,  0.3639,  ..., -0.9202, -0.4903,  1.0130],\n",
      "          [-1.2951,  2.1769,  0.7387,  ...,  0.1608,  0.6523, -1.9239],\n",
      "          [-0.2264,  1.0940, -0.6249,  ...,  0.2619, -0.6977,  0.9542],\n",
      "          ...,\n",
      "          [ 0.0791, -0.2441,  1.3546,  ..., -0.3320, -0.1688, -0.4512],\n",
      "          [ 0.0974,  0.4031,  0.5799,  ..., -0.2461,  0.1552, -0.7473],\n",
      "          [ 0.4149, -2.0617,  0.4459,  ..., -0.6113,  0.0826, -0.6723]],\n",
      "\n",
      "         [[ 1.1650, -1.3479,  0.7947,  ...,  0.5296,  2.7036, -0.1600],\n",
      "          [-1.0171,  0.5547,  1.8738,  ..., -1.1924,  0.1319, -0.2517],\n",
      "          [-1.0879, -0.9150, -0.7116,  ..., -1.0599, -1.7596, -0.2359],\n",
      "          ...,\n",
      "          [ 1.7395, -0.7476, -0.1147,  ..., -0.4067, -0.2565,  0.5273],\n",
      "          [ 1.7040,  0.7107, -1.5018,  ...,  0.4341,  0.9790,  0.1285],\n",
      "          [ 0.8496,  2.0883, -0.2924,  ...,  0.8788,  0.1201,  0.9200]],\n",
      "\n",
      "         [[-1.0870,  0.0758, -0.3701,  ...,  0.6044, -0.7530,  1.5144],\n",
      "          [ 0.1693,  0.9079,  0.9509,  ..., -0.2482,  1.5120, -0.2312],\n",
      "          [ 1.0565, -0.7567, -0.1976,  ..., -1.2063,  0.0283,  0.1362],\n",
      "          ...,\n",
      "          [-0.0154,  0.9844, -0.9017,  ...,  1.2959,  0.1901, -0.1676],\n",
      "          [-0.5661,  0.6745,  1.0446,  ..., -1.7092, -1.1281, -0.1719],\n",
      "          [ 0.2292,  1.2340,  0.7689,  ..., -2.0927, -0.4174,  1.5347]]],\n",
      "\n",
      "\n",
      "        [[[-1.3616, -0.0923, -0.2601,  ...,  0.3429, -1.5476,  0.0300],\n",
      "          [-0.3267,  1.2237, -1.1731,  ...,  0.0422, -1.5622,  0.1420],\n",
      "          [-0.5046,  0.9611,  3.0900,  ..., -0.0880,  0.5976,  0.7888],\n",
      "          ...,\n",
      "          [-0.4200,  1.0714, -0.5225,  ...,  0.2277, -0.6368,  0.3971],\n",
      "          [-1.4068,  0.6258,  0.4665,  ...,  0.3271, -0.7359,  1.1108],\n",
      "          [-0.3486, -0.2369, -0.9561,  ..., -0.6502, -0.7834,  0.9103]],\n",
      "\n",
      "         [[-0.0483,  0.4618,  0.1629,  ...,  0.4708, -0.2431, -0.7806],\n",
      "          [ 2.3481,  1.4626,  0.1451,  ...,  1.1575,  0.7607, -0.2155],\n",
      "          [-1.3368, -0.7474, -0.4484,  ...,  1.2644,  0.0810, -0.0374],\n",
      "          ...,\n",
      "          [ 0.9976, -0.2093, -0.6536,  ...,  0.0684,  0.2718,  0.0323],\n",
      "          [ 0.1845,  0.1637, -1.0004,  ...,  1.3529,  0.1192, -2.0515],\n",
      "          [ 1.3791,  0.5638,  0.4832,  ..., -0.2878,  0.3797, -2.5959]],\n",
      "\n",
      "         [[-1.5172,  0.1348,  0.0130,  ..., -1.0978,  0.5414,  0.5406],\n",
      "          [-0.0075,  1.3697, -1.7322,  ..., -1.3613, -2.1236,  1.1711],\n",
      "          [-0.6678,  0.7594, -0.3141,  ..., -0.0295, -0.4882, -1.2341],\n",
      "          ...,\n",
      "          [-0.2637, -0.0841,  1.0625,  ..., -0.6643,  1.7002,  0.0994],\n",
      "          [-0.3312, -0.2222,  0.3260,  ...,  1.6039, -0.6416, -1.1693],\n",
      "          [ 0.0344, -0.6025,  1.6675,  ...,  1.4284,  0.2272, -0.1811]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.1345,  0.3219,  0.1665,  ...,  0.5317,  1.5812,  0.4583],\n",
      "          [-2.2322, -0.0893,  1.1779,  ...,  0.6564,  1.1957,  0.3716],\n",
      "          [ 1.2644,  0.1115, -0.5814,  ...,  0.0313, -1.4923, -1.1681],\n",
      "          ...,\n",
      "          [-1.1730, -0.5526, -1.3648,  ...,  0.5524, -0.0924,  1.8179],\n",
      "          [ 1.5569,  1.5142,  0.0366,  ...,  0.2220,  0.6966, -0.0776],\n",
      "          [ 0.7290,  1.1923, -3.1286,  ...,  0.2471,  0.1742,  2.3363]],\n",
      "\n",
      "         [[ 1.0876,  0.0895, -2.0307,  ..., -0.0286, -0.6417, -2.0621],\n",
      "          [ 0.1081, -2.8824, -0.7993,  ...,  2.6229,  1.8272,  0.7391],\n",
      "          [-1.1942, -1.1520,  1.1850,  ...,  1.0228,  1.4490, -0.5206],\n",
      "          ...,\n",
      "          [ 1.0270,  0.8417, -0.7330,  ...,  0.6063, -2.1507,  1.0191],\n",
      "          [-0.3696, -0.0143, -0.9332,  ..., -1.0438, -1.4881, -0.5968],\n",
      "          [ 0.3373,  0.8947, -1.2112,  ...,  1.1386,  0.2046, -0.4415]],\n",
      "\n",
      "         [[ 2.1700, -0.3115,  1.2180,  ...,  1.2269,  1.1036, -0.1097],\n",
      "          [ 0.1911,  1.5083, -0.2124,  ..., -2.4858, -0.3128, -0.2079],\n",
      "          [-0.0386, -0.5550,  2.8206,  ..., -0.4309,  0.1940,  2.2403],\n",
      "          ...,\n",
      "          [-0.2120,  0.6461,  0.3586,  ...,  1.8488,  0.0974,  1.0001],\n",
      "          [-0.1812,  2.1495,  0.0612,  ...,  0.2493, -0.3525,  0.4095],\n",
      "          [-0.2078,  0.3684,  1.5171,  ..., -1.6064,  0.4246,  1.5779]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7532, -0.0789, -0.1875,  ..., -0.4434, -0.1155,  0.2339],\n",
      "          [ 0.6086, -0.2768,  0.4837,  ..., -0.1718, -0.0672,  0.2538],\n",
      "          [-0.4163,  0.3880, -1.6337,  ...,  0.3864, -0.6360,  0.7924],\n",
      "          ...,\n",
      "          [-1.1278,  1.3583, -1.6431,  ..., -0.2873,  0.3819,  0.7238],\n",
      "          [ 0.4214,  1.9372,  0.6654,  ..., -0.8822,  0.3807,  1.2006],\n",
      "          [ 0.4988, -0.2411, -1.0798,  ...,  0.1015, -0.6885, -0.8214]],\n",
      "\n",
      "         [[ 1.2887, -0.7723,  0.5874,  ...,  0.6834, -0.7768,  0.2916],\n",
      "          [-0.4054,  0.4647,  1.1468,  ..., -0.8061, -0.2065,  0.3244],\n",
      "          [-0.9063, -1.0741, -2.0918,  ...,  0.3932, -0.9197, -0.0149],\n",
      "          ...,\n",
      "          [ 1.0902,  0.4247, -0.4006,  ...,  0.9640,  0.0772, -0.4631],\n",
      "          [-0.8884,  0.4047, -0.1663,  ...,  0.3583, -2.1515, -1.1184],\n",
      "          [ 0.9282,  0.8521, -3.2769,  ..., -1.6424, -0.3263,  0.8094]],\n",
      "\n",
      "         [[ 1.4826,  0.5641, -0.6721,  ..., -0.6583,  0.4953,  0.4519],\n",
      "          [ 0.9587,  1.3491,  0.7838,  ...,  1.2784,  0.2947, -0.1217],\n",
      "          [-0.1084,  0.6936, -1.7716,  ...,  0.0582, -1.4989,  0.9649],\n",
      "          ...,\n",
      "          [ 0.3261, -0.6126, -1.3866,  ..., -0.2762, -0.3522,  1.8889],\n",
      "          [ 0.9639, -0.8859, -1.1940,  ...,  1.5904,  0.0809, -0.3192],\n",
      "          [ 1.6518,  0.9315, -2.2535,  ..., -0.6700,  0.4934, -0.0155]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5941, -0.8804,  0.3438,  ...,  0.0898, -1.4051, -0.0901],\n",
      "          [-0.7375,  0.4932, -1.4966,  ..., -0.4694, -0.9265,  0.1091],\n",
      "          [ 0.3069,  1.4387,  0.4786,  ...,  0.3763,  0.5871, -0.5004],\n",
      "          ...,\n",
      "          [-0.4320,  1.7536, -0.7753,  ..., -1.3488, -0.6418,  0.2472],\n",
      "          [-1.9342, -0.0818, -1.3595,  ...,  0.5009,  1.7026, -1.1629],\n",
      "          [-0.7711, -0.7482,  0.7110,  ...,  0.4905, -0.8032, -1.1772]],\n",
      "\n",
      "         [[ 0.2319,  0.2086, -1.0553,  ..., -1.1618, -0.3211, -2.3222],\n",
      "          [ 0.4762, -1.7008,  0.8778,  ...,  0.8052, -0.6791,  1.0146],\n",
      "          [ 0.2778,  0.2590,  0.3109,  ...,  0.8910, -1.1885, -0.4344],\n",
      "          ...,\n",
      "          [ 0.1492,  0.6878, -1.1827,  ..., -0.3267, -0.5804, -0.3247],\n",
      "          [-0.3105,  0.3088, -0.4361,  ...,  2.2546, -0.4345, -1.2894],\n",
      "          [ 0.2330, -0.9161, -1.3445,  ..., -0.1065,  0.8184, -1.4188]],\n",
      "\n",
      "         [[-1.2150,  0.5975, -0.4413,  ..., -0.4915, -1.4884,  0.7102],\n",
      "          [ 1.0463,  0.4217, -0.3046,  ...,  1.7044, -0.3323,  0.1965],\n",
      "          [ 1.1641, -0.6777,  0.0561,  ..., -1.3836,  0.2001, -2.2145],\n",
      "          ...,\n",
      "          [-0.1223, -2.8839, -0.5589,  ...,  0.2685,  1.3966,  1.1558],\n",
      "          [-0.1603, -0.6678, -0.0977,  ...,  1.2173, -1.4445, -0.5165],\n",
      "          [ 1.4601, -0.6087, -1.0022,  ..., -0.1470, -2.4160, -0.2574]]]])\n",
      "\n",
      "img2.view(B, -1)          | shape=(8, 2352) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([[ 0.6958,  1.7900, -1.1804,  ..., -0.4297,  0.7812, -2.5030],\n",
      "        [ 0.1779, -0.6192,  0.3639,  ..., -2.0927, -0.4174,  1.5347],\n",
      "        [-1.3616, -0.0923, -0.2601,  ...,  1.4284,  0.2272, -0.1811],\n",
      "        ...,\n",
      "        [ 0.1345,  0.3219,  0.1665,  ..., -1.6064,  0.4246,  1.5779],\n",
      "        [ 0.7532, -0.0789, -0.1875,  ..., -0.6700,  0.4934, -0.0155],\n",
      "        [ 0.5941, -0.8804,  0.3438,  ..., -0.1470, -2.4160, -0.2574]])\n",
      "\n",
      "torch.flatten(img2, start_dim=1) | shape=(8, 2352) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([[ 0.6958,  1.7900, -1.1804,  ..., -0.4297,  0.7812, -2.5030],\n",
      "        [ 0.1779, -0.6192,  0.3639,  ..., -2.0927, -0.4174,  1.5347],\n",
      "        [-1.3616, -0.0923, -0.2601,  ...,  1.4284,  0.2272, -0.1811],\n",
      "        ...,\n",
      "        [ 0.1345,  0.3219,  0.1665,  ..., -1.6064,  0.4246,  1.5779],\n",
      "        [ 0.7532, -0.0789, -0.1875,  ..., -0.6700,  0.4934, -0.0155],\n",
      "        [ 0.5941, -0.8804,  0.3438,  ..., -0.1470, -2.4160, -0.2574]])\n",
      "\n",
      "m                         | shape=(2, 3, 4) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([[[ 0.3413, -0.5064, -0.1914, -1.2717],\n",
      "         [ 0.8251,  0.5341,  1.1019,  0.5211],\n",
      "         [ 0.2571, -0.1674,  0.1150,  0.4579]],\n",
      "\n",
      "        [[ 0.0221,  0.2421,  0.8819, -1.6296],\n",
      "         [ 0.8176, -0.3769,  0.6872, -0.7697],\n",
      "         [ 0.0911,  1.0235,  0.9284,  0.1676]]])\n",
      "\n",
      "m.transpose(1,2)          | shape=(2, 4, 3) | dtype=torch.float32 | device=cpu | contiguous=False\n",
      "  -> tensor([[[ 0.3413,  0.8251,  0.2571],\n",
      "         [-0.5064,  0.5341, -0.1674],\n",
      "         [-0.1914,  1.1019,  0.1150],\n",
      "         [-1.2717,  0.5211,  0.4579]],\n",
      "\n",
      "        [[ 0.0221,  0.8176,  0.0911],\n",
      "         [ 0.2421, -0.3769,  1.0235],\n",
      "         [ 0.8819,  0.6872,  0.9284],\n",
      "         [-1.6296, -0.7697,  0.1676]]])\n",
      "\n",
      "m_t.is_contiguous() -> False\n",
      "m_t.contiguous().view(B,-1) | shape=(2, 12) | dtype=torch.float32 | device=cpu | contiguous=True\n",
      "  -> tensor([[ 0.3413,  0.8251,  0.2571, -0.5064,  0.5341, -0.1674, -0.1914,  1.1019,\n",
      "          0.1150, -1.2717,  0.5211,  0.4579],\n",
      "        [ 0.0221,  0.8176,  0.0911,  0.2421, -0.3769,  1.0235,  0.8819,  0.6872,\n",
      "          0.9284, -1.6296, -0.7697,  0.1676]])\n",
      "\n",
      "Quick summary:\n",
      "- use .view() for reshaping when tensor is contiguous (fast, no-copy).\n",
      "- use .reshape() if you want convenience and don't care about copying.\n",
      "- use .contiguous() after transpose/slice before .view().\n",
      "- use .unsqueeze() / .squeeze() to add/remove singleton dims (batch dims).\n",
      "- use .flatten(start_dim=..) to collapse dims safely.\n",
      "- use .expand() to broadcast a view (no memory copy) and .repeat() to copy data.\n",
      "\n",
      "‚úÖ Step 5 code demo complete.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Step 5 ‚Äî Tensor Reshaping & Dimension Manipulation (examples + comments)\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 1) Basic tensor & shape helpers\n",
    "# --------------------------\n",
    "t = torch.arange(6)            # 1D tensor [0,1,2,3,4,5]\n",
    "p(\"original t\", t)\n",
    "print(\"t.ndim:\", t.ndim, \" t.numel():\", t.numel())\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 2) view() vs reshape()\n",
    "# --------------------------\n",
    "# view() returns a view (no copy) but requires the tensor to be contiguous.\n",
    "# reshape() returns a tensor with desired shape; it will copy if necessary.\n",
    "t2 = t.reshape(2, 3)          # safe: reshape a (6,) -> (2,3)\n",
    "p(\"t.reshape(2,3)\", t2)\n",
    "\n",
    "t3 = t.view(2, 3)             # view() also works here (contiguous)\n",
    "p(\"t.view(2,3)\", t3)\n",
    "\n",
    "# reshape that produces a copy when necessary: show with transpose later\n",
    "\n",
    "# --------------------------\n",
    "# 3) Creating a non-contiguous tensor (transpose)\n",
    "# --------------------------\n",
    "M = torch.arange(12).reshape(3, 4)   # shape (3,4)\n",
    "p(\"M (3x4)\", M)\n",
    "\n",
    "Mt = M.T                             # transpose -> shape (4,3)\n",
    "p(\"Mt = M.T (transposed)\", Mt)\n",
    "\n",
    "# Transpose often produces a non-contiguous tensor\n",
    "print(\"Is Mt contiguous?\", Mt.is_contiguous())\n",
    "print(\"Attempting Mt.view(-1) may fail if non-contiguous; use contiguous() first.\")\n",
    "print()\n",
    "\n",
    "# Demonstrate view() failing safely with try/except, then fix with contiguous()\n",
    "try:\n",
    "    # This may raise an error if Mt is non-contiguous\n",
    "    v_bad = Mt.view(-1)\n",
    "    print(\"Mt.view(-1) succeeded (unexpected):\", v_bad.shape)\n",
    "except Exception as e:\n",
    "    print(\"Mt.view(-1) failed (expected for non-contiguous):\", type(e).__name__, \"-\", e)\n",
    "\n",
    "# Fix: make contiguous then view\n",
    "v_fix = Mt.contiguous().view(-1)\n",
    "p(\"Mt.contiguous().view(-1)\", v_fix)\n",
    "\n",
    "# --------------------------\n",
    "# 4) unsqueeze() and squeeze()\n",
    "# --------------------------\n",
    "a = torch.tensor([1.0, 2.0, 3.0])   # shape (3,)\n",
    "p(\"a (orig)\", a)\n",
    "\n",
    "a_u0 = a.unsqueeze(0)               # add batch dim at front -> (1,3)\n",
    "p(\"a.unsqueeze(0)\", a_u0)\n",
    "\n",
    "a_u1 = a.unsqueeze(1)               # add dim in middle -> (3,1)\n",
    "p(\"a.unsqueeze(1)\", a_u1)\n",
    "\n",
    "# squeeze removes dims of size 1\n",
    "b = torch.tensor([[[7.0], [8.0], [9.0]]])  # shape (1,3,1)\n",
    "p(\"b (1,3,1)\", b)\n",
    "\n",
    "b_s = b.squeeze()                    # removes all size-1 dims -> (3,)\n",
    "p(\"b.squeeze()\", b_s)\n",
    "\n",
    "# squeeze with dim parameter - only remove if that dim is size 1\n",
    "b_s_dim = b.squeeze(0)               # remove first dim only -> (3,1)\n",
    "p(\"b.squeeze(0)\", b_s_dim)\n",
    "\n",
    "# --------------------------\n",
    "# 5) flatten()\n",
    "# --------------------------\n",
    "img = torch.arange(2*3*4).reshape(2, 3, 4)  # pretend (B=2, C=3, W=4)\n",
    "p(\"img (2,3,4)\", img)\n",
    "\n",
    "flat_all = torch.flatten(img)         # flatten all dims -> (24,)\n",
    "p(\"torch.flatten(img)\", flat_all)\n",
    "\n",
    "flat_from1 = torch.flatten(img, start_dim=1)  # keep batch (2, 12)\n",
    "p(\"torch.flatten(img, start_dim=1)\", flat_from1)\n",
    "\n",
    "# --------------------------\n",
    "# 6) expand() vs repeat()\n",
    "# --------------------------\n",
    "# expand returns a view that reuses the same memory (no new copy),\n",
    "# repeat actually copies data into a new tensor.\n",
    "\n",
    "t_small = torch.tensor([1.0, 2.0, 3.0])   # shape (3,)\n",
    "p(\"t_small\", t_small)\n",
    "\n",
    "# expand: make it (3,3) by broadcasting (no extra memory)\n",
    "# to call expand, shape must be compatible: here (3,) -> (3,3) by (3,1) view first\n",
    "t_expand = t_small.unsqueeze(1).expand(3, 3)  # (3,1) -> (3,3) by broadcasting\n",
    "p(\"t_small.unsqueeze(1).expand(3,3)\", t_expand)\n",
    "\n",
    "# repeat: physically copies data\n",
    "t_repeat = t_small.repeat(2, 1)   # (2,3) creates an actual copy\n",
    "p(\"t_small.repeat(2,1)\", t_repeat)\n",
    "\n",
    "# Show that expand shares storage pointer while repeat uses different storage\n",
    "print(\"Storage ptrs -> expand.data_ptr:\", t_expand.storage().data_ptr(), \n",
    "      \"repeat.data_ptr:\", t_repeat.storage().data_ptr())\n",
    "print(\"Note: different ptrs for repeat => new memory; expand may reuse original storage (broadcast view).\")\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 7) view/resahpe with -1 (inferred dimension)\n",
    "# --------------------------\n",
    "x = torch.arange(24)\n",
    "p(\"x (24)\", x)\n",
    "\n",
    "y = x.view(2, -1)   # infer second dim => (2, 12)\n",
    "p(\"x.view(2, -1)\", y)\n",
    "\n",
    "z = x.reshape(-1, 6)  # infer first dim => (4, 6)\n",
    "p(\"x.reshape(-1, 6)\", z)\n",
    "\n",
    "# --------------------------\n",
    "# 8) chaining ops: common patterns\n",
    "# --------------------------\n",
    "# Example: image (B,C,H,W) -> flatten per sample -> feed to linear layer\n",
    "img2 = torch.randn(8, 3, 28, 28)   # batch of 8 images\n",
    "p(\"img2 (8,3,28,28)\", img2)\n",
    "\n",
    "# keep batch dimension, flatten rest\n",
    "img2_flat = img2.view(img2.size(0), -1)   # common pattern, requires contiguous layout\n",
    "p(\"img2.view(B, -1)\", img2_flat)\n",
    "\n",
    "# safer alternative using flatten:\n",
    "img2_flat2 = torch.flatten(img2, start_dim=1)\n",
    "p(\"torch.flatten(img2, start_dim=1)\", img2_flat2)\n",
    "\n",
    "# --------------------------\n",
    "# 9) Practical caution: when to use contiguous()\n",
    "# --------------------------\n",
    "# Start from a transposed tensor and then want to view -> must call contiguous()\n",
    "m = torch.randn(2, 3, 4)\n",
    "m_t = m.transpose(1, 2)   # swap dims -> likely non-contiguous\n",
    "p(\"m\", m)\n",
    "p(\"m.transpose(1,2)\", m_t)\n",
    "print(\"m_t.is_contiguous() ->\", m_t.is_contiguous())\n",
    "# Fix before view\n",
    "m_t_fixed = m_t.contiguous().view(m_t.shape[0], -1)\n",
    "p(\"m_t.contiguous().view(B,-1)\", m_t_fixed)\n",
    "\n",
    "# --------------------------\n",
    "# 10) Summary prints\n",
    "# --------------------------\n",
    "print(\"Quick summary:\")\n",
    "print(\"- use .view() for reshaping when tensor is contiguous (fast, no-copy).\")\n",
    "print(\"- use .reshape() if you want convenience and don't care about copying.\")\n",
    "print(\"- use .contiguous() after transpose/slice before .view().\")\n",
    "print(\"- use .unsqueeze() / .squeeze() to add/remove singleton dims (batch dims).\")\n",
    "print(\"- use .flatten(start_dim=..) to collapse dims safely.\")\n",
    "print(\"- use .expand() to broadcast a view (no memory copy) and .repeat() to copy data.\")\n",
    "print()\n",
    "print(\"‚úÖ Step 5 code demo complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
