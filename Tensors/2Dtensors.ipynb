{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b95308d",
   "metadata": {},
   "source": [
    "# üìò Step 1: Two-Dimensional Tensors\n",
    "\n",
    "In this section, we‚Äôll explore **2D tensors** ‚Äî the building block of most machine learning data.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1Ô∏è‚É£ What Are Two-Dimensional Tensors?\n",
    "\n",
    "A **two-dimensional tensor** can be viewed as a **matrix**,  \n",
    "which holds numerical values of the same type.\n",
    "\n",
    "- Each **row** represents a *sample* or *observation*  \n",
    "- Each **column** represents a *feature* or *attribute*\n",
    "\n",
    "---\n",
    "\n",
    "### üè† Example: Housing Data\n",
    "\n",
    "Consider a dataset storing information about houses:\n",
    "\n",
    "| Rooms | Age | Price |\n",
    "|:------:|:----:|:------:|\n",
    "| 3 | 10 | 250000 |\n",
    "| 4 | 15 | 300000 |\n",
    "| 2 | 20 | 180000 |\n",
    "\n",
    "This can be represented as a **2D tensor**:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "3 & 10 & 250000 \\\\\n",
    "4 & 15 & 300000 \\\\\n",
    "2 & 20 & 180000\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each **row** = one house üè†  \n",
    "Each **column** = one feature üîπ\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2Ô∏è‚É£ 2D Tensors in Images\n",
    "\n",
    "- A **grayscale image** can be represented as a 2D tensor  \n",
    "  Each element represents **pixel intensity** between **0 (black)** and **255 (white)**.  \n",
    "\n",
    "$$\n",
    "\\text{Pixel intensity range: } 0 \\leq I(x,y) \\leq 255\n",
    "$$\n",
    "\n",
    "- A **color image (RGB)** is a 3D tensor ‚Äî one 2D tensor for each channel:  \n",
    "  - Red üî¥  \n",
    "  - Green üü¢  \n",
    "  - Blue üîµ  \n",
    "\n",
    "Thus,  \n",
    "$$\n",
    "\\text{Color Image Tensor Shape} = (3, H, W)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3Ô∏è‚É£ Creating a 2D Tensor\n",
    "\n",
    "We can create a 2D tensor using a list of lists.\n",
    "\n",
    "Example:\n",
    "- Each nested list ‚Üí a row\n",
    "- Each element ‚Üí a column value\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4Ô∏è‚É£ Tensor Attributes\n",
    "\n",
    "| Property | Method | Description |\n",
    "|-----------|----------|-------------|\n",
    "| `t.ndimension()` | ‚Üí Rank | Number of dimensions |\n",
    "| `t.shape` or `t.size()` | ‚Üí Shape | Number of rows & columns |\n",
    "| `t.numel()` | ‚Üí Count | Total number of elements |\n",
    "\n",
    "If a tensor has shape $(3,3)$,  \n",
    "then total elements = $3 \\times 3 = 9$.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 5Ô∏è‚É£ Indexing and Slicing in 2D\n",
    "\n",
    "### üìå Indexing Convention\n",
    "\n",
    "| Syntax | Meaning |\n",
    "|---------|----------|\n",
    "| `t[row, column]` | Access a single element |\n",
    "| `t[row]` | Access an entire row |\n",
    "| `t[:, column]` | Access an entire column |\n",
    "| `t[start:end, :]` | Slice rows |\n",
    "| `t[:, start:end]` | Slice columns |\n",
    "\n",
    "**Example Visualization:**\n",
    "\n",
    "For a tensor:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "11 & 12 & 13 \\\\\n",
    "21 & 22 & 23 \\\\\n",
    "31 & 32 & 33\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- `A[1, 2]` ‚Üí element at **2nd row**, **3rd column** = 23  \n",
    "- `A[0, 1]` ‚Üí 1st row, 2nd column = 12  \n",
    "- `A[0, :2]` ‚Üí 1st row, first 2 columns = `[11, 12]`  \n",
    "- `A[1:, 2]` ‚Üí last 2 rows, last column = `[23, 33]`\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 6Ô∏è‚É£ Basic Tensor Operations\n",
    "\n",
    "### ‚ûï Addition (Matrix Addition)\n",
    "\n",
    "If $X$ and $Y$ are two tensors of the same shape:\n",
    "\n",
    "$$\n",
    "Z = X + Y \\quad \\Rightarrow \\quad z_{ij} = x_{ij} + y_{ij}\n",
    "$$\n",
    "\n",
    "### ‚úñÔ∏è Scalar Multiplication\n",
    "\n",
    "$$\n",
    "Z = \\alpha Y \\quad \\Rightarrow \\quad z_{ij} = \\alpha \\times y_{ij}\n",
    "$$\n",
    "\n",
    "### ‚®Ä Element-wise Multiplication (Hadamard Product)\n",
    "\n",
    "$$\n",
    "Z = X \\odot Y \\quad \\Rightarrow \\quad z_{ij} = x_{ij} \\times y_{ij}\n",
    "$$\n",
    "\n",
    "### üîπ Matrix Multiplication\n",
    "\n",
    "Matrix product rule:  \n",
    "If $A$ is $(m \\times n)$ and $B$ is $(n \\times p)$,  \n",
    "then the result $C = A \\times B$ has shape $(m \\times p)$.\n",
    "\n",
    "Each element:\n",
    "$$\n",
    "c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Operation | PyTorch Syntax | Description |\n",
    "|------------|----------------|--------------|\n",
    "| Addition | `X + Y` | Element-wise sum |\n",
    "| Scalar multiplication | `2 * X` | Scales each element |\n",
    "| Element-wise product | `X * Y` | Hadamard product |\n",
    "| Matrix multiplication | `torch.mm(A, B)` or `A @ B` | Linear algebra product |\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Key Takeaway:**  \n",
    "2D tensors are everywhere ‚Äî from tabular data to images.  \n",
    "Understanding how to index, slice, and operate on them forms the foundation for working with real datasets and neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1444bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor X:\n",
      " tensor([[11, 12, 13],\n",
      "        [21, 22, 23],\n",
      "        [31, 32, 33]]) \n",
      "\n",
      "üîπ Tensor attributes\n",
      "ndimension(): 2\n",
      "shape: torch.Size([3, 3])\n",
      "size(): torch.Size([3, 3])\n",
      "numel(): 9\n",
      "\n",
      "üîπ Indexing and slicing\n",
      "X[1, 2] (2nd row, 3rd col): 23\n",
      "X[0, 1] (1st row, 2nd col): 12\n",
      "X[0, :2] (first row, first two cols): tensor([11, 12])\n",
      "X[1:, 2] (last two rows, last column): tensor([23, 33])\n",
      "X[:, 1] (entire 2nd column): tensor([12, 22, 32])\n",
      "X[2] (entire 3rd row): tensor([31, 32, 33])\n",
      "\n",
      "üîπ Matrix addition (element-wise)\n",
      "Y:\n",
      " tensor([[1, 1, 1],\n",
      "        [2, 2, 2],\n",
      "        [3, 3, 3]])\n",
      "X + Y =\n",
      " tensor([[12, 13, 14],\n",
      "        [23, 24, 25],\n",
      "        [34, 35, 36]])\n",
      "\n",
      "üîπ Scalar multiplication\n",
      "2 * Y =\n",
      " tensor([[2, 2, 2],\n",
      "        [4, 4, 4],\n",
      "        [6, 6, 6]])\n",
      "\n",
      "üîπ Element-wise (Hadamard) multiplication\n",
      "X * Y =\n",
      " tensor([[11, 12, 13],\n",
      "        [42, 44, 46],\n",
      "        [93, 96, 99]])\n",
      "\n",
      "üîπ Matrix multiplication (A @ B)\n",
      "A (2x3):\n",
      " tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "B (3x2):\n",
      " tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n",
      "C = A @ B =\n",
      " tensor([[10, 13],\n",
      "        [28, 40]])\n",
      "Verify C[0,0] manually: 10.0 == C[0,0] -> 10\n",
      "\n",
      "üîπ Transpose, determinant, inverse\n",
      "S:\n",
      " tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "S.T (transpose):\n",
      " tensor([[1., 3.],\n",
      "        [2., 4.]])\n",
      "det(S): -2.0\n",
      "S inverse:\n",
      " tensor([[-2.0000,  1.0000],\n",
      "        [ 1.5000, -0.5000]])\n",
      "S @ S^{-1} =\n",
      " tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "\n",
      "üîπ Flatten and reshape\n",
      "X.flatten() -> tensor([11, 12, 13, 21, 22, 23, 31, 32, 33]) shape: torch.Size([9])\n",
      "X_flat.view(3,3) ->\n",
      " tensor([[11, 12, 13],\n",
      "        [21, 22, 23],\n",
      "        [31, 32, 33]])\n",
      "\n",
      "üîπ Broadcasting\n",
      "row shape: torch.Size([3]) col shape: torch.Size([3, 1])\n",
      "col + row -> shape: torch.Size([3, 3])\n",
      "tensor([[11, 12, 13],\n",
      "        [21, 22, 23],\n",
      "        [31, 32, 33]])\n",
      "\n",
      "üîπ Comparisons and masking\n",
      "mask (X > 20):\n",
      " tensor([[False, False, False],\n",
      "        [ True,  True,  True],\n",
      "        [ True,  True,  True]])\n",
      "X[mask] -> tensor([21, 22, 23, 31, 32, 33])\n",
      "X_clone after masked assignment (values >20 -> -1):\n",
      " tensor([[11, 12, 13],\n",
      "        [-1, -1, -1],\n",
      "        [-1, -1, -1]])\n",
      "\n",
      "üîπ Grayscale image example (2D tensor)\n",
      "img (dtype uint8):\n",
      " tensor([[  0,  30,  60,  90, 120],\n",
      "        [ 15,  45,  75, 105, 135],\n",
      "        [ 30,  60,  90, 120, 150],\n",
      "        [ 45,  75, 105, 135, 165],\n",
      "        [ 60,  90, 120, 150, 180]], dtype=torch.uint8)\n",
      "img shape: torch.Size([5, 5])\n",
      "img normalized (float):\n",
      " tensor([[0.0000, 0.1176, 0.2353, 0.3529, 0.4706],\n",
      "        [0.0588, 0.1765, 0.2941, 0.4118, 0.5294],\n",
      "        [0.1176, 0.2353, 0.3529, 0.4706, 0.5882],\n",
      "        [0.1765, 0.2941, 0.4118, 0.5294, 0.6471],\n",
      "        [0.2353, 0.3529, 0.4706, 0.5882, 0.7059]])\n",
      "\n",
      "üîπ Dtype conversion and device\n",
      "X_float dtype: torch.float32\n",
      "No GPU available on this system (device stays on cpu).\n",
      "\n",
      "‚úÖ Step 6 code demo complete: creation, indexing, arithmetic, matmul, reshape, broadcasting, masking, and image example.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Complete Step 6 ‚Äî Two-Dimensional Tensors (complete examples + comments)\n",
    "# ------------------------------------------------------------------------\n",
    "import torch\n",
    "\n",
    "# --------------------------\n",
    "# 1) Create a 2D tensor (list of lists) - example: housing / tabular data\n",
    "# --------------------------\n",
    "a = [[11, 12, 13],\n",
    "     [21, 22, 23],\n",
    "     [31, 32, 33]]\n",
    "\n",
    "X = torch.tensor(a)                 # dtype inferred (int64)\n",
    "print(\"Tensor X:\\n\", X, \"\\n\")       # print full matrix\n",
    "\n",
    "# --------------------------\n",
    "# 2) Tensor attributes (rank, shape, size, number of elements)\n",
    "# --------------------------\n",
    "print(\"üîπ Tensor attributes\")\n",
    "print(\"ndimension():\", X.ndimension())   # number of dimensions (rank)\n",
    "print(\"shape:\", X.shape)                 # (rows, columns)\n",
    "print(\"size():\", X.size())               # same as shape\n",
    "print(\"numel():\", X.numel())             # total elements = rows * cols\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 3) Indexing & slicing in 2D\n",
    "# --------------------------\n",
    "print(\"üîπ Indexing and slicing\")\n",
    "print(\"X[1, 2] (2nd row, 3rd col):\", X[1, 2].item())   # single element -> python scalar\n",
    "print(\"X[0, 1] (1st row, 2nd col):\", X[0, 1].item())\n",
    "print(\"X[0, :2] (first row, first two cols):\", X[0, :2])   # slice returns tensor\n",
    "print(\"X[1:, 2] (last two rows, last column):\", X[1:, 2])\n",
    "print(\"X[:, 1] (entire 2nd column):\", X[:, 1])\n",
    "print(\"X[2] (entire 3rd row):\", X[2])\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 4) Matrix (element-wise) addition\n",
    "# --------------------------\n",
    "print(\"üîπ Matrix addition (element-wise)\")\n",
    "Y = torch.tensor([[1, 1, 1],\n",
    "                  [2, 2, 2],\n",
    "                  [3, 3, 3]])\n",
    "Z_add = X + Y\n",
    "print(\"Y:\\n\", Y)\n",
    "print(\"X + Y =\\n\", Z_add)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 5) Scalar multiplication (tensor <-> scalar)\n",
    "# --------------------------\n",
    "print(\"üîπ Scalar multiplication\")\n",
    "Z_scalar = 2 * Y\n",
    "print(\"2 * Y =\\n\", Z_scalar)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 6) Element-wise (Hadamard) multiplication\n",
    "# --------------------------\n",
    "print(\"üîπ Element-wise (Hadamard) multiplication\")\n",
    "Z_hadamard = X * Y    # elementwise product\n",
    "print(\"X * Y =\\n\", Z_hadamard)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 7) Matrix multiplication (linear algebra)\n",
    "#    A shape (m x n)  B shape (n x p)  => C shape (m x p)\n",
    "# --------------------------\n",
    "print(\"üîπ Matrix multiplication (A @ B)\")\n",
    "A = torch.tensor([[0, 1, 2],\n",
    "                  [3, 4, 5]])   # shape (2,3)\n",
    "\n",
    "B = torch.tensor([[0, 1],\n",
    "                  [2, 3],\n",
    "                  [4, 5]])       # shape (3,2)\n",
    "\n",
    "print(\"A (2x3):\\n\", A)\n",
    "print(\"B (3x2):\\n\", B)\n",
    "\n",
    "C = torch.mm(A, B)   # or A @ B\n",
    "print(\"C = A @ B =\\n\", C)\n",
    "# Manual verification for element (0,0): dot(A[0,:], B[:,0]) = 0*0 + 1*2 + 2*4 = 10\n",
    "print(\"Verify C[0,0] manually:\", (A[0].float() * B[:,0].float()).sum().item(), \"== C[0,0] ->\", C[0,0].item())\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 8) Transpose, determinant, inverse (when square & invertible)\n",
    "# --------------------------\n",
    "print(\"üîπ Transpose, determinant, inverse\")\n",
    "S = torch.tensor([[1., 2.],\n",
    "                  [3., 4.]])   # square 2x2 (float for det/inv)\n",
    "print(\"S:\\n\", S)\n",
    "print(\"S.T (transpose):\\n\", S.T)\n",
    "detS = torch.det(S)\n",
    "print(\"det(S):\", detS.item())\n",
    "invS = torch.inverse(S)\n",
    "print(\"S inverse:\\n\", invS)\n",
    "print(\"S @ S^{-1} =\\n\", torch.mm(S, invS))  # should approximate identity\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 9) Flatten and reshape 2D -> 1D and back\n",
    "# --------------------------\n",
    "print(\"üîπ Flatten and reshape\")\n",
    "X_flat = X.flatten()         # (3,3) -> (9,)\n",
    "print(\"X.flatten() ->\", X_flat, \"shape:\", X_flat.shape)\n",
    "\n",
    "X_view = X_flat.view(3, 3)   # reshape back (requires contiguity here)\n",
    "print(\"X_flat.view(3,3) ->\\n\", X_view)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 10) Broadcasting examples (tensor <-> tensor with compatible shapes)\n",
    "# --------------------------\n",
    "print(\"üîπ Broadcasting\")\n",
    "row = torch.tensor([1, 2, 3])        # shape (3,)\n",
    "col = torch.tensor([[10], [20], [30]])  # shape (3,1)\n",
    "\n",
    "print(\"row shape:\", row.shape, \"col shape:\", col.shape)\n",
    "# row will be broadcast to (3,3) when added to col\n",
    "broadcast_sum = col + row\n",
    "print(\"col + row -> shape:\", broadcast_sum.shape)\n",
    "print(broadcast_sum)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 11) Comparisons, boolean masks & masked assignment\n",
    "# --------------------------\n",
    "print(\"üîπ Comparisons and masking\")\n",
    "mask = X > 20\n",
    "print(\"mask (X > 20):\\n\", mask)\n",
    "print(\"X[mask] ->\", X[mask])   # flatten of selected items\n",
    "\n",
    "# masked assignment: set values >20 to -1 (operate on a clone to show original preserved)\n",
    "X_clone = X.clone()\n",
    "X_clone[X_clone > 20] = -1\n",
    "print(\"X_clone after masked assignment (values >20 -> -1):\\n\", X_clone)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 12) Small grayscale image example (2D image tensor)\n",
    "# --------------------------\n",
    "print(\"üîπ Grayscale image example (2D tensor)\")\n",
    "# create a toy 5x5 'image' with intensities 0..255 scaled down\n",
    "img = torch.tensor([\n",
    "    [0,  30, 60, 90, 120],\n",
    "    [15, 45, 75, 105,135],\n",
    "    [30, 60, 90, 120,150],\n",
    "    [45, 75, 105,135,165],\n",
    "    [60, 90, 120,150,180]\n",
    "], dtype=torch.uint8)   # intensities as unsigned 8-bit values\n",
    "print(\"img (dtype uint8):\\n\", img)\n",
    "print(\"img shape:\", img.shape)\n",
    "# convert to float normalized [0,1] for processing\n",
    "img_float = img.float() / 255.0\n",
    "print(\"img normalized (float):\\n\", img_float)\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# 13) Extra: converting dtype and device (common utilities)\n",
    "# --------------------------\n",
    "print(\"üîπ Dtype conversion and device\")\n",
    "X_float = X.float()             # int -> float\n",
    "print(\"X_float dtype:\", X_float.dtype)\n",
    "if torch.cuda.is_available():\n",
    "    X_gpu = X_float.to('cuda')\n",
    "    print(\"Moved X to GPU:\", X_gpu.device)\n",
    "    # move back to CPU when needed\n",
    "    X_back = X_gpu.to('cpu')\n",
    "    print(\"Back on CPU:\", X_back.device)\n",
    "else:\n",
    "    print(\"No GPU available on this system (device stays on cpu).\")\n",
    "print()\n",
    "\n",
    "# --------------------------\n",
    "# End summary\n",
    "# --------------------------\n",
    "print(\"‚úÖ Step 6 code demo complete: creation, indexing, arithmetic, matmul, reshape, broadcasting, masking, and image example.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537adc9a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìò Step 2: Advanced Concepts ‚Äî 2D Tensor Operations in Depth\n",
    "\n",
    "We now extend our understanding of two-dimensional tensors with additional\n",
    "concepts used constantly in deep-learning workflows.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 7Ô∏è‚É£ Broadcasting (Automatic Shape Alignment)\n",
    "\n",
    "When performing element-wise operations, PyTorch automatically **broadcasts**\n",
    "smaller tensors to match larger shapes.  \n",
    "Broadcasting follows NumPy rules ‚Äî comparing shapes **from right to left**:\n",
    "\n",
    "1. Dimensions are equal, or  \n",
    "2. One of the dimensions is 1 (stretched virtually), or  \n",
    "3. Otherwise the operation fails.\n",
    "\n",
    "**Example**\n",
    "\n",
    "If  \n",
    "$$\n",
    "A \\in \\mathbb{R}^{3 \\times 1}, \\quad B \\in \\mathbb{R}^{1 \\times 4}\n",
    "$$  \n",
    "then  \n",
    "$$\n",
    "A + B \\Rightarrow \\text{shape } (3,4)\n",
    "$$\n",
    "\n",
    "\n",
    "Broadcasting is **logical** (no extra memory).  \n",
    "Use `expand()` for views and `repeat()` when actual copies are needed.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 8Ô∏è‚É£ Transpose and Contiguity\n",
    "\n",
    "Transposing swaps axes (rows ‚Üî columns):\n",
    "\n",
    "$$A^T_{ij} = A_{ji}.$$\n",
    "\n",
    "In PyTorch, `A.T` or `A.transpose(0,1)` creates a **non-contiguous** view.  \n",
    "Some operations such as `.view()` require contiguous memory.\n",
    "\n",
    "To fix this, we use:\n",
    "\n",
    "$$\n",
    "A = A.T.contiguous()\n",
    "$$\n",
    "\n",
    "Now you can safely reshape or flatten the tensor.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 9Ô∏è‚É£ Reshape vs View vs Flatten\n",
    "\n",
    "| Function | Copy? | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `reshape()` | May copy | Safest reshape |\n",
    "| `view()` | No copy (contiguous only) | Fast view on same storage |\n",
    "| `flatten(start_dim=0)` | ‚Äî | Collapse dims into 1D |\n",
    "\n",
    "Use `-1` to let PyTorch **infer** a dimension:\n",
    "\n",
    "$$\n",
    "x.reshape(2,-1) \\Rightarrow (2, \\text{auto})\n",
    "$$\n",
    "\n",
    "Example: if tensor `x` has 24 elements,  \n",
    "`x.reshape(2, -1)` ‚Üí `(2, 12)` and `x.reshape(-1, 6)` ‚Üí `(4, 6)`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ üîü Unsqueeze and Squeeze\n",
    "\n",
    "Used to **add** or **remove** singleton (size = 1) dimensions.\n",
    "\n",
    "| Function | Example | Result |\n",
    "|-----------|----------|--------|\n",
    "| `unsqueeze(dim)` | `(3,) ‚Üí (1,3)` | Adds axis |\n",
    "| `squeeze(dim)` | `(1,3,1) ‚Üí (3,)` | Removes axis |\n",
    "\n",
    "**Common use-cases:**\n",
    "- Add batch dim ‚Üí `x.unsqueeze(0)`  \n",
    "- Remove redundant dims ‚Üí `x.squeeze()`\n",
    "\n",
    "Mathematically, if  \n",
    "$$\n",
    "x = [1, 2, 3]\n",
    "$$  \n",
    "then  \n",
    "$$\n",
    "x.unsqueeze(0) \\Rightarrow [[1, 2, 3]]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 11Ô∏è‚É£ Expand vs Repeat\n",
    "\n",
    "Both replicate data but behave differently:\n",
    "\n",
    "| Function | Copies Data? | Description |\n",
    "|-----------|--------------|-------------|\n",
    "| `expand()` | ‚ùå | Creates a **view**, no memory cost |\n",
    "| `repeat()` | ‚úÖ | Physically duplicates elements |\n",
    "\n",
    "üß† **Rule of Thumb:**  \n",
    "Use `expand()` for lightweight broadcasting views;  \n",
    "use `repeat()` when you truly need multiple copies.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "If  \n",
    "$$\n",
    "t = [1, 2, 3]\n",
    "$$  \n",
    "then  \n",
    "\n",
    "- `t.expand(3,3)` ‚Üí broadcasted view  \n",
    "- `t.repeat(2,1)` ‚Üí actual duplicated data\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 12Ô∏è‚É£ Masking and Conditional Assignment\n",
    "\n",
    "Boolean masks let you **filter** or **modify** tensor elements based on conditions.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "5 & 15 & 25 \\\\\n",
    "10 & 20 & 30 \\\\\n",
    "0 & 8 & 40\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then  \n",
    "`mask = X > 15` gives a boolean tensor:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "False & False & True \\\\\n",
    "False & True & True \\\\\n",
    "False & False & True\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- `X[mask]` ‚Üí returns elements greater than 15  \n",
    "- `X[mask] = -1` ‚Üí replaces those elements in place  \n",
    "\n",
    "Masks are powerful for thresholding, clipping, and logical filtering.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 13Ô∏è‚É£ Matmul Family and Dot Product\n",
    "\n",
    "- **Dot product (1D vectors):**\n",
    "\n",
    "$$\n",
    "\\text{torch.dot}(a,b) = \\sum_i a_i b_i\n",
    "$$\n",
    "\n",
    "- **Matrix multiplication (2D tensors):**\n",
    "\n",
    "$$\n",
    "C = A \\times B \\Rightarrow c_{ij} = \\sum_k a_{ik} b_{kj}\n",
    "$$\n",
    "\n",
    "Use `torch.mm(A, B)` or `A @ B`.\n",
    "\n",
    "- **Generalized matmul (N-D tensors):**\n",
    "\n",
    "`torch.matmul()` handles batched or broadcasted matrix multiplies.  \n",
    "For example, `(B, m, n) @ (B, n, p)` ‚Üí `(B, m, p)`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 14Ô∏è‚É£ Dtype and Device Utilities\n",
    "\n",
    "Convert tensors between data types and devices:\n",
    "\n",
    "| Purpose | Example |\n",
    "|----------|----------|\n",
    "| Change dtype | `X.float()` / `X.long()` |\n",
    "| Move to GPU | `X.to(\"cuda\")` |\n",
    "| Move back to CPU | `X.to(\"cpu\")` |\n",
    "\n",
    "Always convert to CPU before calling `.numpy()` ‚Äî  \n",
    "GPU tensors cannot be directly converted to NumPy arrays.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary ‚Äî Advanced 2D Tensor Tools\n",
    "\n",
    "| Category | Function / Example | Purpose |\n",
    "|-----------|-------------------|----------|\n",
    "| Broadcasting | `A + B` | Auto-align shapes |\n",
    "| Transpose | `A.T` | Swap rows ‚Üî columns |\n",
    "| Contiguity | `A.contiguous()` | Safe reshape |\n",
    "| Reshape / View | `A.view(2, -1)` | Change layout |\n",
    "| Flatten | `torch.flatten(A)` | Collapse dims |\n",
    "| Unsqueeze / Squeeze | `x.unsqueeze(0)` / `x.squeeze()` | Add / remove dims |\n",
    "| Expand / Repeat | `A.expand(3,3)` / `A.repeat(2,1)` | View vs copy |\n",
    "| Masking | `A[A>0]` | Conditional selection |\n",
    "| Matmul | `torch.mm(A,B)` | Matrix product |\n",
    "| Device / Dtype | `A.to(\"cuda\")` | Move or cast tensor |\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Key Takeaway:**  \n",
    "These advanced tensor operations make your workflow flexible and efficient.  \n",
    "They‚Äôre essential for handling batches, channels, and shapes correctly when building deep learning pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843adc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Broadcasting ===\n",
      "col shape: (3, 1) row shape: (3,)\n",
      "col + row -> shape: (3, 3)\n",
      "tensor([[11, 12, 13],\n",
      "        [21, 22, 23],\n",
      "        [31, 32, 33]])\n",
      "\n",
      "unexpected success: tensor([[2, 3, 4],\n",
      "        [3, 4, 5]])\n",
      "\n",
      "=== Transpose & Contiguity ===\n",
      "M shape: (3, 4)\n",
      "Mt shape: (4, 3)\n",
      "Mt.is_contiguous(): False\n",
      "Mt.view(-1) failed (expected on some systems): RuntimeError - view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
      "Fixed: Mt.contiguous().view(-1) -> shape: (12,)\n",
      "\n",
      "=== reshape vs view vs flatten ===\n",
      "x shape: (24,)\n",
      "x.reshape(2, -1) -> (2, 12)\n",
      "x.view(2, -1) -> (2, 12)\n",
      "x.flatten() -> (24,)\n",
      "\n",
      "=== unsqueeze / squeeze ===\n",
      "v shape: (3,) v: tensor([7, 8, 9])\n",
      "v.unsqueeze(0) shape: (1, 3)\n",
      "tensor([[7, 8, 9]])\n",
      "v.unsqueeze(1) shape: (3, 1)\n",
      "tensor([[7],\n",
      "        [8],\n",
      "        [9]])\n",
      "s shape before squeeze: (1, 3, 1)\n",
      "s.squeeze() -> (3,)\n",
      "s.squeeze(0) -> (3, 1)\n",
      "\n",
      "=== expand vs repeat ===\n",
      "t shape: (3,) t: tensor([1., 2., 3.])\n",
      "t.unsqueeze(1).expand(3,3) shape: (3, 3)\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]])\n",
      "t.repeat(2,1) shape: (2, 3)\n",
      "tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "storage pointer (t_expanded.storage().data_ptr()): 5387077376\n",
      "storage pointer (t_repeated.storage().data_ptr()): 5387075968\n",
      "If pointers differ, repeat allocated new storage; expand reuses original where possible.\n",
      "\n",
      "=== masking & conditional assignment ===\n",
      "X:\n",
      " tensor([[ 5, 15, 25],\n",
      "        [10, 20, 30],\n",
      "        [ 0,  8, 40]])\n",
      "mask (X > 15):\n",
      " tensor([[False, False,  True],\n",
      "        [False,  True,  True],\n",
      "        [False, False,  True]])\n",
      "Selected elements (X[mask]): tensor([25, 20, 30, 40])\n",
      "X clone after Xc[Xc > 15] = -1:\n",
      " tensor([[ 5, 15, -1],\n",
      "        [10, -1, -1],\n",
      "        [ 0,  8, -1]])\n",
      "\n",
      "=== dot, mm, matmul ===\n",
      "dot(a,b): tensor(32.)\n",
      "A @ B =\n",
      " tensor([[19., 22.],\n",
      "        [43., 50.]])\n",
      "batchA shape: (2, 2, 2) batchB shape: (2, 2, 2)\n",
      "torch.matmul(batchA, batchB) -> shape: (2, 2, 2)\n",
      "tensor([[[19., 22.],\n",
      "         [43., 50.]],\n",
      "\n",
      "        [[36., 41.],\n",
      "         [64., 73.]]])\n",
      "\n",
      "=== dtype & device utilities ===\n",
      "X_int dtype: torch.int64\n",
      "X_int.float() dtype: torch.float32\n",
      "No CUDA device found ‚Äî staying on CPU.\n",
      "X_float.numpy() -> [[1. 2.]\n",
      " [3. 4.]]\n",
      "\n",
      "‚úÖ Advanced 2D tensor demo complete.\n"
     ]
    }
   ],
   "source": [
    "# Advanced 2D tensor toolbox ‚Äî Sections 8..14\n",
    "# Includes: broadcasting, transpose & contiguity, reshape/view/flatten,\n",
    "# unsqueeze/squeeze, expand/repeat, masking, matmul/dot, dtype/device utilities.\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 8) Broadcasting example (col + row -> (3,3))\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"=== Broadcasting ===\")\n",
    "col = torch.tensor([[10], [20], [30]])   # shape (3,1)\n",
    "row = torch.tensor([1, 2, 3])            # shape (3,)\n",
    "print(\"col shape:\", tuple(col.shape), \"row shape:\", tuple(row.shape))\n",
    "\n",
    "# col + row broadcasts row to shape (3,3)\n",
    "B = col + row\n",
    "print(\"col + row -> shape:\", tuple(B.shape))\n",
    "print(B)   # each row = original col value + row vector\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 8.1) Demonstrate broadcasting rules failure (incompatible shapes)\n",
    "# ---------------------------------------------------------------------\n",
    "try:\n",
    "    bad = torch.tensor([1,2]).reshape(2,1) + torch.tensor([1,2,3])  # (2,1) + (3,) -> should raise\n",
    "    print(\"unexpected success:\", bad)\n",
    "except Exception as e:\n",
    "    print(\"Broadcasting failed as expected for incompatible shapes:\", type(e).__name__, e)\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 9) Transpose & Contiguity ‚Äî why view() can fail, how to fix with contiguous()\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"=== Transpose & Contiguity ===\")\n",
    "M = torch.arange(12).reshape(3, 4)   # shape (3,4)\n",
    "print(\"M shape:\", tuple(M.shape))\n",
    "Mt = M.T                             # (4,3) - often non-contiguous view\n",
    "print(\"Mt shape:\", tuple(Mt.shape))\n",
    "print(\"Mt.is_contiguous():\", Mt.is_contiguous())\n",
    "\n",
    "# Trying to do Mt.view(-1) may raise if non-contiguous. Show safe pattern:\n",
    "try:\n",
    "    v_bad = Mt.view(-1)   # attempt view on possibly non-contiguous tensor\n",
    "    print(\"Mt.view(-1) succeeded (unexpected):\", v_bad.shape)\n",
    "except Exception as e:\n",
    "    print(\"Mt.view(-1) failed (expected on some systems):\", type(e).__name__, \"-\", e)\n",
    "    # Fix by making contiguous first\n",
    "    v_fix = Mt.contiguous().view(-1)\n",
    "    print(\"Fixed: Mt.contiguous().view(-1) -> shape:\", tuple(v_fix.shape))\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 9.1) reshape() vs view() vs flatten()\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"=== reshape vs view vs flatten ===\")\n",
    "x = torch.arange(24)\n",
    "print(\"x shape:\", tuple(x.shape))\n",
    "\n",
    "r1 = x.reshape(2, -1)   # reshape: safe (may copy if necessary)\n",
    "r2 = x.view(2, -1)      # view: fast, requires contiguous storage\n",
    "f = x.flatten()         # flatten to 1D\n",
    "print(\"x.reshape(2, -1) ->\", tuple(r1.shape))\n",
    "print(\"x.view(2, -1) ->\", tuple(r2.shape))\n",
    "print(\"x.flatten() ->\", tuple(f.shape))\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 10) Unsqueeze and Squeeze (add/remove singleton dims)\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"=== unsqueeze / squeeze ===\")\n",
    "v = torch.tensor([7, 8, 9])   # (3,)\n",
    "print(\"v shape:\", tuple(v.shape), \"v:\", v)\n",
    "\n",
    "v_u0 = v.unsqueeze(0)         # (1,3) add batch dim at front\n",
    "v_u1 = v.unsqueeze(1)         # (3,1) add dim in middle\n",
    "print(\"v.unsqueeze(0) shape:\", tuple(v_u0.shape))\n",
    "print(v_u0)\n",
    "print(\"v.unsqueeze(1) shape:\", tuple(v_u1.shape))\n",
    "print(v_u1)\n",
    "\n",
    "# create a tensor with singleton dims and remove them\n",
    "s = v_u1.unsqueeze(0)         # (1,3,1)\n",
    "print(\"s shape before squeeze:\", tuple(s.shape))\n",
    "s_squeezed_all = s.squeeze()  # removes all size-1 dims -> (3,)\n",
    "s_squeezed_dim0 = s.squeeze(0)  # remove only first dim -> (3,1)\n",
    "print(\"s.squeeze() ->\", tuple(s_squeezed_all.shape))\n",
    "print(\"s.squeeze(0) ->\", tuple(s_squeezed_dim0.shape))\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 11) Expand vs Repeat (view vs copy)\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"=== expand vs repeat ===\")\n",
    "t = torch.tensor([1.0, 2.0, 3.0])   # shape (3,)\n",
    "print(\"t shape:\", tuple(t.shape), \"t:\", t)\n",
    "\n",
    "# To expand to (3,3) we first unsqueeze to (3,1) then expand\n",
    "t_expanded = t.unsqueeze(1).expand(3, 3)   # view: no new memory for element values\n",
    "t_repeated = t.repeat(2, 1)                # copy: new memory allocated (2,3)\n",
    "\n",
    "print(\"t.unsqueeze(1).expand(3,3) shape:\", tuple(t_expanded.shape))\n",
    "print(t_expanded)\n",
    "print(\"t.repeat(2,1) shape:\", tuple(t_repeated.shape))\n",
    "print(t_repeated)\n",
    "\n",
    "# Demonstrate that repeat creates new storage pointer (expand may share)\n",
    "print(\"storage pointer (t_expanded.storage().data_ptr()):\", t_expanded.storage().data_ptr())\n",
    "print(\"storage pointer (t_repeated.storage().data_ptr()):\", t_repeated.storage().data_ptr())\n",
    "print(\"If pointers differ, repeat allocated new storage; expand reuses original where possible.\")\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 12) Masking and conditional assignment\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"=== masking & conditional assignment ===\")\n",
    "X = torch.tensor([[5, 15, 25],\n",
    "                  [10,20,30],\n",
    "                  [0, 8, 40]])\n",
    "print(\"X:\\n\", X)\n",
    "\n",
    "mask = X > 15\n",
    "print(\"mask (X > 15):\\n\", mask)\n",
    "print(\"Selected elements (X[mask]):\", X[mask])\n",
    "\n",
    "# In-place masked assignment (do on a clone to preserve original)\n",
    "Xc = X.clone()\n",
    "Xc[Xc > 15] = -1\n",
    "print(\"X clone after Xc[Xc > 15] = -1:\\n\", Xc)\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 13) dot, mm, matmul (including batched matmul)\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"=== dot, mm, matmul ===\")\n",
    "a = torch.tensor([1., 2., 3.])\n",
    "b = torch.tensor([4., 5., 6.])\n",
    "print(\"dot(a,b):\", torch.dot(a, b))   # scalar: 1*4 + 2*5 + 3*6 = 32\n",
    "\n",
    "# 2D matrix multiplication\n",
    "A = torch.tensor([[1., 2.],\n",
    "                  [3., 4.]])   # (2,2)\n",
    "B = torch.tensor([[5., 6.],\n",
    "                  [7., 8.]])   # (2,2)\n",
    "print(\"A @ B =\\n\", A @ B)            # or torch.mm(A,B)\n",
    "\n",
    "# Batched matmul example: shape (batch, m, n) @ (batch, n, p) -> (batch, m, p)\n",
    "batchA = torch.stack([A, A + 1.0])   # shape (2,2,2)\n",
    "batchB = torch.stack([B, B + 1.0])   # shape (2,2,2)\n",
    "print(\"batchA shape:\", tuple(batchA.shape), \"batchB shape:\", tuple(batchB.shape))\n",
    "batchC = torch.matmul(batchA, batchB)   # batched matmul\n",
    "print(\"torch.matmul(batchA, batchB) -> shape:\", tuple(batchC.shape))\n",
    "print(batchC)\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 14) Dtype and Device utilities (casting & moving tensors)\n",
    "# ---------------------------------------------------------------------\n",
    "print(\"=== dtype & device utilities ===\")\n",
    "X_int = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"X_int dtype:\", X_int.dtype)\n",
    "X_float = X_int.float()    # cast to float\n",
    "print(\"X_int.float() dtype:\", X_float.dtype)\n",
    "\n",
    "# Move to GPU if available, otherwise show CPU path\n",
    "if torch.cuda.is_available():\n",
    "    dev = torch.device(\"cuda\")\n",
    "    X_gpu = X_float.to(dev)\n",
    "    print(\"Moved X to GPU:\", X_gpu.device)\n",
    "    # move back and convert to numpy (safe only on CPU)\n",
    "    X_back = X_gpu.to(\"cpu\")\n",
    "    print(\"Back on CPU:\", X_back.device, \"-> as numpy:\", X_back.numpy())\n",
    "else:\n",
    "    print(\"No CUDA device found ‚Äî staying on CPU.\")\n",
    "    # Convert to numpy directly (CPU tensor)\n",
    "    print(\"X_float.numpy() ->\", X_float.numpy())\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ Advanced 2D tensor demo complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2abb4c5",
   "metadata": {},
   "source": [
    "## üîπ 2Ô∏è‚É£ Matrix Multiplication (Linear Algebra Product)\n",
    "\n",
    "For the **matrix product**,  \n",
    "each element $(i, j)$ of the result is the **dot product** of the *i-th row* of $A$ and the *j-th column* of $B$.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Formula\n",
    "\n",
    "$$\n",
    "C = A \\times B, \\quad\n",
    "c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\n",
    "$$\n",
    "\n",
    "where $n$ = number of columns in $A$ (or rows in $B$).\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Step-by-Step with Positions\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "2 & 3 \\\\\n",
    "4 & 5\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "6 & 7 \\\\\n",
    "8 & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ For position **(0, 0)**\n",
    "\n",
    "We take the **1st row** of $A$ and the **1st column** of $B$:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "\\color{red}{2} & \\color{red}{3} \\\\\n",
    "4 & 5\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "\\color{red}{6} & 7 \\\\\n",
    "\\color{red}{8} & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "(0,0): \\; (2 \\times 6) + (3 \\times 8) = 46\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ For position **(0, 1)**\n",
    "\n",
    "Take the **1st row** of $A$ and the **2nd column** of $B$:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "\\color{red}{2} & \\color{red}{3} \\\\\n",
    "4 & 5\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "6 & \\color{red}{7} \\\\\n",
    "8 & \\color{red}{9}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "(0,1): \\; (2 \\times 7) + (3 \\times 9) = 51\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ For position **(1, 0)**\n",
    "\n",
    "Take the **2nd row** of $A$ and the **1st column** of $B$:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "2 & 3 \\\\\n",
    "\\color{red}{4} & \\color{red}{5}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "\\color{red}{6} & 7 \\\\\n",
    "\\color{red}{8} & 9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "(1,0): \\; (4 \\times 6) + (5 \\times 8) = 68\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ For position **(1, 1)**\n",
    "\n",
    "Take the **2nd row** of $A$ and the **2nd column** of $B$:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "2 & 3 \\\\\n",
    "\\color{red}{4} & \\color{red}{5}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "6 & \\color{red}{7} \\\\\n",
    "8 & \\color{red}{9}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "(1,1): \\; (4 \\times 7) + (5 \\times 9) = 75\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Matrix Multiplication Result\n",
    "\n",
    "$$\n",
    "A \\times B =\n",
    "\\begin{bmatrix}\n",
    "46 & 51 \\\\\n",
    "68 & 75\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Summary:**  \n",
    "Each cell $(i, j)$ of the resulting matrix is obtained by **multiplying the i-th row of A**  \n",
    "with the **j-th column of B** and summing the products.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0abe5",
   "metadata": {},
   "source": [
    "## üîπ Matrix Multiplication Example ‚Äî  (2√ó3) √ó (3√ó2)\n",
    "\n",
    "We have two tensors:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1 \\\\\n",
    "-1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Shape of $A$: **(2 √ó 3)**  \n",
    "Shape of $B$: **(3 √ó 2)**  \n",
    "‚úÖ Result shape = **(2 √ó 2)**  \n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Formula\n",
    "\n",
    "$$\n",
    "C = A \\times B, \\quad\n",
    "c_{ij} = \\sum_{k=1}^{3} a_{ik} b_{kj}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ For position **(0, 0)**\n",
    "\n",
    "Take the **1st row** of $A$ and the **1st column** of $B$:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "\\color{red}{0} & \\color{red}{1} & \\color{red}{1} \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "\\color{red}{1} & 1 \\\\\n",
    "\\color{red}{1} & 1 \\\\\n",
    "\\color{red}{-1} & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "(0,0): \\; (0\\times1) + (1\\times1) + (1\\times(-1)) = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ For position **(0, 1)**\n",
    "\n",
    "Take the **1st row** of $A$ and the **2nd column** of $B$:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "\\color{red}{0} & \\color{red}{1} & \\color{red}{1} \\\\\n",
    "1 & 0 & 1\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "1 & \\color{red}{1} \\\\\n",
    "1 & \\color{red}{1} \\\\\n",
    "-1 & \\color{red}{1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "(0,1): \\; (0\\times1) + (1\\times1) + (1\\times1) = 2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ For position **(1, 0)**\n",
    "\n",
    "Take the **2nd row** of $A$ and the **1st column** of $B$:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 1 \\\\\n",
    "\\color{red}{1} & \\color{red}{0} & \\color{red}{1}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "\\color{red}{1} & 1 \\\\\n",
    "\\color{red}{1} & 1 \\\\\n",
    "\\color{red}{-1} & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "(1,0): \\; (1\\times1) + (0\\times1) + (1\\times(-1)) = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ For position **(1, 1)**\n",
    "\n",
    "Take the **2nd row** of $A$ and the **2nd column** of $B$:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 1 \\\\\n",
    "\\color{red}{1} & \\color{red}{0} & \\color{red}{1}\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "B =\n",
    "\\begin{bmatrix}\n",
    "1 & \\color{red}{1} \\\\\n",
    "1 & \\color{red}{1} \\\\\n",
    "-1 & \\color{red}{1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "$$\n",
    "(1,1): \\; (1\\times1) + (0\\times1) + (1\\times1) = 2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Matrix Multiplication Result\n",
    "\n",
    "$$\n",
    "A \\times B =\n",
    "\\begin{bmatrix}\n",
    "0 & 2 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Summary:**  \n",
    "Each element $c_{ij}$ is computed by multiplying the **i-th row of A**  \n",
    "with the **j-th column of B** and summing the products.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
