{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5055ba",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Step 1: Differentiation in PyTorch\n",
    "\n",
    "In this module, weâ€™ll learn how **PyTorch performs differentiation** â€”  \n",
    "a core concept used to train neural networks by adjusting their parameters automatically.  \n",
    "\n",
    "Weâ€™ll start with **simple derivatives**, and later extend to **partial derivatives** for multivariable functions.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1ï¸âƒ£ What is a Derivative?\n",
    "\n",
    "A **derivative** measures how a function changes as its input changes.\n",
    "\n",
    "Consider a quadratic function:\n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "\n",
    "Evaluating this function at $( x = 2 )$ gives:\n",
    "\n",
    "$$\n",
    "y = 2^2 = 4\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2ï¸âƒ£ Finding the Derivative â€” Step by Step\n",
    "\n",
    "According to the rules of calculus:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = 2x\n",
    "$$\n",
    "\n",
    "Hereâ€™s how:\n",
    "- Bring down the **power** of $( x )$ (which is 2) in front of \\( x \\)  \n",
    "- Multiply by $$( x^{(2 - 1)} = x^1 )$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\color{#00b300}{\\frac{dy}{dx} = 2x}\n",
    "$$\n",
    "\n",
    "Evaluating at \\( x = 2 \\):\n",
    "\n",
    "$$\n",
    "\\color{#1f77b4}{\\frac{dy}{dx}\\Big|_{x=2}} = 2 \\times 2 = 4\n",
    "$$\n",
    "\n",
    "ðŸ“˜ **Interpretation:**  \n",
    "At $( x = 2 )$, the function is increasing at a rate of **4 units per unit change in $( x )$**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3ï¸âƒ£ Computing the Derivative in PyTorch (Concept + Intuition)\n",
    "\n",
    "PyTorchâ€™s **Autograd** engine performs **automatic differentiation** â€” it builds a **computational graph** that records every operation on tensors that have `requires_grad=True`.\n",
    "\n",
    "This allows PyTorch to compute derivatives automatically using the **chain rule**, without manually writing calculus.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Step 1 â€” Create a Tensor with Gradient Tracking\n",
    "\n",
    "When you create a tensor:\n",
    "\n",
    "```python\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "```\n",
    "\n",
    "PyTorch starts **tracking all operations** performed on `x`.  \n",
    "This means that when you later compute a scalar function of `x`, PyTorch can automatically differentiate it.\n",
    "\n",
    "The flag `requires_grad=True` tells PyTorch:\n",
    "\n",
    "> â€œKeep track of all operations on this tensor because I will need its derivatives later.â€\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§© Internal View of a Tracked Tensor\n",
    "\n",
    "| Attribute | Meaning |\n",
    "|------------|----------|\n",
    "| `requires_grad=True` | Enables gradient tracking |\n",
    "| `.grad_fn` | Refers to the **function** that created this tensor (e.g., `PowBackward`) |\n",
    "| `.is_leaf` | `True` if tensor is created by the user (not result of an op) |\n",
    "| `.grad` | Will store the computed gradient after `.backward()` |\n",
    "| `.data` | The raw tensor value (without gradient info) |\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Step 2 â€” Define a Function $y = f(x)$\n",
    "\n",
    "Suppose we define:\n",
    "\n",
    "```python\n",
    "y = x ** 2\n",
    "```\n",
    "\n",
    "This operation builds a **computation graph** internally:\n",
    "\n",
    "$$\n",
    "\\color{#1f77b4}{x}\n",
    "\\;\\xrightarrow{\\text{square}}\\;\n",
    "\\color{#ff7f0e}{y = x^2}\n",
    "$$\n",
    "\n",
    "Each node in this graph knows:\n",
    "- what operation produced it, and  \n",
    "- how to compute its derivative for the backward pass.\n",
    "\n",
    "PyTorch stores this information â€” but it hasnâ€™t calculated any gradient yet.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Step 3 â€” Compute the Derivative with `.backward()`\n",
    "\n",
    "When you call:\n",
    "\n",
    "```python\n",
    "y.backward()\n",
    "```\n",
    "\n",
    "PyTorch initiates the **backward pass** through the computation graph.\n",
    "\n",
    "It applies the **chain rule** from calculus to compute the derivative:\n",
    "\n",
    "$$\n",
    "\\color{#00b300}{\\frac{dy}{dx}} = \\frac{d(x^2)}{dx} = 2x\n",
    "$$\n",
    "\n",
    "Internally, this happens in **reverse order** of operations â€” hence the name **reverse-mode autodiff** (used in backpropagation).\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Step 4 â€” Access the Computed Gradient\n",
    "\n",
    "Once `.backward()` finishes, the gradient is stored in the tensorâ€™s `.grad` attribute:\n",
    "\n",
    "```python\n",
    "print(x.grad)\n",
    "```\n",
    "\n",
    "At the current value $x = 2$:\n",
    "\n",
    "$$\n",
    "\\color{#00b300}{\\frac{dy}{dx}\\Big|_{x=2}} = 2 \\times 2 = 4\n",
    "$$\n",
    "\n",
    "âœ… So, PyTorch outputs:\n",
    "\n",
    "```\n",
    "tensor(4.)\n",
    "```\n",
    "\n",
    "which perfectly matches the analytical result.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Summary Table\n",
    "\n",
    "| Concept | PyTorch Command | Mathematical Meaning | Example |\n",
    "|----------|-----------------|----------------------|----------|\n",
    "| Enable gradient tracking | `requires_grad=True` | Track variable $x$ | $x = 2$ |\n",
    "| Define differentiable function | `y = x ** 2` | $y = x^2$ | Square operation |\n",
    "| Compute derivative | `y.backward()` | $\\frac{dy}{dx} = 2x$ | Chain rule |\n",
    "| Access gradient | `x.grad` | Gradient at current $x$ | $4.0$ |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§¬ How Autograd Works Internally\n",
    "\n",
    "| Stage | Graph Representation | Description |\n",
    "|--------|----------------------|--------------|\n",
    "| â‘  Tensor creation | $\\color{#1f77b4}{x}$ | Leaf node, gradient tracking enabled |\n",
    "| â‘¡ Operation | $\\color{#1f77b4}{x} \\to \\text{square} \\to \\color{#ff7f0e}{y}$ | PyTorch builds computational graph |\n",
    "| â‘¢ Backward pass | $\\color{#ff7f0e}{y} \\Rightarrow \\text{backprop} \\Rightarrow \\color{#00b300}{x.grad}$ | Reverse-mode autodiff |\n",
    "| â‘£ Result | $\\color{#00b300}{x.grad = 4.0}$ | Derivative stored in `.grad` |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Behind the Scenes Summary\n",
    "\n",
    "- **`requires_grad=True`** â†’ Enables gradient tracking  \n",
    "- **Computation graph** â†’ Connects operations and tensors  \n",
    "- **`.backward()`** â†’ Performs differentiation using chain rule  \n",
    "- **`.grad`** â†’ Stores computed derivative for each **leaf tensor**\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Final Intuition Recap\n",
    "\n",
    "| Step | Code | Math | Meaning |\n",
    "|------|------|------|----------|\n",
    "| 1ï¸âƒ£ | `x = torch.tensor(2., requires_grad=True)` | $x = 2$ | Define differentiable variable |\n",
    "| 2ï¸âƒ£ | `y = x ** 2` | $y = x^2$ | Define function |\n",
    "| 3ï¸âƒ£ | `y.backward()` | $\\frac{dy}{dx} = 2x$ | Compute derivative |\n",
    "| 4ï¸âƒ£ | `x.grad` | $4.0$ | Retrieve derivative value |\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“˜ **Key Takeaway:**  \n",
    "PyTorchâ€™s **Autograd** system is a dynamic, real-time differentiation engine  \n",
    "that automatically builds and traverses computational graphs,  \n",
    "applying the **chain rule** to compute derivatives efficiently â€”  \n",
    "the exact mechanism that powers **deep learning training**. âš¡\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¸ Explanation of Each Step\n",
    "\n",
    "| Step | What you conceptually do | Why it matters |\n",
    "|------|---------------------------|----------------|\n",
    "| 1ï¸âƒ£ Enable grad tracking | Mark $( x )$ with `requires_grad=True` | Tells PyTorch to **track operations** on $( x )$ for differentiation. |\n",
    "| 2ï¸âƒ£ Build the function | Compute $( y = x^2 )$ using tensor ops | PyTorch creates a **computational graph** linking $( y $) back to $( x )$. |\n",
    "| 3ï¸âƒ£ Backward pass | Call a backward step on the scalar output $( y )$ | Runs **reverse-mode autodiff**, applying the **chain rule** through the graph. |\n",
    "| 4ï¸âƒ£ Read the gradient | Inspect `x.grad` | Stores $( \\left.\\frac{dy}{dx}\\right|_{x=2} = \\color{#1f77b4}{4} )$. |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Whatâ€™s Happening Behind the Scenes\n",
    "\n",
    "PyTorch creates a **computational graph** dynamically as you perform tensor operations.\n",
    "\n",
    "Each node in this graph represents:\n",
    "- A **tensor** (like `x` or `y`)  \n",
    "- A **function** (power, addition, multiplication, etc.) that produced that tensor\n",
    "\n",
    "When you call a backward step on the scalar output:\n",
    "1. PyTorch starts from the **output** node (here, \\( y \\))  \n",
    "2. Traverses the graph **backward**  \n",
    "3. Applies the **chain rule of calculus** at each operation  \n",
    "4. Accumulates gradients into the `.grad` attribute of **leaf tensors** (like  $x$)\n",
    "\n",
    "This is **reverse-mode automatic differentiation** â€” the engine behind **backpropagation**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Key Formula Recap\n",
    "\n",
    "The derivative of $( y = x^2 )$ with respect to $( x )$:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = 2x\n",
    "$$\n",
    "\n",
    "Evaluating at $x = 2$:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx}\\Big|_{x=2} = 2 \\times 2 = 4\n",
    "$$\n",
    "\n",
    "\n",
    "| **Function** | **Derivative** | **At x = 2** | **Result** |\n",
    "|:-------------:|:--------------:|:-------------:|:-----------:|\n",
    "| $$ y = x^2 $$ | $$ \\color{#00b300}{\\frac{dy}{dx} = 2x} $$ | $$ 2 \\times 2 $$ | **4** |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Summary**\n",
    "- `requires_grad=True` â†’ enables gradient tracking  \n",
    "- Backward step â†’ computes derivative automatically  \n",
    "- `x.grad` â†’ stores the gradient value  \n",
    "- PyTorch internally builds and traverses a **backward graph** ðŸ§©  \n",
    "- This mechanism is the **core of training neural networks** ðŸ¤–ðŸ’¡\n",
    "\n",
    "âœ¨ **In short:**  \n",
    "PyTorch performs **automatic differentiation** â€” you define the function, and it does the calculus for you âš¡\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a794bc",
   "metadata": {},
   "source": [
    "# âš¡ The Power Rule in Differentiation\n",
    "\n",
    "The example we computed earlier for  \n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "\n",
    "is a direct application of the **Power Rule** â€” one of the most fundamental derivative rules in calculus.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ The Power Rule â€” General Formula\n",
    "\n",
    "If you have a power function of the form:\n",
    "\n",
    "$$\n",
    "\\color{#00b300}{y = x^n}\n",
    "$$\n",
    "\n",
    "then its derivative with respect to \\(x\\) is:\n",
    "\n",
    "$$\n",
    "\\color{#1f77b4}{\\frac{dy}{dx} = n \\, x^{n - 1}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¸ Applying the Power Rule to $y = x^2$\n",
    "\n",
    "Here, \\( n = 2 \\):\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\color{#00b300}{2} \\, x^{\\,\\color{#d62728}{2 - 1}} = \\color{#00b300}{2x}\n",
    "$$\n",
    "\n",
    "Evaluating at \\( x = 2 \\):\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx}\\Big|_{x=2} = \\color{#00b300}{2} \\times \\color{#1f77b4}{2} = \\color{#d62728}{4}\n",
    "$$\n",
    "\n",
    "âœ… **Result:**  \n",
    "The slope of the curve $y = x^2$ at $x = 2$ is **4**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Intuition Behind the Power Rule\n",
    "\n",
    "The Power Rule tells us **how the slope changes** depending on the exponent \\(n\\):\n",
    "\n",
    "| Exponent $n$ | Function | Derivative | Behavior |\n",
    "|:---------------:|:----------|:------------|:-----------|\n",
    "| $> 1$ | $x^n$ | $n x^{n-1}$ | Slope increases rapidly |\n",
    "| $= 1$ | $x$ | $1$ | Constant slope |\n",
    "| $0 < n < 1$ | $x^{1/2}, x^{1/3}$ | Fractional power | Decreasing slope |\n",
    "| $< 0$ | $x^{-1}, x^{-2}$ | Negative powers | Inverse relationship |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ More Examples\n",
    "\n",
    "| Function | Derivative (Using Power Rule) | Simplified Form |\n",
    "|-----------|-------------------------------|----------------|\n",
    "| $y = x^3$ | $\\frac{dy}{dx} = 3x^2$ | â€” |\n",
    "| $y = x^5$ | $\\frac{dy}{dx} = 5x^4$ | â€” |\n",
    "| $y = x^{-1}$ | $\\frac{dy}{dx} = -x^{-2}$ | $-\\frac{1}{x^2}$ |\n",
    "| $y = x^{1/2}$ | $\\frac{dy}{dx} = \\frac{1}{2}x^{-1/2}$ | $\\frac{1}{2\\sqrt{x}}$ |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ The Power Rule in PyTorch ðŸ§®\n",
    "\n",
    "PyTorch automatically applies the **Power Rule** through its autograd system:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2          # y = xÂ²\n",
    "y.backward()        # Compute dy/dx\n",
    "print(x.grad)       # âžœ 4.0\n",
    "```\n",
    "--- \n",
    "\n",
    "## âš™ï¸ Behind the Scenes: How PyTorch Applies the Power Rule\n",
    "\n",
    "When you execute the above code, PyTorch internally performs the differentiation using the **Power Rule**:\n",
    "\n",
    "$$\n",
    "\\color{#1f77b4}{\\frac{dy}{dx}} = \\color{#00b300}{2x}\n",
    "$$\n",
    "\n",
    "Substituting $x = 2$:\n",
    "\n",
    "$$\n",
    "\\color{#1f77b4}{\\frac{dy}{dx}\\Big|_{x=2}} = \\color{#00b300}{2 \\times 2 = 4}\n",
    "$$\n",
    "\n",
    "âœ… Thus, PyTorch computes the same derivative as calculus â€” automatically via **autograd**.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Summary\n",
    "\n",
    "| Concept | Formula | Meaning |\n",
    "|----------|----------|----------|\n",
    "| **Power Rule** | $ \\color{#1f77b4}{\\frac{dy}{dx} = n x^{n-1}} $ | Multiply by the exponent and reduce the power by 1 |\n",
    "| **At $n=2$** | $ \\frac{dy}{dx} = 2x $ | Linear slope increasing with $x$ |\n",
    "| **At $x=2$** | $ 2 \\times 2 = 4 $ | Instantaneous rate of change = 4 |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“˜ **Key Takeaway:**  \n",
    "The **Power Rule** is one of the most common differentiation tools â€”  \n",
    "and PyTorchâ€™s **autograd** applies it automatically whenever you perform power operations on tensors âš¡ðŸ¤–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fdde9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- A) Single-variable derivative: y = x^2 ----------\n",
      "x (input)            | value=  2.0000 | dtype=torch.float32 | req_grad=True\n",
      "y = x**2             | value=  4.0000 | dtype=torch.float32 | req_grad=True\n",
      "dy/dx at x=2 -> 4.0 (expected 4.0)\n",
      "x (after backward)   | shape=() | dtype=torch.float32 | req_grad=True\n",
      "  .is_leaf=True | .grad_fn=None | .grad=4.0\n",
      "\n",
      "y (result node)      | shape=() | dtype=torch.float32 | req_grad=True\n",
      "  .is_leaf=False | .grad_fn=<PowBackward0 object at 0x125c1ee60> | .grad=None\n",
      "\n",
      "\n",
      "---------- B) Another single-variable example ----------\n",
      "x2 (input)           | value=  2.0000 | dtype=torch.float32 | req_grad=True\n",
      "z = 1.5*x2**2 + 3    | value=  9.0000 | dtype=torch.float32 | req_grad=True\n",
      "z(2)      -> 9.0 (expected 9.0)\n",
      "dz/dx|2   -> 6.0 (expected 6.0)\n",
      "\n",
      "---------- C) Autograd graph introspection ----------\n",
      "x.is_leaf: True  | x.grad_fn: None\n",
      "y.is_leaf: False  | y.grad_fn: <PowBackward0 object at 0x1277b85b0>\n",
      "x.requires_grad: True\n",
      "x.data: tensor(2.)\n",
      "x.grad: tensor(4.)\n",
      "\n",
      "\n",
      "---------- D) Partial derivatives f(u,v) = u*v + u**2 ----------\n",
      "u                    | value=  1.0000 | dtype=torch.float32 | req_grad=True\n",
      "v                    | value=  3.0000 | dtype=torch.float32 | req_grad=True\n",
      "f = u*v + u**2       | value=  4.0000 | dtype=torch.float32 | req_grad=True\n",
      "f(u,v) = 4.0  (expected 4)\n",
      "df/du  = 5.0  (expected 5 = v + 2u)\n",
      "df/dv  = 1.0  (expected 1 = u)\n",
      "\n",
      "---------- E) Gradient accumulation & zeroing ----------\n",
      "After first backward: x3.grad = 4.0 (2*x @ x=2 -> 4)\n",
      "After zero_ and second backward: x3.grad = 12.0 (6*x @ x=2 -> 12)\n",
      "\n",
      "---------- F) Detach vs no_grad ----------\n",
      "a.requires_grad: True\n",
      "b.requires_grad: True\n",
      "c.requires_grad: False  (no-grad)\n",
      "d.requires_grad: True  (depends on a)\n",
      "a.grad from e.backward(): tensor(1.)\n",
      "\n",
      "---------- G) Non-scalar outputs & grad_outputs (VJP) ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vf/rst47yl90f74cf3frj58cvw00000gn/T/ipykernel_2121/3057287102.py:38: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(f\"  .is_leaf={t.is_leaf} | .grad_fn={t.grad_fn} | .grad={t.grad}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F(x) = [4.0, 6.0] at x=2\n",
      "âˆ‚(w^T F)/âˆ‚x at x=2 with w=[1,1]: 7.0 (expected 7)\n",
      "\n",
      "---------- H) Jacobian & Hessian via autograd.functional ----------\n",
      "Jacobian of F=[u*v+u^2, u-v] at (u=1,v=3):\n",
      "tensor([[ 5.,  1.],\n",
      "        [ 1., -1.]])\n",
      "Hessian (second derivative) of x^4 at x=2 -> 48.0 (expected 48)\n",
      "\n",
      "---------- I) retain_graph example ----------\n",
      "First pass dy/dx (accumulated): 16.0 (= 3*x^2 + 2*x at x=2 -> 12 + 4 = 16)\n",
      "Second pass dy/dx (after zero_): 16.0 (same expected 16)\n",
      "\n",
      "âœ… Completed: single & partial derivatives, accumulation, detach/no_grad, VJP, Jacobian, Hessian, retain_graph.\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Autograd â€” Complete Differentiation Demo (with detailed comments)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This notebook cell demonstrates:\n",
    "#  A) Single-variable derivative: y = x^2 at x=2\n",
    "#  B) Another example: choose z so that z(2)=9 and z'(2)=6\n",
    "#  C) Autograd graph introspection: .data, .grad, .grad_fn, .is_leaf, .requires_grad\n",
    "#  D) Partial derivatives: f(u,v) = u*v + u^2   (df/du = v+2u, df/dv = u)\n",
    "#  E) Gradient accumulation & zeroing grads\n",
    "#  F) Detach vs no_grad (turning off gradient tracking)\n",
    "#  G) Non-scalar outputs: providing grad_outputs (vector-Jacobian product)\n",
    "#  H) Jacobian & Hessian via autograd.functional\n",
    "#  I) Bonus: retain_graph example (multiple backward passes through the same graph)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "import torch                                  # import PyTorch main package\n",
    "from torch.autograd import functional as A     # functional API for Jacobian/Hessian\n",
    "\n",
    "torch.manual_seed(0)                           # set RNG seed for reproducibility (if randomness used later)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper pretty-printers\n",
    "# ---------------------------\n",
    "def p_scalar(name, t):\n",
    "    \"\"\"\n",
    "    Print a scalar tensor's value and key autograd flags.\n",
    "    - name: label to print\n",
    "    - t: a 0-dim (scalar) tensor\n",
    "    \"\"\"\n",
    "    # f-string formats a line: name | value | dtype | requires_grad flag\n",
    "    print(f\"{name:20s} | value={t.item():>8.4f} | dtype={t.dtype} | req_grad={t.requires_grad}\")\n",
    "\n",
    "def p_info(name, t):\n",
    "    \"\"\"\n",
    "    Print general tensor info including autograd attributes.\n",
    "    Useful to see .is_leaf, .grad_fn, and current .grad.\n",
    "    \"\"\"\n",
    "    print(f\"{name:20s} | shape={tuple(t.shape)} | dtype={t.dtype} | req_grad={t.requires_grad}\")\n",
    "    print(f\"  .is_leaf={t.is_leaf} | .grad_fn={t.grad_fn} | .grad={t.grad}\")\n",
    "    print()\n",
    "\n",
    "def sep(title):\n",
    "    \"\"\"Pretty separator line for sections.\"\"\"\n",
    "    print(\"\\n\" + \"-\"*10 + f\" {title} \" + \"-\"*10)\n",
    "\n",
    "# =============================================================================\n",
    "# A) Single-variable derivative: y = x^2  at x=2  â†’ dy/dx = 2x, so 4 at x=2\n",
    "# =============================================================================\n",
    "sep(\"A) Single-variable derivative: y = x^2\")\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)   # create a leaf tensor with value 2.0 and enable gradient tracking\n",
    "y = x**2                                    # build computation graph node y = x^2 (records pow op with grad_fn)\n",
    "\n",
    "p_scalar(\"x (input)\", x)                    # print scalar info for x\n",
    "p_scalar(\"y = x**2\", y)                     # print scalar info for y\n",
    "\n",
    "y.backward()                                # perform backward pass from scalar y; computes dy/dx at current x\n",
    "print(\"dy/dx at x=2 ->\", x.grad.item(), \"(expected 4.0)\")  # read gradient accumulated into x.grad\n",
    "\n",
    "# Inspect core autograd fields\n",
    "p_info(\"x (after backward)\", x)             # x is a leaf: .grad_fn=None, .is_leaf=True, .grad holds dy/dx\n",
    "p_info(\"y (result node)\", y)                # y is non-leaf: has a grad_fn describing the operation that produced it\n",
    "\n",
    "# =============================================================================\n",
    "# B) Another example: choose z s.t. z(2)=9 and z'(2)=6\n",
    "#    One choice: z(x) = 1.5*x^2 + 3  -> z(2)=1.5*4+3=9 ; z'(x) = 3x -> z'(2)=6\n",
    "# =============================================================================\n",
    "sep(\"B) Another single-variable example\")\n",
    "\n",
    "x2 = torch.tensor(2.0, requires_grad=True)  # new independent leaf tensor at 2.0\n",
    "z  = 1.5 * x2**2 + 3.0                      # define z(x) with chosen coefficients to match target value/derivative\n",
    "\n",
    "p_scalar(\"x2 (input)\", x2)                  # print scalar info for x2\n",
    "p_scalar(\"z = 1.5*x2**2 + 3\", z)            # print scalar info for z\n",
    "\n",
    "z.backward()                                # compute dz/dx2 at x2=2\n",
    "print(\"z(2)      ->\", z.item(),   \"(expected 9.0)\")                # verify function value\n",
    "print(\"dz/dx|2   ->\", x2.grad.item(), \"(expected 6.0)\")            # verify derivative value\n",
    "\n",
    "# =============================================================================\n",
    "# C) Autograd graph introspection\n",
    "# =============================================================================\n",
    "sep(\"C) Autograd graph introspection\")\n",
    "\n",
    "print(\"x.is_leaf:\", x.is_leaf, \" | x.grad_fn:\", x.grad_fn)          # True; None for leaf tensors (created by user)\n",
    "print(\"y.is_leaf:\", y.is_leaf, \" | y.grad_fn:\", y.grad_fn)          # False; has a PowBackward grad_fn\n",
    "print(\"x.requires_grad:\", x.requires_grad)                          # True; signals autograd tracking\n",
    "print(\"x.data:\", x.data)                                            # underlying raw tensor (no tracking)\n",
    "print(\"x.grad:\", x.grad)                                            # accumulated gradient from previous backward\n",
    "print()                                                             # blank line for readability\n",
    "\n",
    "# =============================================================================\n",
    "# D) Partial derivatives: f(u,v) = u*v + u^2\n",
    "#     df/du = v + 2u, df/dv = u\n",
    "#     Evaluate at (u,v)=(1,3) -> f=4, df/du=5, df/dv=1\n",
    "# =============================================================================\n",
    "sep(\"D) Partial derivatives f(u,v) = u*v + u**2\")\n",
    "\n",
    "u = torch.tensor(1.0, requires_grad=True)   # leaf tensor u with gradient tracking\n",
    "v = torch.tensor(3.0, requires_grad=True)   # leaf tensor v with gradient tracking\n",
    "f = u*v + u**2                              # build graph for f(u,v) = u*v + u^2\n",
    "\n",
    "p_scalar(\"u\", u)                            # show u\n",
    "p_scalar(\"v\", v)                            # show v\n",
    "p_scalar(\"f = u*v + u**2\", f)               # show f\n",
    "\n",
    "f.backward()                                # compute gradients âˆ‚f/âˆ‚u into u.grad and âˆ‚f/âˆ‚v into v.grad\n",
    "print(\"f(u,v) =\", f.item(), \" (expected 4)\")                   # verify function value at (1,3)\n",
    "print(\"df/du  =\", u.grad.item(), \" (expected 5 = v + 2u)\")     # check u.grad == v + 2u = 3 + 2*1 = 5\n",
    "print(\"df/dv  =\", v.grad.item(), \" (expected 1 = u)\")          # check v.grad == u = 1\n",
    "\n",
    "# =============================================================================\n",
    "# E) Gradient accumulation & zeroing grads\n",
    "#    By default, calling backward() accumulates into .grad. Zero them if reusing leaves.\n",
    "# =============================================================================\n",
    "sep(\"E) Gradient accumulation & zeroing\")\n",
    "\n",
    "x3 = torch.tensor(2.0, requires_grad=True)  # new leaf\n",
    "g1 = x3**2                                  # g1(x) = x^2 ; derivative is 2x\n",
    "g1.backward()                               # compute dg1/dx at x=2 -> 4, store in x3.grad\n",
    "print(\"After first backward: x3.grad =\", x3.grad.item(), \"(2*x @ x=2 -> 4)\")\n",
    "\n",
    "x3.grad.zero_()                             # IMPORTANT: clear accumulated grads before another backward\n",
    "g2 = 3*x3**2                                # g2(x) = 3x^2 ; derivative is 6x\n",
    "g2.backward()                               # compute dg2/dx at x=2 -> 12\n",
    "print(\"After zero_ and second backward: x3.grad =\", x3.grad.item(), \"(6*x @ x=2 -> 12)\")\n",
    "\n",
    "# =============================================================================\n",
    "# F) Detach vs torch.no_grad (stop tracking computations)\n",
    "# =============================================================================\n",
    "sep(\"F) Detach vs no_grad\")\n",
    "\n",
    "a = torch.tensor(2.0, requires_grad=True)   # leaf requiring grad\n",
    "b = a**3                                    # tracked op: b depends on a (grad flows a <- b)\n",
    "\n",
    "with torch.no_grad():                       # temporarily disable autograd tracking inside this block\n",
    "    c = b * 10                              # c does NOT require grad; ops inside no_grad are excluded from graph\n",
    "\n",
    "d = c + a                                   # d depends on a (requires_grad=True) + c (no grad); result still requires grad\n",
    "print(\"a.requires_grad:\", a.requires_grad)  # True (leaf)\n",
    "print(\"b.requires_grad:\", b.requires_grad)  # True (tracked)\n",
    "print(\"c.requires_grad:\", c.requires_grad, \" (no-grad)\")  # False (created under no_grad)\n",
    "print(\"d.requires_grad:\", d.requires_grad, \" (depends on a)\")  # True (because of a)\n",
    "\n",
    "b_detached = b.detach()                     # create a tensor with same data as b but NO grad history (cut graph)\n",
    "e = b_detached * 5 + a                      # e depends only on a for gradients; path through b is cut\n",
    "# Zero a.grad first to avoid accumulation from earlier demos\n",
    "if a.grad is not None:                      # guard in case this is the first run\n",
    "    a.grad.zero_()                          # clear any previous gradient on 'a'\n",
    "e.backward()                                # compute de/da; derivative wrt a is 1 from \"+ a\" term\n",
    "print(\"a.grad from e.backward():\", a.grad)  # should be tensor(1.) after zeroing then backward\n",
    "\n",
    "# =============================================================================\n",
    "# G) Non-scalar outputs â†’ provide grad_outputs (vector-Jacobian product)\n",
    "#    Example: F(x) = [x^2, 3x], evaluated at x=2\n",
    "#    For vector outputs, backward needs a \"gradient\" of the same shape (weights for dot product).\n",
    "# =============================================================================\n",
    "sep(\"G) Non-scalar outputs & grad_outputs (VJP)\")\n",
    "\n",
    "x4 = torch.tensor(2.0, requires_grad=True)  # new leaf\n",
    "F = torch.stack([x4**2, 3*x4])              # construct a vector output tensor of shape (2,)\n",
    "w = torch.tensor([1.0, 1.0])                # choose weights for linear combination w^T F (same shape as F)\n",
    "F.backward(gradient=w)                      # computes d/dx4 (w1*x^2 + w2*3x) at x=2; here: 2*x + 3 = 7\n",
    "print(\"F(x) =\", F.detach().tolist(), \"at x=2\")                   # show the vector F(2)\n",
    "print(\"âˆ‚(w^T F)/âˆ‚x at x=2 with w=[1,1]:\", x4.grad.item(), \"(expected 7)\")  # print resulting gradient\n",
    "\n",
    "# =============================================================================\n",
    "# H) Full Jacobian and Hessian with autograd.functional\n",
    "#    - jacobian(func, inputs) returns partial derivatives of each output wrt inputs\n",
    "#    - hessian(func, inputs) returns second derivatives (can be expensive)\n",
    "# =============================================================================\n",
    "sep(\"H) Jacobian & Hessian via autograd.functional\")\n",
    "\n",
    "def F_uv(u, v):\n",
    "    \"\"\"\n",
    "    Vector-valued function F(u, v) returning:\n",
    "      F1 = u*v + u^2\n",
    "      F2 = u - v\n",
    "    This version explicitly takes two arguments (u, v),\n",
    "    because autograd.functional.jacobian expands tuple inputs.\n",
    "    \"\"\"\n",
    "    return torch.stack([\n",
    "        u * v + u**2,   # F1(u,v)\n",
    "        u - v           # F2(u,v)\n",
    "    ])\n",
    "\n",
    "# Define the two scalar inputs (u, v)\n",
    "u = torch.tensor(1.0, requires_grad=True)\n",
    "v = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Compute the full Jacobian matrix of F wrt (u, v)\n",
    "J = A.jacobian(F_uv, (u, v))  # autograd will call F_uv(u, v)\n",
    "\n",
    "# J is nested tuple: ( (dF/du), (dF/dv) )\n",
    "# Each entry is a tensor of shape (output_dim, input_dim_component)\n",
    "J_mat = torch.stack([\n",
    "    torch.stack([J[0][0], J[0][1]]),  # dF1/du, dF1/dv\n",
    "    torch.stack([J[1][0], J[1][1]])   # dF2/du, dF2/dv\n",
    "])\n",
    "print(\"Jacobian of F=[u*v+u^2, u-v] at (u=1,v=3):\")\n",
    "print(J_mat)  # Expected [[v+2u, u], [1, -1]] -> [[5,1],[1,-1]]\n",
    "\n",
    "# Hessian example (second derivative) for scalar function h(x) = x^4\n",
    "def h_scalar(x_):\n",
    "    \"\"\"Scalar function h(x) = x^4, second derivative = 12x^2\"\"\"\n",
    "    return x_**4\n",
    "\n",
    "xh = torch.tensor(2.0, requires_grad=True)\n",
    "H = A.hessian(h_scalar, xh)\n",
    "print(\"Hessian (second derivative) of x^4 at x=2 ->\", H.item(), \"(expected 48)\")\n",
    "# =============================================================================\n",
    "# I) Bonus: retain_graph example\n",
    "#    If you need to call backward twice on the SAME graph, pass retain_graph=True on the first call.\n",
    "#    Note: Gradients accumulate into .grad, so zero them between passes if needed.\n",
    "# =============================================================================\n",
    "sep(\"I) retain_graph example\")\n",
    "\n",
    "x5 = torch.tensor(2.0, requires_grad=True)    # new leaf\n",
    "y5 = (x5**3) + (x5**2)                        # define y5(x) = x^3 + x^2\n",
    "# First backward pass: retain_graph=True keeps the graph so we can call backward again\n",
    "y5.backward(retain_graph=True)                # computes dy5/dx5 and keeps graph alive\n",
    "print(\"First pass dy/dx (accumulated):\", x5.grad.item(), \"(= 3*x^2 + 2*x at x=2 -> 12 + 4 = 16)\")\n",
    "\n",
    "x5.grad.zero_()                               # clear accumulated gradient before second backward\n",
    "y5.backward()                                 # second backward; graph is still valid because we retained it earlier\n",
    "print(\"Second pass dy/dx (after zero_):\", x5.grad.item(), \"(same expected 16)\")\n",
    "\n",
    "print(\"\\nâœ… Completed: single & partial derivatives, accumulation, detach/no_grad, VJP, Jacobian, Hessian, retain_graph.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
